{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69207995c2b68685",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "LSTM with attention.\n",
    "\n",
    "Got mse 173ish with all agent data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12ecbb45022667b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T20:31:11.408289Z",
     "start_time": "2025-05-29T20:31:07.814153Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-30 03:13:38.189181: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-05-30 03:13:38.189251: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-05-30 03:13:38.190869: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-30 03:13:38.201132: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T20:31:12.602195Z",
     "start_time": "2025-05-29T20:31:11.408612Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data's shape (10000, 50, 110, 6)\n",
      "test_data's shape (2100, 50, 50, 6)\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "import numpy as np\n",
    "train_file = np.load('data/train.npz')\n",
    "train_data = train_file['data']\n",
    "print(\"train_data's shape\", train_data.shape)\n",
    "test_file = np.load('data/test_input.npz')\n",
    "test_data = test_file['data']\n",
    "print(\"test_data's shape\", test_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14d9241b88b2e2e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T20:31:12.660923Z",
     "start_time": "2025-05-29T20:31:12.603987Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input, RepeatVector, TimeDistributed, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9df4b831ee8e017",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T20:31:12.661641Z",
     "start_time": "2025-05-29T20:31:12.630902Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def standardize_data_dimensions(scenario_data):\n",
    "    \"\"\"\n",
    "    Standardize position data by centering a single scenario at the origin.\n",
    "    \n",
    "    :param scenario_data: numpy array of shape (50, 110, 6)\n",
    "                         where dimensions are [position_x, position_y, velocity_x, velocity_y, heading, object_type]\n",
    "    :returns: tuple of (standardized_data, min_values)\n",
    "             - standardized_data: same shape as input with centered positions\n",
    "             - min_values: array of shape (2,) containing [min_x, min_y] for this scenario\n",
    "    \"\"\"\n",
    "    # Copy the data to avoid modifying the original\n",
    "    standardized_data = scenario_data.copy()\n",
    "    \n",
    "    # Extract position data (first 2 dimensions)\n",
    "    positions = scenario_data[:, :, :2]  # Shape: (50, 110, 2)\n",
    "    \n",
    "    # Create mask for non-zero positions (to ignore padding)\n",
    "    # We consider a position valid if it's not (0,0) or if the object_type is not 0\n",
    "    object_types = scenario_data[:, :, 5]  # Shape: (50, 110)\n",
    "    valid_mask = (positions[:, :, 0] != 0) | (positions[:, :, 1] != 0) | (object_types != 0)\n",
    "    \n",
    "    # Find min values across all valid positions in this scenario\n",
    "    if np.any(valid_mask):\n",
    "        valid_positions = positions[valid_mask]  # Shape: (num_valid_points, 2)\n",
    "        min_x = np.min(valid_positions[:, 0])\n",
    "        min_y = np.min(valid_positions[:, 1])\n",
    "    else:\n",
    "        # If no valid positions found, use 0 as min values\n",
    "        min_x = 0\n",
    "        min_y = 0\n",
    "    \n",
    "    # Store min values\n",
    "    min_values = np.array([min_x, min_y])\n",
    "    \n",
    "    # Standardize positions by subtracting min values\n",
    "    # Only modify non-zero positions to preserve padding\n",
    "    for agent_idx in range(scenario_data.shape[0]):\n",
    "        for time_idx in range(scenario_data.shape[1]):\n",
    "            if valid_mask[agent_idx, time_idx]:\n",
    "                standardized_data[agent_idx, time_idx, 0] -= min_x  # position_x\n",
    "                standardized_data[agent_idx, time_idx, 1] -= min_y  # position_y\n",
    "    \n",
    "    return standardized_data, min_values\n",
    "\n",
    "\n",
    "def denormalize_predictions(predictions, min_values):\n",
    "    \"\"\"\n",
    "    Helper function to add back the min values to predicted positions.\n",
    "    \n",
    "    :param predictions: predicted data with standardized positions, shape (50, 110, 6) or similar\n",
    "    :param min_values: array of shape (2,) containing [min_x, min_y] for this scenario\n",
    "    :returns: predictions with original coordinate system restored\n",
    "    \"\"\"\n",
    "    denormalized = predictions.copy()\n",
    "    \n",
    "    # Add back the min values to restore original coordinate system\n",
    "    # Assuming predictions have position_x and position_y as first two dimensions\n",
    "    denormalized[:, :, 0] += min_values[0]  # position_x\n",
    "    denormalized[:, :, 1] += min_values[1]  # position_y\n",
    "    \n",
    "    return denormalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db886a7f7f3d9df5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T20:31:12.662294Z",
     "start_time": "2025-05-29T20:31:12.635822Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "\n",
    "def standardize_single_scene(scene_data):\n",
    "    \"\"\"\n",
    "    Wrapper function to standardize a single scene and return both standardized data and min values.\n",
    "    This function will be called in parallel.\n",
    "    \"\"\"\n",
    "    standardized_scene, min_vals = standardize_data_dimensions(scene_data)\n",
    "    return standardized_scene, min_vals\n",
    "\n",
    "def parallel_standardize_training_data(train_data, n_jobs=-1, verbose=True):\n",
    "    \"\"\"\n",
    "    Parallelize the standardization of training data across all scenes.\n",
    "    \n",
    "    :param train_data: numpy array of shape (10000, 50, 110, 6)\n",
    "    :param n_jobs: number of parallel jobs (-1 uses all available cores)\n",
    "    :param verbose: whether to show progress bar\n",
    "    :returns: tuple of (standardized_data, min_values_array)\n",
    "    \"\"\"\n",
    "    print(f\"Standardizing {train_data.shape[0]} scenes using {multiprocessing.cpu_count() if n_jobs == -1 else n_jobs} cores...\")\n",
    "    \n",
    "    # Use joblib to parallelize the processing\n",
    "    if verbose:\n",
    "        # With progress bar\n",
    "        results = Parallel(n_jobs=n_jobs)(\n",
    "            delayed(standardize_single_scene)(train_data[i]) \n",
    "            for i in tqdm(range(train_data.shape[0]), desc=\"Processing scenes\")\n",
    "        )\n",
    "    else:\n",
    "        # Without progress bar\n",
    "        results = Parallel(n_jobs=n_jobs)(\n",
    "            delayed(standardize_single_scene)(train_data[i]) \n",
    "            for i in range(train_data.shape[0])\n",
    "        )\n",
    "    \n",
    "    # Unpack results\n",
    "    standardized_scenes, min_values_list = zip(*results)\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    standardized_data = np.array(standardized_scenes)\n",
    "    min_values_array = np.array(min_values_list)\n",
    "    \n",
    "    print(f\"Standardization complete!\")\n",
    "    print(f\"Standardized data shape: {standardized_data.shape}\")\n",
    "    print(f\"Min values shape: {min_values_array.shape}\")\n",
    "    \n",
    "    return standardized_data, min_values_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c81976ba8c9b548",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T20:31:43.984732Z",
     "start_time": "2025-05-29T20:31:12.649083Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardizing 10000 scenes using 40 cores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing scenes: 100%|██████████| 10000/10000 [00:15<00:00, 650.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardization complete!\n",
      "Standardized data shape: (10000, 50, 110, 6)\n",
      "Min values shape: (10000, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10000, 50, 110, 6)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standardized_train_data, min_values = parallel_standardize_training_data(\n",
    "    train_data, \n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    verbose=True\n",
    ")\n",
    "standardized_train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f40b1487275e6996",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T20:31:44.059459Z",
     "start_time": "2025-05-29T20:31:43.413293Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import (\n",
    "    Input, LSTM, Dense, Dropout, RepeatVector, TimeDistributed, \n",
    "    Concatenate, Activation, Dot, Layer, BatchNormalization, \n",
    "    LayerNormalization, Add\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import Orthogonal, GlorotUniform\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "class ScaleLayer(Layer):\n",
    "    def __init__(self, scale_factor, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.scale_factor = scale_factor\n",
    "    \n",
    "    def call(self, x):\n",
    "        return x / self.scale_factor\n",
    "\n",
    "class MaxSubtractLayer(Layer):\n",
    "    def call(self, x):\n",
    "        return x - K.max(x, axis=-1, keepdims=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0a6a9b5057173bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T20:31:44.061847Z",
     "start_time": "2025-05-29T20:31:43.441321Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "def load_model(model_path='lstm2.keras'):\n",
    "    \n",
    "    custom_objects = {\n",
    "        'ScaleLayer':ScaleLayer,\n",
    "        'MaxSubtractLayer':MaxSubtractLayer\n",
    "    }\n",
    "    \n",
    "    model = keras.models.load_model(model_path, custom_objects=custom_objects, safe_mode=False)\n",
    "    print(f\"Keras model loaded from {model_path}\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2f049ff-a26d-41a5-aa7e-b0ea462265f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-30 03:14:04.842669: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10534 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:08:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras model loaded from lstm2.keras\n"
     ]
    }
   ],
   "source": [
    "model = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d49a92ffcd8f00c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T20:35:31.734388Z",
     "start_time": "2025-05-29T20:35:31.730407Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Dense, RepeatVector, TimeDistributed, Dropout, Attention, Concatenate, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "\n",
    "def create_lstm_encoder_decoder_with_attention(input_dim, output_dim, timesteps_in, timesteps_out,\n",
    "                                             lstm_units=512, num_layers=3, loss_fn='mse', lr=0.001):\n",
    "    inputs = Input(shape=(timesteps_in, input_dim))\n",
    "\n",
    "    # Encoder - preserve all timesteps for attention\n",
    "    x = inputs\n",
    "    for i in range(num_layers):\n",
    "        x = LSTM(lstm_units, return_sequences=True, name=f'encoder_lstm_{i}')(x)\n",
    "    encoder_outputs = x  # All encoder hidden states\n",
    "    \n",
    "    # Get context vector from final timestep\n",
    "    context_vector = Lambda(lambda x: x[:, -1, :])(encoder_outputs)\n",
    "\n",
    "    # Decoder\n",
    "    decoder_input = RepeatVector(timesteps_out)(context_vector)\n",
    "    \n",
    "    # Decoder LSTM layers\n",
    "    decoder_output = decoder_input\n",
    "    for i in range(num_layers):\n",
    "        decoder_output = LSTM(lstm_units, return_sequences=True, name=f'decoder_lstm_{i}')(decoder_output)\n",
    "    \n",
    "    # Attention mechanism\n",
    "    attention_layer = Attention(name='attention')\n",
    "    attended_context = attention_layer([decoder_output, encoder_outputs])\n",
    "    \n",
    "    # Combine decoder output with attended context\n",
    "    combined = Concatenate(axis=-1)([decoder_output, attended_context])\n",
    "    \n",
    "    # Output layers\n",
    "    x = TimeDistributed(Dense(128, activation='relu'))(combined)\n",
    "    x = TimeDistributed(Dense(64, activation='relu'))(x)\n",
    "    outputs = TimeDistributed(Dense(output_dim))(x)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(learning_rate=lr, clipnorm=0.5), loss='mse', metrics=['mae'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee1df27124751b62",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T20:35:31.860939Z",
     "start_time": "2025-05-29T20:35:31.859179Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from keras.src.callbacks import LearningRateScheduler, EarlyStopping, Callback\n",
    "from keras.src.optimizers import Adam\n",
    "from keras import Model\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "class DynamicReduceLROnPlateau(Callback):\n",
    "    def __init__(self, factor=0.1, patience=3, min_lr=1e-7, verbose=1):\n",
    "        super().__init__()\n",
    "        self.factor = factor\n",
    "        self.patience = patience\n",
    "        self.min_lr = min_lr\n",
    "        self.verbose = verbose\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.wait = 0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        current_val_loss = logs.get(\"val_loss\")\n",
    "\n",
    "        if current_val_loss is None:\n",
    "            return  # can't do anything if val_loss isn't available\n",
    "\n",
    "        if current_val_loss < self.best_val_loss - 1e-4:  # a small delta\n",
    "            self.best_val_loss = current_val_loss\n",
    "            self.wait = 0\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                old_lr = float(self.model.optimizer.learning_rate.numpy())\n",
    "                if old_lr > self.min_lr:\n",
    "                    new_lr = max(old_lr * self.factor, self.min_lr)\n",
    "                    self.model.optimizer.learning_rate.assign(new_lr)\n",
    "                    if self.verbose:\n",
    "                        print(f\"\\nEpoch {epoch+1}: val_loss did not improve. Reducing LR from {old_lr:.6f} to {new_lr:.6f}\")\n",
    "                    self.wait = 0  # reset after LR reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99dd9a0ab8e0e651",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T20:35:32.032707Z",
     "start_time": "2025-05-29T20:35:32.025961Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class LRThresholdCallback(Callback):\n",
    "    def __init__(self, threshold=9e-5):\n",
    "        super().__init__()\n",
    "        self.threshold = threshold\n",
    "        self.should_stop = False\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        lr = float(self.model.optimizer.learning_rate.numpy())\n",
    "        if lr < self.threshold:\n",
    "            print(f\"\\nLearning rate {lr:.6f} < threshold {self.threshold}, moving to Phase 2.\")\n",
    "            self.model.stop_training = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48e6074507a81432",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T20:35:32.166332Z",
     "start_time": "2025-05-29T20:35:32.159768Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "class GradientMonitoringCallback(Callback):\n",
    "    def __init__(self, clip_min=1e-4, clip_max=1e2, monitor_frequency=3):\n",
    "        \"\"\"\n",
    "        Monitor gradient norms during training\n",
    "        \n",
    "        Args:\n",
    "            clip_min: Minimum threshold for gradient norms\n",
    "            clip_max: Maximum threshold for gradient norms  \n",
    "            monitor_frequency: How often to check gradients (every N batches)\n",
    "        \"\"\"\n",
    "        print(f\"🔧 GradientMonitoringCallback initialized with clip_min={clip_min}, clip_max={clip_max}, monitor_freq={monitor_frequency}\")\n",
    "        self.clip_min = clip_min\n",
    "        self.clip_max = clip_max\n",
    "        self.monitor_frequency = monitor_frequency\n",
    "        self.batch_count = 0\n",
    "        self.total_calls = 0\n",
    "        self.gradient_checks = 0\n",
    "        self.fallback_calls = 0\n",
    "        \n",
    "    def on_train_begin(self, logs=None):\n",
    "        print(\"🚀 GradientMonitoringCallback: Training started!\")\n",
    "        self.batch_count = 0\n",
    "        self.total_calls = 0\n",
    "        self.gradient_checks = 0\n",
    "        self.fallback_calls = 0\n",
    "        \n",
    "    # def on_epoch_begin(self, epoch, logs=None):\n",
    "    #     print(f\"📍 GradientMonitoringCallback: Starting epoch {epoch + 1}\")\n",
    "        \n",
    "    # def on_train_batch_begin(self, batch, logs=None):\n",
    "    #     # Just to prove we're being called\n",
    "    #     if batch % 50 == 0:  # Print every 50 batches to avoid spam\n",
    "    #         print(f\"⚡ GradientMonitoringCallback: Batch {batch} starting\")\n",
    "        \n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        self.batch_count += 1\n",
    "        self.total_calls += 1\n",
    "        \n",
    "        # Print every time to show we're being called\n",
    "        # if batch % 50 == 0:  # Print every 50 batches\n",
    "            # print(f\"📊 GradientMonitoringCallback: Batch {batch} ended (total calls: {self.total_calls})\")\n",
    "        \n",
    "        # Only monitor every N batches to avoid performance overhead\n",
    "        if self.batch_count % self.monitor_frequency != 0:\n",
    "            return\n",
    "            \n",
    "        # print(f\"🔍 GradientMonitoringCallback: Checking gradients at batch {batch} (check #{self.gradient_checks + 1})\")\n",
    "        \n",
    "        # Get gradients from the optimizer's current state\n",
    "        try:\n",
    "            # Access the model's optimizer to get gradient information\n",
    "            optimizer = self.model.optimizer\n",
    "            print(f\"   📋 Optimizer type: {type(optimizer).__name__}\")\n",
    "            \n",
    "            # Get trainable variables\n",
    "            trainable_vars = self.model.trainable_variables\n",
    "            print(f\"   📈 Number of trainable variables: {len(trainable_vars)}\")\n",
    "            \n",
    "            if hasattr(optimizer, 'get_gradients'):\n",
    "                print(\"   ✅ Optimizer has get_gradients method\")\n",
    "                # For some optimizers, we can access gradients directly\n",
    "                grads = optimizer.get_gradients(self.model.total_loss, trainable_vars)\n",
    "                print(f\"   📊 Retrieved {len([g for g in grads if g is not None])} gradients\")\n",
    "            else:\n",
    "                print(\"   ❌ Optimizer doesn't have get_gradients, using variable norms\")\n",
    "                # Alternative approach: check the current variable states\n",
    "                grad_norms = []\n",
    "                for i, var in enumerate(trainable_vars):\n",
    "                    if var is not None:\n",
    "                        var_norm = tf.norm(var)\n",
    "                        grad_norms.append(var_norm)\n",
    "                        if i < 3:  # Print first 3 for debugging\n",
    "                            print(f\"      Variable {i} norm: {float(var_norm.numpy()):.2e}\")\n",
    "                \n",
    "                self._check_norms(grad_norms, \"Variable\")\n",
    "                self.gradient_checks += 1\n",
    "                return\n",
    "                \n",
    "            # Compute gradient norms\n",
    "            grad_norms = []\n",
    "            for i, grad in enumerate(grads):\n",
    "                if grad is not None:\n",
    "                    grad_norm = tf.norm(grad)\n",
    "                    grad_norms.append(grad_norm)\n",
    "                    if i < 3:  # Print first 3 for debugging\n",
    "                        print(f\"      Gradient {i} norm: {float(grad_norm.numpy()):.2e}\")\n",
    "                    \n",
    "            print(f\"   ✅ Computed {len(grad_norms)} gradient norms\")\n",
    "            self._check_norms(grad_norms, \"Gradient\")\n",
    "            self.gradient_checks += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Exception in gradient monitoring: {str(e)}\")\n",
    "            self.fallback_calls += 1\n",
    "            # Fallback: just monitor the loss for signs of instability\n",
    "            print('   🔄 Fallback: monitoring loss only')\n",
    "            if logs:\n",
    "                loss_value = logs.get('loss', 0)\n",
    "                print(f\"   📉 Current loss: {loss_value:.2e}\")\n",
    "                if np.isnan(loss_value) or np.isinf(loss_value):\n",
    "                    print(f\"   ⚠️  WARNING: Loss became {loss_value} at batch {batch}\")\n",
    "                elif loss_value > 1e6:\n",
    "                    print(f\"   ⚠️  WARNING: Very large loss {loss_value:.2e} at batch {batch}\")\n",
    "    \n",
    "    def _check_norms(self, norms, norm_type=\"Gradient\"):\n",
    "        \"\"\"Check if norms are within acceptable range\"\"\"\n",
    "        print(f\"   🔬 Checking {len(norms)} {norm_type.lower()} norms...\")\n",
    "        warnings = 0\n",
    "        \n",
    "        for idx, norm in enumerate(norms):\n",
    "            try:\n",
    "                norm_value = float(norm.numpy()) if hasattr(norm, 'numpy') else float(norm)\n",
    "                \n",
    "                if norm_value > self.clip_max:\n",
    "                    print(f\"   ⚠️  WARNING: {norm_type} norm {norm_value:.2e} is too large (layer {idx})\")\n",
    "                    warnings += 1\n",
    "                elif norm_value < self.clip_min:\n",
    "                    print(f\"   ⚠️  WARNING: {norm_type} norm {norm_value:.2e} is too small (layer {idx})\")\n",
    "                    warnings += 1\n",
    "                elif np.isnan(norm_value) or np.isinf(norm_value):\n",
    "                    print(f\"   ⚠️  WARNING: {norm_type} norm is {norm_value} (layer {idx})\")\n",
    "                    warnings += 1\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"   ❌ Cannot convert norm to float for layer {idx}: {str(e)}\")\n",
    "                continue\n",
    "                \n",
    "        if warnings == 0:\n",
    "            print(f\"   ✅ All {norm_type.lower()} norms are within acceptable range\")\n",
    "        else:\n",
    "            print(f\"   ⚠️  Found {warnings} norm warnings\")\n",
    "    \n",
    "    # def on_epoch_end(self, epoch, logs=None):\n",
    "    #     \"\"\"Print summary at end of each epoch\"\"\"\n",
    "    #     print(f\"📈 GradientMonitoringCallback: Epoch {epoch + 1} completed\")\n",
    "    #     print(f\"   📊 Total batch calls: {self.total_calls}\")\n",
    "    #     print(f\"   🔍 Gradient checks performed: {self.gradient_checks}\")\n",
    "    #     print(f\"   🔄 Fallback calls: {self.fallback_calls}\")\n",
    "    #     \n",
    "    #     if logs:\n",
    "    #         loss = logs.get('loss', 0)\n",
    "    #         val_loss = logs.get('val_loss', 0)\n",
    "    #         print(f\"   📉 Final epoch loss: {loss:.2e}\")\n",
    "    #         if val_loss:\n",
    "    #             print(f\"   📉 Final epoch val_loss: {val_loss:.2e}\")\n",
    "    #         \n",
    "    #         if np.isnan(loss) or np.isinf(loss):\n",
    "    #             print(f\"   ⚠️  WARNING: Training loss became unstable: {loss}\")\n",
    "    #         if val_loss and (np.isnan(val_loss) or np.isinf(val_loss)):\n",
    "    #             print(f\"   ⚠️  WARNING: Validation loss became unstable: {val_loss}\")\n",
    "        \n",
    "    def on_train_end(self, logs=None):\n",
    "        print(\"🏁 GradientMonitoringCallback: Training completed!\")\n",
    "        print(f\"   📊 Final stats - Total calls: {self.total_calls}, Gradient checks: {self.gradient_checks}, Fallbacks: {self.fallback_calls}\")\n",
    "        \n",
    "        if self.total_calls == 0:\n",
    "            print(\"   ❌ ERROR: Callback was never called! Check if it's properly added to callbacks list.\")\n",
    "        elif self.gradient_checks == 0 and self.fallback_calls == 0:\n",
    "            print(\"   ⚠️  WARNING: No gradient monitoring was performed. Check monitor_frequency setting.\")\n",
    "        else:\n",
    "            print(\"   ✅ Gradient monitoring completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d3c3926a64b326d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T20:35:32.305413Z",
     "start_time": "2025-05-29T20:35:32.301588Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from keras.src.callbacks import LearningRateScheduler, EarlyStopping, Callback\n",
    "from keras.src.optimizers import Adam\n",
    "from keras import Model\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def exponential_decay_schedule(epoch, lr):\n",
    "    decay_rate = 0.9\n",
    "    decay_steps = 5\n",
    "    if epoch % decay_steps == 0 and epoch:\n",
    "        print('Learning rate update:', lr * decay_rate)\n",
    "        return lr * decay_rate\n",
    "    return lr\n",
    "\n",
    "\n",
    "# Custom callback to monitor LR and stop training\n",
    "class LRThresholdCallback(Callback):\n",
    "    def __init__(self, threshold=9e-5):\n",
    "        super().__init__()\n",
    "        self.threshold = threshold\n",
    "        self.should_stop = False\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        lr = float(self.model.optimizer.learning_rate.numpy())\n",
    "        if lr < self.threshold:\n",
    "            print(f\"\\nLearning rate {lr:.6f} < threshold {self.threshold}, moving to next phase.\")\n",
    "            self.model.stop_training = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5af39153df21a474",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T20:35:32.439120Z",
     "start_time": "2025-05-29T20:35:32.435880Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class DynamicReduceLROnPlateau(Callback):\n",
    "    def __init__(self, factor=0.1, patience=3, min_lr=1e-7, verbose=1):\n",
    "        super().__init__()\n",
    "        self.factor = factor\n",
    "        self.patience = patience\n",
    "        self.min_lr = min_lr\n",
    "        self.verbose = verbose\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.wait = 0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        current_val_loss = logs.get(\"val_loss\")\n",
    "\n",
    "        if current_val_loss is None:\n",
    "            return  # can't do anything if val_loss isn't available\n",
    "\n",
    "        if current_val_loss < self.best_val_loss - 1e-4:  # a small delta\n",
    "            self.best_val_loss = current_val_loss\n",
    "            self.wait = 0\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                old_lr = float(self.model.optimizer.learning_rate.numpy())\n",
    "                if old_lr > self.min_lr:\n",
    "                    new_lr = max(old_lr * self.factor, self.min_lr)\n",
    "                    self.model.optimizer.learning_rate.assign(new_lr)\n",
    "                    if self.verbose:\n",
    "                        print(f\"\\nEpoch {epoch+1}: val_loss did not improve. Reducing LR from {old_lr:.6f} to {new_lr:.6f}\")\n",
    "                    self.wait = 0  # reset after LR reduction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f08667de7129e4e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T20:35:32.581507Z",
     "start_time": "2025-05-29T20:35:32.576019Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.models import save_model\n",
    "\n",
    "class SaveBestModelCallback(Callback):\n",
    "    def __init__(self, save_path='best_model', monitor='val_loss'):\n",
    "        super().__init__()\n",
    "        self.best = float('inf')\n",
    "        self.monitor = monitor\n",
    "        self.save_path = save_path\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current = logs.get(self.monitor)\n",
    "        if current is not None and current < self.best:\n",
    "            self.best = current\n",
    "            print(f\"\\nNew best {self.monitor}: {current:.6f}. Saving model...\")\n",
    "            self.model.save(self.save_path+'.keras', overwrite=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea4e240971bc7867",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T20:35:32.873166Z",
     "start_time": "2025-05-29T20:35:32.860029Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def train_model(train_data, batch_size=32, validation_split=0.2, Tobs=50, Tpred=60, epochs1=50, epochs2=50, lr1=0.001, lr2=0.000001):\n",
    "    n_scenarios = train_data.shape[0]\n",
    "    n_agents = train_data.shape[1]\n",
    "    X_train_raw = []\n",
    "    y_train_deltas = []\n",
    "\n",
    "    # Counters for pruning reasons\n",
    "    pruned_zero_frame = 0\n",
    "    pruned_observed_or_future_zero = 0\n",
    "    total_agents = n_scenarios * n_agents\n",
    "    \n",
    "    for i in range(n_scenarios):\n",
    "        for agent_id in range(2): #foremerly n_agents\n",
    "            agent_data = train_data[i, agent_id, :, :]  # shape (110, 6)\n",
    "        \n",
    "            observed = agent_data[:Tobs]         # shape (Tobs, 6)\n",
    "            future = agent_data[Tobs:Tobs + Tpred, :2]  # position_x, position_y\n",
    "            last_obs_pos = observed[-1, :2]\n",
    "        \n",
    "            # Skip if more than 20% of observed or future rows are all zeros\n",
    "            observed_zero_ratio = np.mean(np.all(observed == 0, axis=1))\n",
    "            future_zero_ratio = np.mean(np.all(future == 0, axis=1))\n",
    "            \n",
    "            if observed_zero_ratio > 0.2 or future_zero_ratio > 0.2:\n",
    "                pruned_observed_or_future_zero += 1\n",
    "                continue\n",
    "                \n",
    "            # Compute deltas w.r.t. previous future timestep\n",
    "            delta = np.diff(np.vstack([last_obs_pos, future]), axis=0)  # (60, 2)\n",
    "            \n",
    "            # prediction of future steps for agent\n",
    "            X_train_raw.append(observed)\n",
    "            y_train_deltas.append(delta)\n",
    "            \n",
    "    \n",
    "    # Print pruning summary\n",
    "    print(f\"Total agents: {total_agents}\")\n",
    "    print(f\"Pruned due to zero frame in Tobs+Tpred: {pruned_zero_frame}\")\n",
    "    print(f\"Pruned due to zero frame in observed or future window: {pruned_observed_or_future_zero}\")\n",
    "    print(f\"Remaining valid agents: {len(X_train_raw)}\")\n",
    "    \n",
    "    \n",
    "    X_train = np.array(X_train_raw)     # shape (N_valid, Tobs, 6)\n",
    "    y_train = np.array(y_train_deltas)  # shape (N_valid, Tpred, 2)\n",
    "    \n",
    "    # how much of the data is 0?\n",
    "    # For X_train\n",
    "    num_elements_X = X_train.size\n",
    "    num_zeros_X = np.count_nonzero(X_train == 0)\n",
    "    percent_zeros_X = 100 * num_zeros_X / num_elements_X\n",
    "    \n",
    "    # For y_train\n",
    "    num_elements_y = y_train.size\n",
    "    num_zeros_y = np.count_nonzero(y_train == 0)\n",
    "    percent_zeros_y = 100 * num_zeros_y / num_elements_y\n",
    "    \n",
    "    print(f\"X_train: {num_zeros_X} zeros out of {num_elements_X} elements ({percent_zeros_X:.2f}%)\")\n",
    "    print(f\"y_train: {num_zeros_y} zeros out of {num_elements_y} elements ({percent_zeros_y:.2f}%)\")\n",
    "    \n",
    "    # print(f\"ex. y_train {y_train[0]}\")\n",
    "\n",
    "\n",
    "    print(f\"Training on {X_train.shape[0]} valid agent trajectories.\")\n",
    "    print(f\"Input shape: {X_train.shape}, Delta Output shape: {y_train.shape}\")\n",
    "    \n",
    "     # --- Normalize Input and Output ---\n",
    "    X_mean = X_train.mean(axis=(0, 1), keepdims=True)  # shape: (1, 1, 6)\n",
    "    X_std = X_train.std(axis=(0, 1), keepdims=True) + 1e-8\n",
    "\n",
    "    y_mean = y_train.mean(axis=(0, 1), keepdims=True)  # shape: (1, 1, 2)\n",
    "    y_std = y_train.std(axis=(0, 1), keepdims=True) + 1e-8\n",
    "\n",
    "    X_train = (X_train - X_mean) / X_std\n",
    "    y_train = (y_train - y_mean) / y_std\n",
    "    \n",
    "    # X_mean, X_std, y_mean, y_std = None, None, None, None\n",
    "    \n",
    "    \n",
    "    # print(X_train[:2])\n",
    "    # print(y_train[:2])\n",
    "    \n",
    "    model = create_lstm_encoder_decoder_with_attention(\n",
    "        input_dim=X_train.shape[-1],\n",
    "        output_dim=2,\n",
    "        timesteps_in=Tobs,\n",
    "        timesteps_out=Tpred,\n",
    "        loss_fn='mse',\n",
    "        lr=lr1\n",
    "    )\n",
    "    \n",
    "    gradient_monitoring_callback = GradientMonitoringCallback(clip_min=1e-4, clip_max=1e2)\n",
    "    \n",
    "    # Pass normalization parameters to SaveBestModelCallback\n",
    "    save_best_callback = SaveBestModelCallback(\n",
    "        save_path='lstm2', \n",
    "        monitor='val_loss',\n",
    "    )\n",
    "\n",
    "\n",
    "    phase1_callbacks = [\n",
    "        # LearningRateScheduler(exponential_decay_schedule),\n",
    "        DynamicReduceLROnPlateau(factor=0.7, patience=3, min_lr=1e-9),\n",
    "        EarlyStopping(patience=5, restore_best_weights=True, monitor='val_loss'),\n",
    "        LRThresholdCallback(threshold=9e-8),\n",
    "        # gradient_monitoring_callback,\n",
    "        save_best_callback\n",
    "    ]\n",
    "\n",
    "    print(\"\\n--- Phase 1: Training ---\")\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=epochs1,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=validation_split,\n",
    "        callbacks=phase1_callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- Phase 2: Fine-tuning ---\")\n",
    "    model.compile(\n",
    "        optimizer=Adam(\n",
    "            learning_rate=lr2,\n",
    "            clipnorm=0.1,      # More aggressive clipping\n",
    "            beta_1=0.9,         # Standard momentum\n",
    "            beta_2=0.999,       # Standard RMSprop decay\n",
    "            epsilon=1e-7        # Smaller epsilon for stability\n",
    "        ),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    phase2_callbacks = [\n",
    "        # LearningRateScheduler(exponential_decay_schedule),\n",
    "        EarlyStopping(patience=3, restore_best_weights=True, monitor='val_loss'), \n",
    "        DynamicReduceLROnPlateau(factor=0.5, patience=2, min_lr=1e-9),\n",
    "        LRThresholdCallback(threshold=9e-8),\n",
    "        # gradient_monitoring_callback\n",
    "    ]\n",
    "    \n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=epochs2,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=validation_split,\n",
    "        callbacks=phase2_callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(f\"X_mean:{X_mean}, X_std:{X_std}, y_mean:{y_mean}, y_std:{y_std}\")\n",
    "\n",
    "    # Return model and normalization parameters\n",
    "    return model, X_mean, X_std, y_mean, y_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1dae50c6d460015",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T20:35:33.290708Z",
     "start_time": "2025-05-29T20:35:33.284273Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_mae_by_timestep(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Visualize MAE across timesteps in the prediction horizon.\n",
    "    \n",
    "    Args:\n",
    "        y_true (np.ndarray): shape (N, Tpred, 2)\n",
    "        y_pred (np.ndarray): shape (N, Tpred, 2)\n",
    "    \"\"\"\n",
    "    mae_per_timestep = np.mean(np.abs(y_true - y_pred), axis=(0, 2))  # shape (Tpred,)\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(mae_per_timestep, label='MAE per Timestep')\n",
    "    plt.xlabel('Timestep')\n",
    "    plt.ylabel('MAE (meters)')\n",
    "    plt.title('Mean Absolute Error Over Prediction Horizon')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f303823d230afdb9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T20:35:33.427413Z",
     "start_time": "2025-05-29T20:35:33.423950Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def reconstruct_absolute_positions(pred_deltas, last_observed_positions):\n",
    "    \"\"\"\n",
    "    Reconstruct absolute predicted positions in an autoregressive way.\n",
    "    \n",
    "    Args:\n",
    "        pred_deltas: np.ndarray of shape (N, Tpred, 2)\n",
    "        last_observed_positions: np.ndarray of shape (N, 2)\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray of shape (N, Tpred, 2)\n",
    "    \"\"\"\n",
    "    return last_observed_positions[:, None, :] + np.cumsum(pred_deltas, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10132f4e6ee1e1df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T20:35:33.769461Z",
     "start_time": "2025-05-29T20:35:33.765198Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def forecast_positions(scenario_data, Tobs, Tpred, model, X_mean=None, X_std=None, y_mean=None, y_std=None):\n",
    "    \"\"\"\n",
    "    Use LSTM model to forecast future deltas and reconstruct absolute positions of only the ego\n",
    "    Applies normalization only if statistics are provided.\n",
    "\n",
    "    Args:\n",
    "        scenario_data (numpy.ndarray): Shape (agents, time_steps, dimensions)\n",
    "        Tobs (int): Number of observed time steps\n",
    "        Tpred (int): Number of future time steps to predict\n",
    "        model (Model): Trained LSTM model\n",
    "        X_mean, X_std: Optional normalization stats for input\n",
    "        y_mean, y_std: Optional normalization stats for output\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Predicted absolute positions of shape (Tpred, 2)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Only process ego agent (agent_index=0)\n",
    "    agent_idx = 0\n",
    "    agent_data = scenario_data[agent_idx, :Tobs, :].copy()  # shape (Tobs, 6)\n",
    "    \n",
    "    # Skip if fully padded\n",
    "    if np.all(agent_data == 0):\n",
    "        return np.zeros((Tpred, 2))\n",
    "    \n",
    "    X_pred = np.expand_dims(agent_data, axis=0)  # shape (1, Tobs, 6)\n",
    "\n",
    "    # Normalize if stats are provided\n",
    "    if X_mean is not None and X_std is not None:\n",
    "        X_pred = (X_pred - X_mean) / X_std\n",
    "\n",
    "    # Predict deltas (normalized or raw)\n",
    "    pred_deltas = model.predict(X_pred, verbose=0)  # shape (1, Tpred, 2)\n",
    "    \n",
    "    # print(\"pred deltas\")\n",
    "    # print(pred_deltas[:,:])\n",
    "\n",
    "    # Denormalize if stats are provided\n",
    "    if y_mean is not None and y_std is not None:\n",
    "        pred_deltas = pred_deltas * y_std + y_mean\n",
    "\n",
    "    # Reconstruct absolute positions\n",
    "    last_pos = agent_data[Tobs - 1, :2]  # shape (2,)\n",
    "    abs_positions = reconstruct_absolute_positions(\n",
    "        pred_deltas=pred_deltas,\n",
    "        last_observed_positions=np.expand_dims(last_pos, axis=0)\n",
    "    )[0]\n",
    "    \n",
    "    return abs_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e475f845b5d73bae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T20:35:34.135865Z",
     "start_time": "2025-05-29T20:35:34.109898Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "def make_gif(data_matrix1, data_matrix2, name='comparison'):\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.animation as animation\n",
    "\n",
    "    cmap1 = plt.cm.get_cmap('viridis', 50)\n",
    "    cmap2 = plt.cm.get_cmap('plasma', 50)\n",
    "\n",
    "    assert data_matrix1.shape[1] == data_matrix2.shape[1], \"Both matrices must have same number of timesteps\"\n",
    "    timesteps = data_matrix1.shape[1]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(18, 9))\n",
    "    ax1, ax2 = axes\n",
    "\n",
    "    def update(frame):\n",
    "        for ax in axes:\n",
    "            ax.clear()\n",
    "\n",
    "        for i in range(data_matrix1.shape[0]):\n",
    "            for (data_matrix, ax, cmap) in [(data_matrix1, ax1, cmap1), (data_matrix2, ax2, cmap2)]:\n",
    "                x = data_matrix[i, frame, 0]\n",
    "                y = data_matrix[i, frame, 1]\n",
    "                if x != 0 and y != 0:\n",
    "                    xs = data_matrix[i, :frame+1, 0]\n",
    "                    ys = data_matrix[i, :frame+1, 1]\n",
    "                    mask = (xs != 0) & (ys != 0)\n",
    "                    xs = xs[mask]\n",
    "                    ys = ys[mask]\n",
    "                    if len(xs) > 0 and len(ys) > 0:\n",
    "                        color = cmap(i)\n",
    "                        ax.plot(xs, ys, alpha=0.9, color=color)\n",
    "                        ax.scatter(x, y, s=80, color=color)\n",
    "\n",
    "        # Plot ego vehicle (index 0) on both\n",
    "        ax1.plot(data_matrix1[0, :frame, 0], data_matrix1[0, :frame, 1], color='tab:orange', label='Ego Vehicle')\n",
    "        ax1.scatter(data_matrix1[0, frame, 0], data_matrix1[0, frame, 1], s=80, color='tab:orange')\n",
    "        ax1.set_title('Prediction')\n",
    "\n",
    "        ax2.plot(data_matrix2[0, :frame, 0], data_matrix2[0, :frame, 1], color='tab:orange', label='Ego Vehicle')\n",
    "        ax2.scatter(data_matrix2[0, frame, 0], data_matrix2[0, frame, 1], s=80, color='tab:orange')\n",
    "        ax2.set_title('Actual')\n",
    "\n",
    "        for ax, data_matrix in zip(axes, [data_matrix1, data_matrix2]):\n",
    "            ax.set_xlim(data_matrix[:, :, 0][data_matrix[:, :, 0] != 0].min() - 10,\n",
    "                        data_matrix[:, :, 0][data_matrix[:, :, 0] != 0].max() + 10)\n",
    "            ax.set_ylim(data_matrix[:, :, 1][data_matrix[:, :, 1] != 0].min() - 10,\n",
    "                        data_matrix[:, :, 1][data_matrix[:, :, 1] != 0].max() + 10)\n",
    "            ax.legend()\n",
    "            ax.set_xlabel('X')\n",
    "            ax.set_ylabel('Y')\n",
    "\n",
    "        # Compute MSE over non-zero entries up to current frame\n",
    "        mask = (data_matrix2[:, :frame+1, :] != 0) & (data_matrix1[:, :frame+1, :] != 0)\n",
    "        mse = np.mean((data_matrix1[:, :frame+1, :][mask] - data_matrix2[:, :frame+1, :][mask]) ** 2)\n",
    "\n",
    "        fig.suptitle(f\"Timestep {frame} - MSE: {mse:.4f}\", fontsize=16)\n",
    "        return ax1.collections + ax1.lines + ax2.collections + ax2.lines\n",
    "\n",
    "    anim = animation.FuncAnimation(fig, update, frames=list(range(0, timesteps, 3)), interval=100, blit=True)\n",
    "    anim.save(f'trajectory_visualization_{name}.gif', writer='pillow')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac3b5ab02eaf908a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T20:35:34.469765Z",
     "start_time": "2025-05-29T20:35:34.464024Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9998 valid sequences.\n",
      "Input shape: (9998, 50, 6), Delta Output shape: (9998, 60, 2)\n"
     ]
    }
   ],
   "source": [
    "# figure out stats\n",
    "def stats():\n",
    "    Tobs = 50\n",
    "    Tpred = 60\n",
    "    n_scenarios = standardized_train_data.shape[0]\n",
    "    X_train_raw = []\n",
    "    y_train_deltas = []\n",
    "\n",
    "    for i in range(n_scenarios):\n",
    "        ego_data = standardized_train_data[i, 0, :, :]\n",
    "        if np.all(ego_data == 0):\n",
    "            continue\n",
    "\n",
    "        observed = ego_data[:Tobs]            # shape (50, 6)\n",
    "        future = ego_data[Tobs:Tobs+Tpred, :2]\n",
    "        last_obs_pos = observed[-1, :2]\n",
    "\n",
    "        if np.any(np.all(observed == 0, axis=1)) or np.any(np.all(future == 0, axis=1)):\n",
    "            continue\n",
    "\n",
    "        # Compute deltas w.r.t. previous future timestep\n",
    "        delta = np.diff(np.vstack([last_obs_pos, future]), axis=0)  # (60, 2)\n",
    "\n",
    "        X_train_raw.append(observed)\n",
    "        y_train_deltas.append(delta)\n",
    "\n",
    "\n",
    "    X_train = np.array(X_train_raw)\n",
    "    y_train = np.array(y_train_deltas)\n",
    "\n",
    "    print(f\"{X_train.shape[0]} valid sequences.\")\n",
    "    print(f\"Input shape: {X_train.shape}, Delta Output shape: {y_train.shape}\")\n",
    "\n",
    "    # --- Normalize Input and Output ---\n",
    "    X_mean = X_train.mean(axis=(0, 1), keepdims=True)  # shape: (1, 1, 6)\n",
    "    X_std = X_train.std(axis=(0, 1), keepdims=True) + 1e-8\n",
    "\n",
    "    y_mean = y_train.mean(axis=(0, 1), keepdims=True)  # shape: (1, 1, 2)\n",
    "    y_std = y_train.std(axis=(0, 1), keepdims=True) + 1e-8\n",
    "\n",
    "    X_train = (X_train - X_mean) / X_std\n",
    "    y_train = (y_train - y_mean) / y_std\n",
    "    return X_mean, X_std, y_mean, y_std\n",
    "X_mean, X_std, y_mean, y_std = stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1dfecba2866a2d9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T20:35:35.378237Z",
     "start_time": "2025-05-29T20:35:35.373204Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7721496a026fc273",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T20:35:37.227850Z",
     "start_time": "2025-05-29T20:35:35.499244Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 22:56:27.291621: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9804 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:60:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "model = create_lstm_encoder_decoder_with_attention(10000,2,50,60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "49f18bb60147110c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T20:35:37.763357Z",
     "start_time": "2025-05-29T20:35:37.750070Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 50, 10000)]          0         []                            \n",
      "                                                                                                  \n",
      " encoder_lstm_0 (LSTM)       (None, 50, 512)              2153062   ['input_1[0][0]']             \n",
      "                                                          4                                       \n",
      "                                                                                                  \n",
      " encoder_lstm_1 (LSTM)       (None, 50, 512)              2099200   ['encoder_lstm_0[0][0]']      \n",
      "                                                                                                  \n",
      " encoder_lstm_2 (LSTM)       (None, 50, 512)              2099200   ['encoder_lstm_1[0][0]']      \n",
      "                                                                                                  \n",
      " lambda (Lambda)             (None, 512)                  0         ['encoder_lstm_2[0][0]']      \n",
      "                                                                                                  \n",
      " repeat_vector (RepeatVecto  (None, 60, 512)              0         ['lambda[0][0]']              \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " decoder_lstm_0 (LSTM)       (None, 60, 512)              2099200   ['repeat_vector[0][0]']       \n",
      "                                                                                                  \n",
      " decoder_lstm_1 (LSTM)       (None, 60, 512)              2099200   ['decoder_lstm_0[0][0]']      \n",
      "                                                                                                  \n",
      " decoder_lstm_2 (LSTM)       (None, 60, 512)              2099200   ['decoder_lstm_1[0][0]']      \n",
      "                                                                                                  \n",
      " attention (Attention)       (None, 60, 512)              0         ['decoder_lstm_2[0][0]',      \n",
      "                                                                     'encoder_lstm_2[0][0]']      \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 60, 1024)             0         ['decoder_lstm_2[0][0]',      \n",
      "                                                                     'attention[0][0]']           \n",
      "                                                                                                  \n",
      " time_distributed (TimeDist  (None, 60, 128)              131200    ['concatenate[0][0]']         \n",
      " ributed)                                                                                         \n",
      "                                                                                                  \n",
      " time_distributed_1 (TimeDi  (None, 60, 64)               8256      ['time_distributed[0][0]']    \n",
      " stributed)                                                                                       \n",
      "                                                                                                  \n",
      " time_distributed_2 (TimeDi  (None, 60, 2)                130       ['time_distributed_1[0][0]']  \n",
      " stributed)                                                                                       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 32166210 (122.70 MB)\n",
      "Trainable params: 32166210 (122.70 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "71d257d5cc4ad709",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-05-29T20:31:45.325892Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total agents: 500000\n",
      "Pruned due to zero frame in Tobs+Tpred: 0\n",
      "Pruned due to zero frame in observed or future window: 1\n",
      "Remaining valid agents: 19999\n",
      "X_train: 947758 zeros out of 5999700 elements (15.80%)\n",
      "y_train: 0 zeros out of 2399880 elements (0.00%)\n",
      "Training on 19999 valid agent trajectories.\n",
      "Input shape: (19999, 50, 6), Delta Output shape: (19999, 60, 2)\n",
      "🔧 GradientMonitoringCallback initialized with clip_min=0.0001, clip_max=100.0, monitor_freq=3\n",
      "\n",
      "--- Phase 1: Training ---\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1748559398.787330 3127847 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"NVIDIA GeForce RTX 2080 Ti\" frequency: 1545 num_cores: 68 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"12020\" } environment { key: \"cudnn\" value: \"8904\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 5767168 shared_memory_size_per_multiprocessor: 65536 memory_size: 10280763392 bandwidth: 616000000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n",
      "2025-05-29 22:56:39.438326: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - ETA: 0s - loss: 0.2270 - mae: 0.3035"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1748559424.567212 3127847 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"NVIDIA GeForce RTX 2080 Ti\" frequency: 1545 num_cores: 68 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"12020\" } environment { key: \"cudnn\" value: \"8904\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 5767168 shared_memory_size_per_multiprocessor: 65536 memory_size: 10280763392 bandwidth: 616000000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best val_loss: 0.163348. Saving model...\n",
      "500/500 [==============================] - 35s 53ms/step - loss: 0.2270 - mae: 0.3035 - val_loss: 0.1633 - val_mae: 0.2464\n",
      "Epoch 2/50\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.1476 - mae: 0.2354\n",
      "New best val_loss: 0.149131. Saving model...\n",
      "500/500 [==============================] - 25s 49ms/step - loss: 0.1475 - mae: 0.2354 - val_loss: 0.1491 - val_mae: 0.2301\n",
      "Epoch 3/50\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.1299 - mae: 0.2176\n",
      "New best val_loss: 0.122220. Saving model...\n",
      "500/500 [==============================] - 25s 50ms/step - loss: 0.1298 - mae: 0.2176 - val_loss: 0.1222 - val_mae: 0.2121\n",
      "Epoch 4/50\n",
      "500/500 [==============================] - ETA: 0s - loss: 0.1183 - mae: 0.2060\n",
      "New best val_loss: 0.120806. Saving model...\n",
      "500/500 [==============================] - 25s 50ms/step - loss: 0.1183 - mae: 0.2060 - val_loss: 0.1208 - val_mae: 0.2091\n",
      "Epoch 5/50\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.1149 - mae: 0.2027\n",
      "New best val_loss: 0.113386. Saving model...\n",
      "500/500 [==============================] - 26s 53ms/step - loss: 0.1149 - mae: 0.2027 - val_loss: 0.1134 - val_mae: 0.1994\n",
      "Epoch 6/50\n",
      "500/500 [==============================] - ETA: 0s - loss: 0.1075 - mae: 0.1941\n",
      "New best val_loss: 0.104720. Saving model...\n",
      "500/500 [==============================] - 26s 52ms/step - loss: 0.1075 - mae: 0.1941 - val_loss: 0.1047 - val_mae: 0.1919\n",
      "Epoch 7/50\n",
      "500/500 [==============================] - ETA: 0s - loss: 0.1060 - mae: 0.1935\n",
      "New best val_loss: 0.100634. Saving model...\n",
      "500/500 [==============================] - 26s 52ms/step - loss: 0.1060 - mae: 0.1935 - val_loss: 0.1006 - val_mae: 0.1835\n",
      "Epoch 8/50\n",
      "500/500 [==============================] - 25s 50ms/step - loss: 0.1014 - mae: 0.1888 - val_loss: 0.1071 - val_mae: 0.2026\n",
      "Epoch 9/50\n",
      "500/500 [==============================] - ETA: 0s - loss: 0.0977 - mae: 0.1856\n",
      "New best val_loss: 0.097270. Saving model...\n",
      "500/500 [==============================] - 26s 53ms/step - loss: 0.0977 - mae: 0.1856 - val_loss: 0.0973 - val_mae: 0.1811\n",
      "Epoch 10/50\n",
      "500/500 [==============================] - ETA: 0s - loss: 0.0936 - mae: 0.1798\n",
      "New best val_loss: 0.093423. Saving model...\n",
      "500/500 [==============================] - 26s 52ms/step - loss: 0.0936 - mae: 0.1798 - val_loss: 0.0934 - val_mae: 0.1768\n",
      "Epoch 11/50\n",
      "500/500 [==============================] - 26s 51ms/step - loss: 0.0927 - mae: 0.1796 - val_loss: 0.0971 - val_mae: 0.1844\n",
      "Epoch 12/50\n",
      "500/500 [==============================] - 25s 50ms/step - loss: 0.0966 - mae: 0.1844 - val_loss: 0.0981 - val_mae: 0.1854\n",
      "Epoch 13/50\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.0894 - mae: 0.1750\n",
      "New best val_loss: 0.088964. Saving model...\n",
      "500/500 [==============================] - 26s 53ms/step - loss: 0.0894 - mae: 0.1750 - val_loss: 0.0890 - val_mae: 0.1690\n",
      "Epoch 14/50\n",
      "500/500 [==============================] - 27s 53ms/step - loss: 0.0870 - mae: 0.1727 - val_loss: 0.0945 - val_mae: 0.1806\n",
      "Epoch 15/50\n",
      "500/500 [==============================] - 26s 53ms/step - loss: 0.0851 - mae: 0.1700 - val_loss: 0.0927 - val_mae: 0.1781\n",
      "Epoch 16/50\n",
      "500/500 [==============================] - ETA: 0s - loss: 0.0831 - mae: 0.1678\n",
      "Epoch 16: val_loss did not improve. Reducing LR from 0.001000 to 0.000700\n",
      "500/500 [==============================] - 26s 53ms/step - loss: 0.0831 - mae: 0.1678 - val_loss: 0.0915 - val_mae: 0.1775\n",
      "Epoch 17/50\n",
      "500/500 [==============================] - ETA: 0s - loss: 0.0773 - mae: 0.1601\n",
      "New best val_loss: 0.086027. Saving model...\n",
      "500/500 [==============================] - 27s 53ms/step - loss: 0.0773 - mae: 0.1601 - val_loss: 0.0860 - val_mae: 0.1683\n",
      "Epoch 18/50\n",
      "500/500 [==============================] - 26s 53ms/step - loss: 0.0767 - mae: 0.1590 - val_loss: 0.0862 - val_mae: 0.1704\n",
      "Epoch 19/50\n",
      "500/500 [==============================] - 27s 53ms/step - loss: 0.0756 - mae: 0.1582 - val_loss: 0.0860 - val_mae: 0.1696\n",
      "Epoch 20/50\n",
      "500/500 [==============================] - ETA: 0s - loss: 0.0745 - mae: 0.1570\n",
      "New best val_loss: 0.084388. Saving model...\n",
      "500/500 [==============================] - 26s 53ms/step - loss: 0.0745 - mae: 0.1570 - val_loss: 0.0844 - val_mae: 0.1640\n",
      "Epoch 21/50\n",
      "500/500 [==============================] - 26s 52ms/step - loss: 0.0737 - mae: 0.1564 - val_loss: 0.0846 - val_mae: 0.1645\n",
      "Epoch 22/50\n",
      "500/500 [==============================] - 26s 53ms/step - loss: 0.0724 - mae: 0.1551 - val_loss: 0.0848 - val_mae: 0.1628\n",
      "Epoch 23/50\n",
      "500/500 [==============================] - ETA: 0s - loss: 0.0708 - mae: 0.1525\n",
      "New best val_loss: 0.083351. Saving model...\n",
      "500/500 [==============================] - 26s 52ms/step - loss: 0.0708 - mae: 0.1525 - val_loss: 0.0834 - val_mae: 0.1600\n",
      "Epoch 24/50\n",
      "500/500 [==============================] - 25s 49ms/step - loss: 0.0704 - mae: 0.1529 - val_loss: 0.0853 - val_mae: 0.1649\n",
      "Epoch 25/50\n",
      "500/500 [==============================] - ETA: 0s - loss: 0.0698 - mae: 0.1524\n",
      "New best val_loss: 0.083139. Saving model...\n",
      "500/500 [==============================] - 26s 51ms/step - loss: 0.0698 - mae: 0.1524 - val_loss: 0.0831 - val_mae: 0.1629\n",
      "Epoch 26/50\n",
      "500/500 [==============================] - 26s 53ms/step - loss: 0.0692 - mae: 0.1514 - val_loss: 0.0870 - val_mae: 0.1696\n",
      "Epoch 27/50\n",
      "500/500 [==============================] - 26s 52ms/step - loss: 0.0684 - mae: 0.1502 - val_loss: 0.0895 - val_mae: 0.1752\n",
      "Epoch 28/50\n",
      "499/500 [============================>.] - ETA: 0s - loss: 0.0668 - mae: 0.1485\n",
      "Epoch 28: val_loss did not improve. Reducing LR from 0.000700 to 0.000490\n",
      "500/500 [==============================] - 26s 53ms/step - loss: 0.0669 - mae: 0.1485 - val_loss: 0.0862 - val_mae: 0.1698\n",
      "Epoch 29/50\n",
      "500/500 [==============================] - 27s 53ms/step - loss: 0.0631 - mae: 0.1423 - val_loss: 0.0846 - val_mae: 0.1628\n",
      "Epoch 30/50\n",
      "500/500 [==============================] - 26s 52ms/step - loss: 0.0608 - mae: 0.1400 - val_loss: 0.0839 - val_mae: 0.1597\n",
      "\n",
      "--- Phase 2: Fine-tuning ---\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1748560186.745032 3127847 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"NVIDIA GeForce RTX 2080 Ti\" frequency: 1545 num_cores: 68 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"12020\" } environment { key: \"cudnn\" value: \"8904\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 5767168 shared_memory_size_per_multiprocessor: 65536 memory_size: 10280763392 bandwidth: 616000000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - ETA: 0s - loss: 0.0616 - mae: 0.1394"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1748560212.992463 3127847 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"NVIDIA GeForce RTX 2080 Ti\" frequency: 1545 num_cores: 68 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"12020\" } environment { key: \"cudnn\" value: \"8904\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 5767168 shared_memory_size_per_multiprocessor: 65536 memory_size: 10280763392 bandwidth: 616000000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 35s 56ms/step - loss: 0.0616 - mae: 0.1394 - val_loss: 0.0793 - val_mae: 0.1545\n",
      "Epoch 2/10\n",
      "500/500 [==============================] - 27s 54ms/step - loss: 0.0599 - mae: 0.1358 - val_loss: 0.0791 - val_mae: 0.1538\n",
      "Epoch 3/10\n",
      "500/500 [==============================] - 27s 54ms/step - loss: 0.0591 - mae: 0.1346 - val_loss: 0.0790 - val_mae: 0.1534\n",
      "Epoch 4/10\n",
      "500/500 [==============================] - ETA: 0s - loss: 0.0585 - mae: 0.1338\n",
      "Epoch 4: val_loss did not improve. Reducing LR from 0.000010 to 0.000005\n",
      "500/500 [==============================] - 27s 54ms/step - loss: 0.0585 - mae: 0.1338 - val_loss: 0.0790 - val_mae: 0.1532\n",
      "Epoch 5/10\n",
      "500/500 [==============================] - 26s 53ms/step - loss: 0.0580 - mae: 0.1331 - val_loss: 0.0791 - val_mae: 0.1532\n",
      "Epoch 6/10\n",
      "500/500 [==============================] - ETA: 0s - loss: 0.0578 - mae: 0.1329\n",
      "Epoch 6: val_loss did not improve. Reducing LR from 0.000005 to 0.000002\n",
      "500/500 [==============================] - 26s 53ms/step - loss: 0.0578 - mae: 0.1329 - val_loss: 0.0792 - val_mae: 0.1531\n",
      "Epoch 7/10\n",
      "500/500 [==============================] - 27s 53ms/step - loss: 0.0576 - mae: 0.1325 - val_loss: 0.0792 - val_mae: 0.1531\n",
      "X_mean:[[[ 1.03108861e+02  9.60536785e+01 -8.67036949e-02 -4.36215841e-02\n",
      "   -1.24122434e-02  2.55912796e-01]]], X_std:[[[61.77947663 56.14216235  5.09055898  4.46388751  1.84648767\n",
      "    1.11174356]]], y_mean:[[[-0.01037113 -0.00533686]]], y_std:[[[0.50479868 0.4433503 ]]]\n"
     ]
    }
   ],
   "source": [
    "# fit\n",
    "model, X_mean, X_std, y_mean, y_std = train_model(standardized_train_data[:],epochs1=50, epochs2=10, validation_split=0.2, lr1=0.001, lr2=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b7209bcaafda7611",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-05-29T20:31:45.326465Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras model loaded from lstm2.keras\n"
     ]
    }
   ],
   "source": [
    "model = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "86ac55f8c53f2860",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T20:31:45.327927Z",
     "start_time": "2025-05-29T20:31:45.327309Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1748560380.343540 3127847 op_level_cost_estimator.cc:699] Error in PredictCost() for the op: op: \"Softmax\" attr { key: \"T\" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: \"GPU\" vendor: \"NVIDIA\" model: \"NVIDIA GeForce RTX 2080 Ti\" frequency: 1545 num_cores: 68 environment { key: \"architecture\" value: \"7.5\" } environment { key: \"cuda\" value: \"12020\" } environment { key: \"cudnn\" value: \"8904\" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 5767168 shared_memory_size_per_multiprocessor: 65536 memory_size: 10280763392 bandwidth: 616000000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 19.72889216 144.68795664]\n",
      " [ 19.71153917 144.23530889]\n",
      " [ 19.69581045 143.79343459]\n",
      " [ 19.67851866 143.3536886 ]\n",
      " [ 19.66052052 142.92034789]]\n",
      "[[ 19.71302446 144.75582367]\n",
      " [ 19.68203702 144.34848456]\n",
      " [ 19.65202753 143.95338695]\n",
      " [ 19.62305751 143.57145674]\n",
      " [ 19.59515847 143.20213145]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3127847/849000278.py:9: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  cmap1 = plt.cm.get_cmap('viridis', 50)\n",
      "/tmp/ipykernel_3127847/849000278.py:10: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  cmap2 = plt.cm.get_cmap('plasma', 50)\n"
     ]
    }
   ],
   "source": [
    "# visualize regular prediction\n",
    "\n",
    "# model = load_model()\n",
    "\n",
    "# Parameters\n",
    "Tobs = 50\n",
    "Tpred = 60\n",
    "\n",
    "data = standardized_train_data[6325]\n",
    "\n",
    "# Select a test scenario (can use any valid index)\n",
    "test_scenario = data.copy()  # shape (agents, time_steps, features)\n",
    "\n",
    "\n",
    "# Forecast future positions\n",
    "predicted_positions = forecast_positions(test_scenario, Tobs, Tpred, model, X_mean, X_std, y_mean, y_std)\n",
    "\n",
    "# Create combined matrix of past observed + predicted for ego agent (agent 0)\n",
    "ego_past = test_scenario[0, :Tobs, :2]               # shape (Tobs, 2)\n",
    "ego_future = predicted_positions                     # shape (Tpred, 2)\n",
    "\n",
    "print(ego_future[:5])\n",
    "print(test_scenario[0, Tobs:Tobs+5, :2])\n",
    "ego_full = np.concatenate([ego_past, ego_future], axis=0)  # shape (Tobs + Tpred, 2)\n",
    "\n",
    "# Create updated scenario with predicted ego and original others\n",
    "updated_scenario = test_scenario.copy()\n",
    "updated_scenario[0, :Tobs+Tpred, :2] = ego_full  # Replace ego trajectory\n",
    "\n",
    "# Visualize\n",
    "make_gif(updated_scenario, data, name='lstm2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "afd67ae1834bd217",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T20:31:45.328472Z",
     "start_time": "2025-05-29T20:31:45.328095Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# # visualize prediction\n",
    "# \n",
    "# # model = load_model()\n",
    "# \n",
    "# # Parameters\n",
    "# Tobs = 50\n",
    "# Tpred = 60\n",
    "# \n",
    "# data = train_data[0]\n",
    "# \n",
    "# # Select a test scenario (can use any valid index)\n",
    "# test_scenario = data.copy()  # shape (agents, time_steps, features)\n",
    "# \n",
    "# \n",
    "# # Forecast future positions\n",
    "# predicted_positions = finetune_forecast_positions(test_scenario, Tobs, Tpred, model, X_mean, X_std, y_mean, y_std)\n",
    "# \n",
    "# # Create combined matrix of past observed + predicted for ego agent (agent 0)\n",
    "# ego_past = test_scenario[0, :Tobs, :2]               # shape (Tobs, 2)\n",
    "# ego_future = predicted_positions[0]                  # shape (Tpred, 2)\n",
    "# ego_full = np.concatenate([ego_past, ego_future], axis=0)  # shape (Tobs + Tpred, 2)\n",
    "# \n",
    "# # Create updated scenario with predicted ego and original others\n",
    "# updated_scenario = test_scenario.copy()\n",
    "# updated_scenario[0, :Tobs+Tpred, :2] = ego_full  # Replace ego trajectory\n",
    "# \n",
    "# # Visualize\n",
    "# make_gif(updated_scenario, data, name='lstm2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11f6401b403c51b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-29T20:31:45.333420Z",
     "start_time": "2025-05-29T20:31:45.328675Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "def evaluate_mse(train_data, model, Tobs=50, Tpred=60):\n",
    "    \"\"\"\n",
    "    Computes LSTM prediction for ego agent and evaluates MSE with progress reporting.\n",
    "    \"\"\"\n",
    "    N = train_data.shape[0]\n",
    "    mse_list = []\n",
    "    valid_scenarios = 0\n",
    "    \n",
    "    print(f\"Evaluating {N} scenarios...\")\n",
    "    \n",
    "    # Progress reporting variables\n",
    "    report_interval = max(1, N // 10)  # Report at 10% intervals\n",
    "    \n",
    "    for i in range(N):\n",
    "        # Progress reporting\n",
    "        if i % report_interval == 0 or i == N-1:\n",
    "            print(f\"Processing scenario {i+1}/{N} ({(i+1)/N*100:.1f}%)\")\n",
    "        \n",
    "        scenario_data = train_data[i]\n",
    "        ego_agent_data = scenario_data[0]\n",
    "        ground_truth = ego_agent_data[Tobs:Tobs+Tpred, :2]\n",
    "        \n",
    "        # Skip if ground truth contains all zeros (padded)\n",
    "        if np.all(ground_truth == 0):\n",
    "            continue\n",
    "            \n",
    "        valid_scenarios += 1\n",
    "        \n",
    "        # Forecast future positions\n",
    "        predicted_positions = forecast_positions(\n",
    "            ego_agent_data[np.newaxis, :, :],\n",
    "            Tobs, Tpred, model, X_mean, X_std, y_mean, y_std\n",
    "        )\n",
    "        \n",
    "        # Compute MSE\n",
    "        mse = mean_squared_error(ground_truth, predicted_positions)\n",
    "        mse_list.append(mse)\n",
    "        \n",
    "        # Occasional MSE reporting\n",
    "        if i % report_interval == 0:\n",
    "            print(f\"  Current scenario MSE: {mse:.4f}\")\n",
    "    \n",
    "    # Final results\n",
    "    if mse_list:\n",
    "        overall_mse = np.mean(mse_list)\n",
    "        print(f\"Evaluation complete: {valid_scenarios} valid scenarios\")\n",
    "        print(f\"Mean Squared Error (MSE): {overall_mse:.4f}\")\n",
    "        print(f\"Min MSE: {np.min(mse_list):.4f}, Max MSE: {np.max(mse_list):.4f}\")\n",
    "        return overall_mse\n",
    "    else:\n",
    "        print(\"No valid scenarios for evaluation.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c75caee9a06e8acb",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-05-29T20:31:45.329244Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating 10000 scenarios...\n",
      "Processing scenario 1/10000 (0.0%)\n",
      "  Current scenario MSE: 3.0316\n",
      "Processing scenario 1001/10000 (10.0%)\n",
      "  Current scenario MSE: 1.6629\n",
      "Processing scenario 2001/10000 (20.0%)\n",
      "  Current scenario MSE: 0.0797\n",
      "Processing scenario 3001/10000 (30.0%)\n",
      "  Current scenario MSE: 2.5753\n",
      "Processing scenario 4001/10000 (40.0%)\n",
      "  Current scenario MSE: 0.3912\n",
      "Processing scenario 5001/10000 (50.0%)\n",
      "  Current scenario MSE: 29.0094\n",
      "Processing scenario 6001/10000 (60.0%)\n",
      "  Current scenario MSE: 29.8110\n",
      "Processing scenario 7001/10000 (70.0%)\n",
      "  Current scenario MSE: 17.9062\n",
      "Processing scenario 8001/10000 (80.0%)\n",
      "  Current scenario MSE: 7.8313\n",
      "Processing scenario 9001/10000 (90.0%)\n",
      "  Current scenario MSE: 0.1848\n",
      "Processing scenario 10000/10000 (100.0%)\n",
      "Evaluation complete: 10000 valid scenarios\n",
      "Mean Squared Error (MSE): 8.4197\n",
      "Min MSE: 0.0002, Max MSE: 239.4549\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8.419701296160717"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate on training data\n",
    "evaluate_mse(standardized_train_data, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "907c41122d6ed7b9",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-05-29T20:31:45.329765Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file 'lstm_attention_submission.csv' saved with shape (126000, 2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def generate_submission(data, output_csv, model, Tobs=50, Tpred=60, X_mean=None, X_std=None, y_mean=None, y_std=None):\n",
    "    \"\"\"\n",
    "    Applies standardization, forecasting, and de-standardization to generate\n",
    "    a submission CSV in the format: index,x,y\n",
    "    \n",
    "    Args:\n",
    "        data (np.ndarray): Test data of shape (num_scenarios, 50, 110, 6)\n",
    "        output_csv (str): Output CSV file path\n",
    "        model: Trained LSTM model\n",
    "        Tobs (int): Number of observed time steps\n",
    "        Tpred (int): Number of time steps to predict\n",
    "        X_mean, X_std: Optional normalization stats for input\n",
    "        y_mean, y_std: Optional normalization stats for output\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    for i in range(data.shape[0]):\n",
    "        scenario_data = data[i]  # Shape: (50, 110, 6)\n",
    "        \n",
    "        # Standardize coordinates\n",
    "        standardized_data, min_values = standardize_data_dimensions(scenario_data)\n",
    "        \n",
    "        # Extract ego agent data (agent 0) - transpose to get correct shape\n",
    "        ego_agent_data = standardized_data[:, 0, :]  # Shape: (50, 6)\n",
    "        ego_agent_data = ego_agent_data[np.newaxis, :, :]  # Shape: (1, 50, 6)\n",
    "        \n",
    "        # Predict future positions for the ego agent\n",
    "        predicted_positions = forecast_positions(\n",
    "            ego_agent_data, Tobs, Tpred, model, X_mean, X_std, y_mean, y_std\n",
    "        )  # Shape: (60, 2)\n",
    "        \n",
    "        # De-standardize predicted positions\n",
    "        predicted_positions[:, 0] += min_values[0]  # x\n",
    "        predicted_positions[:, 1] += min_values[1]  # y\n",
    "        \n",
    "        # Append to all predictions\n",
    "        predictions.extend(predicted_positions)\n",
    "    \n",
    "    # Create submission DataFrame\n",
    "    submission_df = pd.DataFrame(predictions, columns=[\"x\", \"y\"])\n",
    "    submission_df.index.name = 'index'\n",
    "    submission_df.to_csv(output_csv)\n",
    "    print(f\"Submission file '{output_csv}' saved with shape {submission_df.shape}\")\n",
    "\n",
    "# Example usage:\n",
    "generate_submission(test_data, 'lstm_attention_submission.csv', model, \n",
    "                   X_mean=X_mean, X_std=X_std, y_mean=y_mean, y_std=y_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cfeb58-4dc9-4c9c-8aa0-7b6c2e7a7597",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
