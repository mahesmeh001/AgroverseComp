{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "idea: fine tune on test scenario before doing prediction "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "69207995c2b68685"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mehul/Documents/UCSD Classes/Deep Learning/AgroverseComp/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices('GPU')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-25T19:47:25.363409Z",
     "start_time": "2025-05-25T19:47:21.910901Z"
    }
   },
   "id": "12ecbb45022667b6",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-25T19:47:26.568069Z",
     "start_time": "2025-05-25T19:47:25.362562Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data's shape (10000, 50, 110, 6)\n",
      "test_data's shape (2100, 50, 50, 6)\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "import numpy as np\n",
    "train_file = np.load('data/train.npz')\n",
    "train_data = train_file['data']\n",
    "print(\"train_data's shape\", train_data.shape)\n",
    "test_file = np.load('data/test_input.npz')\n",
    "test_data = test_file['data']\n",
    "print(\"test_data's shape\", test_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input, RepeatVector, TimeDistributed, Dropout"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-25T19:47:26.585447Z",
     "start_time": "2025-05-25T19:47:26.566792Z"
    }
   },
   "id": "14d9241b88b2e2e4",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_model(model, filepath='lstm_2.pkl'):\n",
    "    \"\"\"Save model and scaler together in a pickle file\"\"\"\n",
    "    model_json = model.to_json()\n",
    "    model_weights = model.get_weights()\n",
    "    data = {\n",
    "        'model_json': model_json,\n",
    "        'model_weights': model_weights,\n",
    "    }\n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    print(f\"Model saved to {filepath}\")\n",
    "\n",
    "def load_model(filepath='lstm_2.pkl'):\n",
    "    \"\"\"Load model and scaler from pickle file\"\"\"\n",
    "    with open(filepath, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    # Reconstruct model\n",
    "    model = tf.keras.models.model_from_json(data['model_json'])\n",
    "    model.set_weights(data['model_weights'])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-25T20:13:06.303682Z",
     "start_time": "2025-05-25T20:13:06.300249Z"
    }
   },
   "id": "e0a6a9b5057173bc",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import (\n",
    "    Input, LSTM, Dense, Dropout, RepeatVector, TimeDistributed, \n",
    "    Concatenate, Activation, Dot, Layer, BatchNormalization, \n",
    "    LayerNormalization, Add\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import Orthogonal, GlorotUniform\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "class ScaleLayer(Layer):\n",
    "    def __init__(self, scale_factor, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.scale_factor = scale_factor\n",
    "    \n",
    "    def call(self, x):\n",
    "        return x / self.scale_factor\n",
    "\n",
    "class MaxSubtractLayer(Layer):\n",
    "    def call(self, x):\n",
    "        return x - K.max(x, axis=-1, keepdims=True)\n",
    "\n",
    "def create_lstm_encoder_decoder(input_dim, output_dim, timesteps_in, timesteps_out, \n",
    "                               lstm_units=128, num_layers=2, loss_fn='mse', lr=0.001):\n",
    "    \n",
    "    # GRADIENT MITIGATION STRATEGY 1: Keep proper weight initialization (critical for gradients)\n",
    "    lstm_init = Orthogonal(gain=1.0)\n",
    "    dense_init = GlorotUniform()\n",
    "    \n",
    "    # Encoder with minimal regularization for overfitting\n",
    "    encoder_inputs = Input(shape=(timesteps_in, input_dim))\n",
    "    x = encoder_inputs\n",
    "    \n",
    "    # REMOVED: Layer normalization for overfitting experiment\n",
    "    # x = LayerNormalization()(x)\n",
    "    \n",
    "    encoder_states = []\n",
    "    for i in range(num_layers):\n",
    "        # STRATEGY 3: Keep residual connections (help gradients, don't prevent overfitting)\n",
    "        if i > 0 and x.shape[-1] == lstm_units:\n",
    "            residual = x\n",
    "        else:\n",
    "            residual = None\n",
    "            \n",
    "        lstm_out = LSTM(\n",
    "            lstm_units, \n",
    "            return_sequences=True,\n",
    "            recurrent_initializer=lstm_init,\n",
    "            kernel_initializer=lstm_init,\n",
    "            # REMOVED: All dropout for overfitting\n",
    "            # recurrent_dropout=0.1,\n",
    "            # dropout=0.1,\n",
    "            # REMOVED: L2 regularization for overfitting\n",
    "            # kernel_regularizer=l2(1e-4),\n",
    "            # recurrent_regularizer=l2(1e-4)\n",
    "        )(x)\n",
    "        \n",
    "        # REMOVED: Layer normalization for overfitting\n",
    "        # lstm_out = LayerNormalization()(lstm_out)\n",
    "        \n",
    "        # Keep residual connection (helps gradients)\n",
    "        if residual is not None:\n",
    "            x = Add()([lstm_out, residual])\n",
    "        else:\n",
    "            x = lstm_out\n",
    "            \n",
    "        encoder_states.append(x)\n",
    "    \n",
    "    encoder_outputs = x  # (batch, timesteps_in, lstm_units)\n",
    "    \n",
    "    # Get final state for decoder initialization  \n",
    "    encoder_state = LSTM(\n",
    "        lstm_units, \n",
    "        return_state=True,\n",
    "        recurrent_initializer=lstm_init,\n",
    "        kernel_initializer=lstm_init,\n",
    "        # REMOVED: Regularization for overfitting\n",
    "        # kernel_regularizer=l2(1e-4),\n",
    "        # recurrent_regularizer=l2(1e-4)\n",
    "    )(encoder_outputs)\n",
    "    _, state_h, state_c = encoder_state\n",
    "\n",
    "    # Decoder with minimal regularization\n",
    "    decoder_input = RepeatVector(timesteps_out)(state_h)\n",
    "    \n",
    "    decoder_outputs, _, _ = LSTM(\n",
    "        lstm_units, \n",
    "        return_sequences=True, \n",
    "        return_state=True,\n",
    "        recurrent_initializer=lstm_init,\n",
    "        kernel_initializer=lstm_init,\n",
    "        # REMOVED: All dropout and regularization\n",
    "        # recurrent_dropout=0.1,\n",
    "        # dropout=0.1,\n",
    "        # kernel_regularizer=l2(1e-4),\n",
    "        # recurrent_regularizer=l2(1e-4)\n",
    "    )(decoder_input, initial_state=[state_h, state_c])\n",
    "    \n",
    "    # REMOVED: Layer normalization\n",
    "    # decoder_outputs = LayerNormalization()(decoder_outputs)\n",
    "    \n",
    "    # KEEP: Scaled attention (essential for numerical stability, not regularization)\n",
    "    scale_factor = (lstm_units ** 0.5)\n",
    "    attention_scores = Dot(axes=[2, 2])([decoder_outputs, encoder_outputs])\n",
    "    attention_scores = ScaleLayer(scale_factor)(attention_scores)\n",
    "    \n",
    "    # KEEP: Temperature scaling and max subtraction (numerical stability, not regularization)\n",
    "    attention_scores = ScaleLayer(2.0)(attention_scores)  # Temperature scaling\n",
    "    attention_scores = MaxSubtractLayer()(attention_scores)\n",
    "    attention_weights = Activation('softmax')(attention_scores)\n",
    "    \n",
    "    # Apply attention weights\n",
    "    attention_context = Dot(axes=[2, 1])([attention_weights, encoder_outputs])\n",
    "    \n",
    "    # Combine context with decoder outputs\n",
    "    combined = Concatenate()([attention_context, decoder_outputs])\n",
    "    # REMOVED: Layer normalization\n",
    "    # combined = LayerNormalization()(combined)\n",
    "    \n",
    "    # Output layers without regularization\n",
    "    x = TimeDistributed(Dense(\n",
    "        256, \n",
    "        activation='relu',\n",
    "        kernel_initializer=dense_init,\n",
    "        # REMOVED: L2 regularization\n",
    "        # kernel_regularizer=l2(1e-4)\n",
    "    ))(decoder_outputs)\n",
    "    # REMOVED: Normalization and dropout\n",
    "    # x = TimeDistributed(LayerNormalization())(x)\n",
    "    # x = TimeDistributed(Dropout(0.1))(x)\n",
    "    \n",
    "    x = TimeDistributed(Dense(\n",
    "        64, \n",
    "        activation='relu',\n",
    "        kernel_initializer=dense_init,\n",
    "        # REMOVED: L2 regularization\n",
    "        # kernel_regularizer=l2(1e-4)\n",
    "    ))(x)\n",
    "    # REMOVED: Normalization and dropout\n",
    "    # x = TimeDistributed(LayerNormalization())(x)\n",
    "    # x = TimeDistributed(Dropout(0.1))(x)\n",
    "    \n",
    "    # Final output layer\n",
    "    outputs = TimeDistributed(Dense(\n",
    "        output_dim, \n",
    "        activation='linear',\n",
    "        kernel_initializer=dense_init\n",
    "    ))(x)\n",
    "\n",
    "    # KEEP: Aggressive gradient clipping (essential for gradient stability)\n",
    "    model = Model(encoder_inputs, outputs)\n",
    "    model.compile(\n",
    "        optimizer=Adam(\n",
    "            learning_rate=lr,\n",
    "            clipnorm=0.1,       # Even more aggressive clipping for stability without normalization\n",
    "            beta_1=0.9,         \n",
    "            beta_2=0.999,       \n",
    "            epsilon=1e-7        \n",
    "        ),\n",
    "        loss=loss_fn,\n",
    "        metrics=['mae']\n",
    "    )\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-25T20:32:24.493606Z",
     "start_time": "2025-05-25T20:32:24.484026Z"
    }
   },
   "id": "f40b1487275e6996",
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "class GradientMonitoringCallback(Callback):\n",
    "    def __init__(self, clip_min=1e-4, clip_max=1e2, monitor_frequency=3):\n",
    "        \"\"\"\n",
    "        Monitor gradient norms during training\n",
    "        \n",
    "        Args:\n",
    "            clip_min: Minimum threshold for gradient norms\n",
    "            clip_max: Maximum threshold for gradient norms  \n",
    "            monitor_frequency: How often to check gradients (every N batches)\n",
    "        \"\"\"\n",
    "        print(f\"🔧 GradientMonitoringCallback initialized with clip_min={clip_min}, clip_max={clip_max}, monitor_freq={monitor_frequency}\")\n",
    "        self.clip_min = clip_min\n",
    "        self.clip_max = clip_max\n",
    "        self.monitor_frequency = monitor_frequency\n",
    "        self.batch_count = 0\n",
    "        self.total_calls = 0\n",
    "        self.gradient_checks = 0\n",
    "        self.fallback_calls = 0\n",
    "        \n",
    "    def on_train_begin(self, logs=None):\n",
    "        print(\"🚀 GradientMonitoringCallback: Training started!\")\n",
    "        self.batch_count = 0\n",
    "        self.total_calls = 0\n",
    "        self.gradient_checks = 0\n",
    "        self.fallback_calls = 0\n",
    "        \n",
    "    # def on_epoch_begin(self, epoch, logs=None):\n",
    "    #     print(f\"📍 GradientMonitoringCallback: Starting epoch {epoch + 1}\")\n",
    "        \n",
    "    # def on_train_batch_begin(self, batch, logs=None):\n",
    "    #     # Just to prove we're being called\n",
    "    #     if batch % 50 == 0:  # Print every 50 batches to avoid spam\n",
    "    #         print(f\"⚡ GradientMonitoringCallback: Batch {batch} starting\")\n",
    "        \n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        self.batch_count += 1\n",
    "        self.total_calls += 1\n",
    "        \n",
    "        # Print every time to show we're being called\n",
    "        # if batch % 50 == 0:  # Print every 50 batches\n",
    "            # print(f\"📊 GradientMonitoringCallback: Batch {batch} ended (total calls: {self.total_calls})\")\n",
    "        \n",
    "        # Only monitor every N batches to avoid performance overhead\n",
    "        if self.batch_count % self.monitor_frequency != 0:\n",
    "            return\n",
    "            \n",
    "        # print(f\"🔍 GradientMonitoringCallback: Checking gradients at batch {batch} (check #{self.gradient_checks + 1})\")\n",
    "        \n",
    "        # Get gradients from the optimizer's current state\n",
    "        try:\n",
    "            # Access the model's optimizer to get gradient information\n",
    "            optimizer = self.model.optimizer\n",
    "            print(f\"   📋 Optimizer type: {type(optimizer).__name__}\")\n",
    "            \n",
    "            # Get trainable variables\n",
    "            trainable_vars = self.model.trainable_variables\n",
    "            print(f\"   📈 Number of trainable variables: {len(trainable_vars)}\")\n",
    "            \n",
    "            if hasattr(optimizer, 'get_gradients'):\n",
    "                print(\"   ✅ Optimizer has get_gradients method\")\n",
    "                # For some optimizers, we can access gradients directly\n",
    "                grads = optimizer.get_gradients(self.model.total_loss, trainable_vars)\n",
    "                print(f\"   📊 Retrieved {len([g for g in grads if g is not None])} gradients\")\n",
    "            else:\n",
    "                print(\"   ❌ Optimizer doesn't have get_gradients, using variable norms\")\n",
    "                # Alternative approach: check the current variable states\n",
    "                grad_norms = []\n",
    "                for i, var in enumerate(trainable_vars):\n",
    "                    if var is not None:\n",
    "                        var_norm = tf.norm(var)\n",
    "                        grad_norms.append(var_norm)\n",
    "                        if i < 3:  # Print first 3 for debugging\n",
    "                            print(f\"      Variable {i} norm: {float(var_norm.numpy()):.2e}\")\n",
    "                \n",
    "                self._check_norms(grad_norms, \"Variable\")\n",
    "                self.gradient_checks += 1\n",
    "                return\n",
    "                \n",
    "            # Compute gradient norms\n",
    "            grad_norms = []\n",
    "            for i, grad in enumerate(grads):\n",
    "                if grad is not None:\n",
    "                    grad_norm = tf.norm(grad)\n",
    "                    grad_norms.append(grad_norm)\n",
    "                    if i < 3:  # Print first 3 for debugging\n",
    "                        print(f\"      Gradient {i} norm: {float(grad_norm.numpy()):.2e}\")\n",
    "                    \n",
    "            print(f\"   ✅ Computed {len(grad_norms)} gradient norms\")\n",
    "            self._check_norms(grad_norms, \"Gradient\")\n",
    "            self.gradient_checks += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Exception in gradient monitoring: {str(e)}\")\n",
    "            self.fallback_calls += 1\n",
    "            # Fallback: just monitor the loss for signs of instability\n",
    "            print('   🔄 Fallback: monitoring loss only')\n",
    "            if logs:\n",
    "                loss_value = logs.get('loss', 0)\n",
    "                print(f\"   📉 Current loss: {loss_value:.2e}\")\n",
    "                if np.isnan(loss_value) or np.isinf(loss_value):\n",
    "                    print(f\"   ⚠️  WARNING: Loss became {loss_value} at batch {batch}\")\n",
    "                elif loss_value > 1e6:\n",
    "                    print(f\"   ⚠️  WARNING: Very large loss {loss_value:.2e} at batch {batch}\")\n",
    "    \n",
    "    def _check_norms(self, norms, norm_type=\"Gradient\"):\n",
    "        \"\"\"Check if norms are within acceptable range\"\"\"\n",
    "        print(f\"   🔬 Checking {len(norms)} {norm_type.lower()} norms...\")\n",
    "        warnings = 0\n",
    "        \n",
    "        for idx, norm in enumerate(norms):\n",
    "            try:\n",
    "                norm_value = float(norm.numpy()) if hasattr(norm, 'numpy') else float(norm)\n",
    "                \n",
    "                if norm_value > self.clip_max:\n",
    "                    print(f\"   ⚠️  WARNING: {norm_type} norm {norm_value:.2e} is too large (layer {idx})\")\n",
    "                    warnings += 1\n",
    "                elif norm_value < self.clip_min:\n",
    "                    print(f\"   ⚠️  WARNING: {norm_type} norm {norm_value:.2e} is too small (layer {idx})\")\n",
    "                    warnings += 1\n",
    "                elif np.isnan(norm_value) or np.isinf(norm_value):\n",
    "                    print(f\"   ⚠️  WARNING: {norm_type} norm is {norm_value} (layer {idx})\")\n",
    "                    warnings += 1\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"   ❌ Cannot convert norm to float for layer {idx}: {str(e)}\")\n",
    "                continue\n",
    "                \n",
    "        if warnings == 0:\n",
    "            print(f\"   ✅ All {norm_type.lower()} norms are within acceptable range\")\n",
    "        else:\n",
    "            print(f\"   ⚠️  Found {warnings} norm warnings\")\n",
    "    \n",
    "    # def on_epoch_end(self, epoch, logs=None):\n",
    "    #     \"\"\"Print summary at end of each epoch\"\"\"\n",
    "    #     print(f\"📈 GradientMonitoringCallback: Epoch {epoch + 1} completed\")\n",
    "    #     print(f\"   📊 Total batch calls: {self.total_calls}\")\n",
    "    #     print(f\"   🔍 Gradient checks performed: {self.gradient_checks}\")\n",
    "    #     print(f\"   🔄 Fallback calls: {self.fallback_calls}\")\n",
    "    #     \n",
    "    #     if logs:\n",
    "    #         loss = logs.get('loss', 0)\n",
    "    #         val_loss = logs.get('val_loss', 0)\n",
    "    #         print(f\"   📉 Final epoch loss: {loss:.2e}\")\n",
    "    #         if val_loss:\n",
    "    #             print(f\"   📉 Final epoch val_loss: {val_loss:.2e}\")\n",
    "    #         \n",
    "    #         if np.isnan(loss) or np.isinf(loss):\n",
    "    #             print(f\"   ⚠️  WARNING: Training loss became unstable: {loss}\")\n",
    "    #         if val_loss and (np.isnan(val_loss) or np.isinf(val_loss)):\n",
    "    #             print(f\"   ⚠️  WARNING: Validation loss became unstable: {val_loss}\")\n",
    "        \n",
    "    def on_train_end(self, logs=None):\n",
    "        print(\"🏁 GradientMonitoringCallback: Training completed!\")\n",
    "        print(f\"   📊 Final stats - Total calls: {self.total_calls}, Gradient checks: {self.gradient_checks}, Fallbacks: {self.fallback_calls}\")\n",
    "        \n",
    "        if self.total_calls == 0:\n",
    "            print(\"   ❌ ERROR: Callback was never called! Check if it's properly added to callbacks list.\")\n",
    "        elif self.gradient_checks == 0 and self.fallback_calls == 0:\n",
    "            print(\"   ⚠️  WARNING: No gradient monitoring was performed. Check monitor_frequency setting.\")\n",
    "        else:\n",
    "            print(\"   ✅ Gradient monitoring completed successfully!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-25T20:14:30.667725Z",
     "start_time": "2025-05-25T20:14:30.662788Z"
    }
   },
   "id": "48e6074507a81432",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from keras.src.callbacks import LearningRateScheduler, EarlyStopping, Callback\n",
    "from keras.src.optimizers import Adam\n",
    "from keras import Model\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def exponential_decay_schedule(epoch, lr):\n",
    "    decay_rate = 0.9\n",
    "    decay_steps = 5\n",
    "    if epoch % decay_steps == 0 and epoch:\n",
    "        print('Learning rate update:', lr * decay_rate)\n",
    "        return lr * decay_rate\n",
    "    return lr\n",
    "\n",
    "\n",
    "# Custom callback to monitor LR and stop training\n",
    "class LRThresholdCallback(Callback):\n",
    "    def __init__(self, threshold=9e-5):\n",
    "        super().__init__()\n",
    "        self.threshold = threshold\n",
    "        self.should_stop = False\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        lr = float(self.model.optimizer.learning_rate.numpy())\n",
    "        if lr < self.threshold:\n",
    "            print(f\"\\nLearning rate {lr:.6f} < threshold {self.threshold}, moving to next phase.\")\n",
    "            self.model.stop_training = True\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-25T20:14:31.862933Z",
     "start_time": "2025-05-25T20:14:31.726884Z"
    }
   },
   "id": "2d3c3926a64b326d",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.models import save_model\n",
    "\n",
    "class SaveBestModelCallback(Callback):\n",
    "    def __init__(self, save_path='best_model.keras', monitor='val_loss'):\n",
    "        super().__init__()\n",
    "        self.best = float('inf')\n",
    "        self.monitor = monitor\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current = logs.get(self.monitor)\n",
    "        if current is not None and current < self.best:\n",
    "            self.best = current\n",
    "            print(f\"\\nNew best {self.monitor}: {current:.6f}. Saving model...\")\n",
    "            save_model(self.model, 'lstm_2.pkl')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-25T20:14:34.666023Z",
     "start_time": "2025-05-25T20:14:34.634038Z"
    }
   },
   "id": "f08667de7129e4e0",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train_model(train_data, batch_size=32, validation_split=0.2, Tobs=50, Tpred=60, epochs1=50, epochs2=50):\n",
    "    n_scenarios = train_data.shape[0]\n",
    "    n_agents = train_data.shape[1]\n",
    "    X_train_raw = []\n",
    "    y_train_deltas = []\n",
    "\n",
    "    # Counters for pruning reasons\n",
    "    pruned_zero_frame = 0\n",
    "    pruned_observed_or_future_zero = 0\n",
    "    total_agents = n_scenarios * n_agents\n",
    "    \n",
    "    for i in range(n_scenarios):\n",
    "        for agent_id in range(n_agents):\n",
    "            agent_data = train_data[i, agent_id, :, :]  # shape (110, 6)\n",
    "    \n",
    "            # Skip if any time step in the observation+prediction window is all zeros\n",
    "            if np.any(np.all(agent_data[:Tobs + Tpred] == 0, axis=1)):\n",
    "                pruned_zero_frame += 1\n",
    "                continue\n",
    "    \n",
    "            observed = agent_data[:Tobs]         # shape (Tobs, 6)\n",
    "            future = agent_data[Tobs:Tobs + Tpred, :2]  # only position_x, position_y\n",
    "            last_obs_pos = observed[-1, :2]\n",
    "    \n",
    "            # Skip if observed or future window contains any full-zero frame\n",
    "            if np.any(np.all(observed == 0, axis=1)) or np.any(np.all(future == 0, axis=1)):\n",
    "                pruned_observed_or_future_zero += 1\n",
    "                continue\n",
    "    \n",
    "            # Compute deltas\n",
    "            delta = np.diff(np.vstack([last_obs_pos, future]), axis=0)  # shape (Tpred, 2)\n",
    "    \n",
    "            X_train_raw.append(observed)\n",
    "            y_train_deltas.append(delta)\n",
    "    \n",
    "    # Print pruning summary\n",
    "    print(f\"Total agents: {total_agents}\")\n",
    "    print(f\"Pruned due to zero frame in Tobs+Tpred: {pruned_zero_frame}\")\n",
    "    print(f\"Pruned due to zero frame in observed or future window: {pruned_observed_or_future_zero}\")\n",
    "    print(f\"Remaining valid agents: {len(X_train_raw)}\")\n",
    "    \n",
    "    \n",
    "    X_train = np.array(X_train_raw)     # shape (N_valid, Tobs, 6)\n",
    "    y_train = np.array(y_train_deltas)  # shape (N_valid, Tpred, 2)\n",
    "    \n",
    "    \n",
    "    print(f\"ex. y_train {y_train[0]}\")\n",
    "\n",
    "\n",
    "    print(f\"Training on {X_train.shape[0]} valid agent trajectories.\")\n",
    "    print(f\"Input shape: {X_train.shape}, Delta Output shape: {y_train.shape}\")\n",
    "    \n",
    "    # --- Normalize Input and Output ---\n",
    "    X_mean = X_train.mean(axis=(0, 1), keepdims=True)  # shape: (1, 1, 6)\n",
    "    X_std = X_train.std(axis=(0, 1), keepdims=True) + 1e-8\n",
    "\n",
    "    y_mean = y_train.mean(axis=(0, 1), keepdims=True)  # shape: (1, 1, 2)\n",
    "    y_std = y_train.std(axis=(0, 1), keepdims=True) + 1e-8\n",
    "\n",
    "    X_std = np.where(X_std < 1e-6, 1.0, X_std)\n",
    "    y_std = np.where(y_std < 1e-6, 1.0, y_std)\n",
    "\n",
    "    X_train = (X_train - X_mean) / X_std\n",
    "    y_train = (y_train - y_mean) / y_std \n",
    "    \n",
    "    print(\"X_train NaNs:\", np.isnan(X_train).sum())\n",
    "    print(\"y_train NaNs:\", np.isnan(y_train).sum())\n",
    "\n",
    "    print(\"Any std == 0?\", np.any(X_std == 0), np.any(y_std == 0))\n",
    "    \n",
    "    X_mean, X_std, y_mean, y_std = None, None, None, None\n",
    "    \n",
    "    # print(X_train[:2])\n",
    "    # print(y_train[:2])\n",
    "    \n",
    "    model = create_lstm_encoder_decoder(\n",
    "        input_dim=X_train.shape[-1],\n",
    "        output_dim=2,\n",
    "        timesteps_in=Tobs,\n",
    "        timesteps_out=Tpred,\n",
    "        loss_fn='mse',\n",
    "        lr=0.001\n",
    "    )\n",
    "    \n",
    "    gradient_monitoring_callback = GradientMonitoringCallback(clip_min=1e-4, clip_max=1e2)\n",
    "    \n",
    "    save_best_callback = SaveBestModelCallback(save_path='lstm2', monitor='val_loss')\n",
    "\n",
    "\n",
    "\n",
    "    phase1_callbacks = [\n",
    "        # LearningRateScheduler(exponential_decay_schedule),\n",
    "        # EarlyStopping(patience=4, restore_best_weights=True, monitor='val_loss'),\n",
    "        LRThresholdCallback(threshold=9e-5),\n",
    "        gradient_monitoring_callback,\n",
    "        save_best_callback\n",
    "    ]\n",
    "\n",
    "    print(\"\\n--- Phase 1: Training ---\")\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=epochs1,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=validation_split,\n",
    "        callbacks=phase1_callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- Phase 2: Fine-tuning ---\")\n",
    "    model.compile(\n",
    "        optimizer=Adam(\n",
    "            learning_rate=1e-4,\n",
    "            clipnorm=0.25,      # More aggressive clipping\n",
    "            # clipvalue=0.5,      # Also clip individual gradients\n",
    "            beta_1=0.9,         # Standard momentum\n",
    "            beta_2=0.999,       # Standard RMSprop decay\n",
    "            epsilon=1e-7        # Smaller epsilon for stability\n",
    "        ),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    phase2_callbacks = [\n",
    "        # LearningRateScheduler(exponential_decay_schedule),\n",
    "        # EarlyStopping(patience=3, restore_best_weights=True, monitor='val_loss'), \n",
    "        gradient_monitoring_callback\n",
    "    ]\n",
    "    \n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=epochs2,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=validation_split,\n",
    "        callbacks=phase2_callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(f\"X_mean:{X_mean}, X_std:{X_std}, y_mean:{y_mean}, y_std:{y_std}\")\n",
    "\n",
    "    # Return model and normalization parameters\n",
    "    return model, X_mean, X_std, y_mean, y_std"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-25T20:32:18.363715Z",
     "start_time": "2025-05-25T20:32:18.347650Z"
    }
   },
   "id": "ea4e240971bc7867",
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def plot_mae_by_timestep(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Visualize MAE across timesteps in the prediction horizon.\n",
    "    \n",
    "    Args:\n",
    "        y_true (np.ndarray): shape (N, Tpred, 2)\n",
    "        y_pred (np.ndarray): shape (N, Tpred, 2)\n",
    "    \"\"\"\n",
    "    mae_per_timestep = np.mean(np.abs(y_true - y_pred), axis=(0, 2))  # shape (Tpred,)\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(mae_per_timestep, label='MAE per Timestep')\n",
    "    plt.xlabel('Timestep')\n",
    "    plt.ylabel('MAE (meters)')\n",
    "    plt.title('Mean Absolute Error Over Prediction Horizon')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-25T19:47:26.669829Z",
     "start_time": "2025-05-25T19:47:26.628379Z"
    }
   },
   "id": "1dae50c6d460015",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def reconstruct_absolute_positions(pred_deltas, last_observed_positions):\n",
    "    \"\"\"\n",
    "    Reconstruct absolute predicted positions by adding deltas to the last observed position.\n",
    "\n",
    "    Args:\n",
    "        pred_deltas: np.ndarray of shape (N, Tpred, 2)\n",
    "        last_observed_positions: np.ndarray of shape (N, 2)\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray of shape (N, Tpred, 2)\n",
    "    \"\"\"\n",
    "    return last_observed_positions[:, None, :] + np.cumsum(pred_deltas, axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-25T19:47:26.670878Z",
     "start_time": "2025-05-25T19:47:26.633502Z"
    }
   },
   "id": "f303823d230afdb9",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# # figure out stats\n",
    "# def stats():\n",
    "#     n_scenarios = train_data.shape[0]\n",
    "#     X_train_raw = []\n",
    "#     y_train_deltas = []\n",
    "#     \n",
    "#     for i in range(n_scenarios):\n",
    "#         ego_data = train_data[i, 0, :, :]\n",
    "#         if np.all(ego_data == 0):\n",
    "#             continue\n",
    "#     \n",
    "#         observed = ego_data[:Tobs]            # shape (50, 6)\n",
    "#         future = ego_data[Tobs:Tobs+Tpred, :2]\n",
    "#         last_obs_pos = observed[-1, :2]\n",
    "#     \n",
    "#         if np.any(np.all(observed == 0, axis=1)) or np.any(np.all(future == 0, axis=1)):\n",
    "#             continue\n",
    "#     \n",
    "#         # Compute deltas w.r.t. previous future timestep\n",
    "#         delta = np.diff(np.vstack([last_obs_pos, future]), axis=0)  # (60, 2)\n",
    "#     \n",
    "#         X_train_raw.append(observed)\n",
    "#         y_train_deltas.append(delta)\n",
    "#     \n",
    "#     \n",
    "#     X_train = np.array(X_train_raw)\n",
    "#     y_train = np.array(y_train_deltas)\n",
    "#     \n",
    "#     print(f\"{X_train.shape[0]} valid sequences.\")\n",
    "#     print(f\"Input shape: {X_train.shape}, Delta Output shape: {y_train.shape}\")\n",
    "#     \n",
    "#     # --- Normalize Input and Output ---\n",
    "#     X_mean = X_train.mean(axis=(0, 1), keepdims=True)  # shape: (1, 1, 6)\n",
    "#     X_std = X_train.std(axis=(0, 1), keepdims=True) + 1e-8\n",
    "#     \n",
    "#     y_mean = y_train.mean(axis=(0, 1), keepdims=True)  # shape: (1, 1, 2)\n",
    "#     y_std = y_train.std(axis=(0, 1), keepdims=True) + 1e-8\n",
    "#     \n",
    "#     X_train = (X_train - X_mean) / X_std\n",
    "#     y_train = (y_train - y_mean) / y_std\n",
    "#     return X_mean, X_std, y_mean, y_std\n",
    "# X_mean, X_std, y_mean, y_std = stats()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-25T19:47:26.673142Z",
     "start_time": "2025-05-25T19:47:26.635808Z"
    }
   },
   "id": "ac3b5ab02eaf908a",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def forecast_positions(scenario_data, Tobs, Tpred, model, X_mean=None, X_std=None, y_mean=None, y_std=None):\n",
    "    \"\"\"\n",
    "    Use LSTM model to forecast future deltas and reconstruct absolute positions.\n",
    "    Applies normalization only if statistics are provided.\n",
    "\n",
    "    Args:\n",
    "        scenario_data (numpy.ndarray): Shape (agents, time_steps, dimensions)\n",
    "        Tobs (int): Number of observed time steps\n",
    "        Tpred (int): Number of future time steps to predict\n",
    "        model (Model): Trained LSTM model\n",
    "        X_mean, X_std: Optional normalization stats for input\n",
    "        y_mean, y_std: Optional normalization stats for output\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Predicted absolute positions of shape (agents, Tpred, 2)\n",
    "    \"\"\"\n",
    "    agents, _, _ = scenario_data.shape\n",
    "    predicted_positions = np.zeros((agents, Tpred, 2))\n",
    "\n",
    "    for agent_idx in range(agents):\n",
    "        agent_data = scenario_data[agent_idx, :Tobs, :]  # shape (Tobs, 6)\n",
    "\n",
    "        # Skip if fully padded\n",
    "        if np.all(agent_data == 0):\n",
    "            continue\n",
    "\n",
    "        X_pred = np.expand_dims(agent_data, axis=0)  # shape (1, Tobs, 6)\n",
    "\n",
    "        # Normalize if stats are provided\n",
    "        if X_mean is not None and X_std is not None:\n",
    "            X_pred = (X_pred - X_mean) / X_std\n",
    "\n",
    "        # Predict deltas (normalized or raw)\n",
    "        pred_deltas = model.predict(X_pred, verbose=0)  # shape (1, Tpred, 2)\n",
    "        \n",
    "        print(\"pred deltas\")\n",
    "        print(pred_deltas[:,:5])\n",
    "\n",
    "        # Denormalize if stats are provided\n",
    "        if y_mean is not None and y_std is not None:\n",
    "            pred_deltas = pred_deltas * y_std + y_mean\n",
    "\n",
    "        # Reconstruct absolute positions\n",
    "        last_pos = agent_data[Tobs - 1, :2]  # shape (2,)\n",
    "        abs_positions = reconstruct_absolute_positions(\n",
    "            pred_deltas=pred_deltas,\n",
    "            last_observed_positions=np.expand_dims(last_pos, axis=0)\n",
    "        )[0]\n",
    "\n",
    "        predicted_positions[agent_idx] = abs_positions\n",
    "\n",
    "    return predicted_positions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-25T19:47:26.689553Z",
     "start_time": "2025-05-25T19:47:26.639459Z"
    }
   },
   "id": "7d53c8f6d9d0c748",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def finetune_forecast_positions(scenario_data, Tobs, Tpred, model, \n",
    "                                 X_mean=None, X_std=None, y_mean=None, y_std=None, \n",
    "                                 epochs=3, lr=1e-4):\n",
    "    \"\"\"\n",
    "    Fine-tune on valid agents from a scenario, then forecast their future positions.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Predicted absolute positions of shape (agents, Tpred, 2)\n",
    "    \"\"\"\n",
    "    import copy\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "    agents, total_steps, _ = scenario_data.shape\n",
    "    assert total_steps >= Tobs + Tpred, \"Not enough time steps for observation + prediction\"\n",
    "\n",
    "    # Prepare fine-tuning data\n",
    "    X_finetune = []\n",
    "    y_finetune = []\n",
    "\n",
    "    for agent_idx in range(agents):\n",
    "        agent_traj = scenario_data[agent_idx, :, :]  # shape (time_steps, 6)\n",
    "        \n",
    "        segment = agent_traj[:Tobs + Tpred]\n",
    "        if np.any(np.all(segment == 0, axis=1)):\n",
    "            continue\n",
    "\n",
    "        observed = segment[:Tobs]            # (Tobs, 6)\n",
    "        future = segment[Tobs:Tobs+Tpred, :2]\n",
    "        last_obs_pos = observed[-1, :2]\n",
    "\n",
    "        if np.any(np.all(observed == 0, axis=1)) or np.any(np.all(future == 0, axis=1)):\n",
    "            continue\n",
    "\n",
    "        delta = np.diff(np.vstack([last_obs_pos, future]), axis=0)  # (Tpred, 2)\n",
    "\n",
    "        X_finetune.append(observed)\n",
    "        y_finetune.append(delta)\n",
    "\n",
    "    if len(X_finetune) == 0:\n",
    "        print(\"No valid agents found for fine-tuning.\")\n",
    "        return np.zeros((agents, Tpred, 2))\n",
    "\n",
    "    X_finetune = np.array(X_finetune)\n",
    "    y_finetune = np.array(y_finetune)\n",
    "\n",
    "    # Normalize if stats provided\n",
    "    if X_mean is not None and X_std is not None:\n",
    "        X_finetune = (X_finetune - X_mean) / X_std\n",
    "\n",
    "    if y_mean is not None and y_std is not None:\n",
    "        y_finetune = (y_finetune - y_mean) / y_std\n",
    "\n",
    "    # Clone and compile the model to avoid modifying the original\n",
    "    model_finetune = copy.deepcopy(model)\n",
    "    model_finetune.compile(optimizer=Adam(learning_rate=lr), loss='mse') #todo: fix optimizer \n",
    "\n",
    "    # Fine-tune\n",
    "    model_finetune.fit(X_finetune, y_finetune, epochs=epochs, verbose=0)\n",
    "\n",
    "    # Predict for each agent\n",
    "    predicted_positions = np.zeros((agents, Tpred, 2))\n",
    "\n",
    "    for agent_idx in range(agents):\n",
    "        agent_obs = scenario_data[agent_idx, :Tobs, :]\n",
    "\n",
    "        if np.any(np.all(agent_obs == 0, axis=1)):\n",
    "            continue\n",
    "\n",
    "        X_pred = np.expand_dims(agent_obs, axis=0)\n",
    "\n",
    "        if X_mean is not None and X_std is not None:\n",
    "            X_pred = (X_pred - X_mean) / X_std\n",
    "\n",
    "        pred_deltas = model_finetune.predict(X_pred, verbose=0)\n",
    "\n",
    "        if y_mean is not None and y_std is not None:\n",
    "            pred_deltas = pred_deltas * y_std + y_mean\n",
    "\n",
    "        last_pos = agent_obs[Tobs - 1, :2]\n",
    "        abs_positions = reconstruct_absolute_positions(\n",
    "            pred_deltas=pred_deltas,\n",
    "            last_observed_positions=np.expand_dims(last_pos, axis=0)\n",
    "        )[0]\n",
    "\n",
    "        predicted_positions[agent_idx] = abs_positions\n",
    "\n",
    "    return predicted_positions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-25T19:47:26.698435Z",
     "start_time": "2025-05-25T19:47:26.641302Z"
    }
   },
   "id": "f0d902d364b6d300",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "def make_gif(data_matrix1, data_matrix2, name='comparison'):\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.animation as animation\n",
    "\n",
    "    cmap1 = plt.cm.get_cmap('viridis', 50)\n",
    "    cmap2 = plt.cm.get_cmap('plasma', 50)\n",
    "\n",
    "    assert data_matrix1.shape[1] == data_matrix2.shape[1], \"Both matrices must have same number of timesteps\"\n",
    "    timesteps = data_matrix1.shape[1]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(18, 9))\n",
    "    ax1, ax2 = axes\n",
    "\n",
    "    def update(frame):\n",
    "        for ax in axes:\n",
    "            ax.clear()\n",
    "\n",
    "        for i in range(data_matrix1.shape[0]):\n",
    "            for (data_matrix, ax, cmap) in [(data_matrix1, ax1, cmap1), (data_matrix2, ax2, cmap2)]:\n",
    "                x = data_matrix[i, frame, 0]\n",
    "                y = data_matrix[i, frame, 1]\n",
    "                if x != 0 and y != 0:\n",
    "                    xs = data_matrix[i, :frame+1, 0]\n",
    "                    ys = data_matrix[i, :frame+1, 1]\n",
    "                    mask = (xs != 0) & (ys != 0)\n",
    "                    xs = xs[mask]\n",
    "                    ys = ys[mask]\n",
    "                    if len(xs) > 0 and len(ys) > 0:\n",
    "                        color = cmap(i)\n",
    "                        ax.plot(xs, ys, alpha=0.9, color=color)\n",
    "                        ax.scatter(x, y, s=80, color=color)\n",
    "\n",
    "        # Plot ego vehicle (index 0) on both\n",
    "        ax1.plot(data_matrix1[0, :frame, 0], data_matrix1[0, :frame, 1], color='tab:orange', label='Ego Vehicle')\n",
    "        ax1.scatter(data_matrix1[0, frame, 0], data_matrix1[0, frame, 1], s=80, color='tab:orange')\n",
    "        ax1.set_title('Prediction')\n",
    "\n",
    "        ax2.plot(data_matrix2[0, :frame, 0], data_matrix2[0, :frame, 1], color='tab:orange', label='Ego Vehicle')\n",
    "        ax2.scatter(data_matrix2[0, frame, 0], data_matrix2[0, frame, 1], s=80, color='tab:orange')\n",
    "        ax2.set_title('Actual')\n",
    "\n",
    "        for ax, data_matrix in zip(axes, [data_matrix1, data_matrix2]):\n",
    "            ax.set_xlim(data_matrix[:, :, 0][data_matrix[:, :, 0] != 0].min() - 10,\n",
    "                        data_matrix[:, :, 0][data_matrix[:, :, 0] != 0].max() + 10)\n",
    "            ax.set_ylim(data_matrix[:, :, 1][data_matrix[:, :, 1] != 0].min() - 10,\n",
    "                        data_matrix[:, :, 1][data_matrix[:, :, 1] != 0].max() + 10)\n",
    "            ax.legend()\n",
    "            ax.set_xlabel('X')\n",
    "            ax.set_ylabel('Y')\n",
    "\n",
    "        # Compute MSE over non-zero entries up to current frame\n",
    "        mask = (data_matrix2[:, :frame+1, :] != 0) & (data_matrix1[:, :frame+1, :] != 0)\n",
    "        mse = np.mean((data_matrix1[:, :frame+1, :][mask] - data_matrix2[:, :frame+1, :][mask]) ** 2)\n",
    "\n",
    "        fig.suptitle(f\"Timestep {frame} - MSE: {mse:.4f}\", fontsize=16)\n",
    "        return ax1.collections + ax1.lines + ax2.collections + ax2.lines\n",
    "\n",
    "    anim = animation.FuncAnimation(fig, update, frames=list(range(0, timesteps, 3)), interval=100, blit=True)\n",
    "    anim.save(f'trajectory_visualization_{name}.gif', writer='pillow')\n",
    "    plt.close()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-25T19:47:26.705558Z",
     "start_time": "2025-05-25T19:47:26.647544Z"
    }
   },
   "id": "e475f845b5d73bae",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total agents: 150\n",
      "Pruned due to zero frame in Tobs+Tpred: 132\n",
      "Pruned due to zero frame in observed or future window: 0\n",
      "Remaining valid agents: 18\n",
      "Training on 18 valid agent trajectories.\n",
      "Input shape: (18, 50, 6), Delta Output shape: (18, 60, 2)\n",
      "X_train NaNs: 0\n",
      "y_train NaNs: 0\n",
      "Any std == 0? False False\n",
      "🔧 GradientMonitoringCallback initialized with clip_min=0.0001, clip_max=100.0, monitor_freq=3\n",
      "\n",
      "--- Phase 1: Training ---\n",
      "🚀 GradientMonitoringCallback: Training started!\n",
      "Epoch 1/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 9s/step - loss: 1.0058 - mae: 0.6692\n",
      "Epoch 2/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 194ms/step - loss: 0.8362 - mae: 0.6214\n",
      "Epoch 3/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 2.45e+00\n",
      "      Variable 1 norm: 1.13e+01\n",
      "      Variable 2 norm: 1.13e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 160ms/step - loss: 0.6653 - mae: 0.5907\n",
      "Epoch 4/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 88ms/step - loss: 0.5345 - mae: 0.5818\n",
      "Epoch 5/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 82ms/step - loss: 0.4983 - mae: 0.5872\n",
      "Epoch 6/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 2.45e+00\n",
      "      Variable 1 norm: 1.13e+01\n",
      "      Variable 2 norm: 1.13e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 86ms/step - loss: 0.5485 - mae: 0.6169\n",
      "Epoch 7/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 108ms/step - loss: 0.5902 - mae: 0.6315\n",
      "Epoch 8/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 85ms/step - loss: 0.5661 - mae: 0.6223\n",
      "Epoch 9/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 2.46e+00\n",
      "      Variable 1 norm: 1.13e+01\n",
      "      Variable 2 norm: 1.13e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 88ms/step - loss: 0.4244 - mae: 0.5286\n",
      "Epoch 10/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 82ms/step - loss: 0.3653 - mae: 0.4780\n",
      "Epoch 11/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 82ms/step - loss: 0.3944 - mae: 0.4820\n",
      "Epoch 12/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 2.46e+00\n",
      "      Variable 1 norm: 1.14e+01\n",
      "      Variable 2 norm: 1.13e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 94ms/step - loss: 0.4033 - mae: 0.4779\n",
      "Epoch 13/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 82ms/step - loss: 0.3837 - mae: 0.4494\n",
      "Epoch 14/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 89ms/step - loss: 0.3680 - mae: 0.4421\n",
      "Epoch 15/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 2.46e+00\n",
      "      Variable 1 norm: 1.14e+01\n",
      "      Variable 2 norm: 1.13e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 92ms/step - loss: 0.3459 - mae: 0.4297\n",
      "Epoch 16/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 84ms/step - loss: 0.3327 - mae: 0.4265\n",
      "Epoch 17/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 85ms/step - loss: 0.3357 - mae: 0.4497\n",
      "Epoch 18/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 2.47e+00\n",
      "      Variable 1 norm: 1.14e+01\n",
      "      Variable 2 norm: 1.13e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 90ms/step - loss: 0.3275 - mae: 0.4501\n",
      "Epoch 19/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 82ms/step - loss: 0.3168 - mae: 0.4388\n",
      "Epoch 20/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 84ms/step - loss: 0.3173 - mae: 0.4354\n",
      "Epoch 21/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 2.47e+00\n",
      "      Variable 1 norm: 1.14e+01\n",
      "      Variable 2 norm: 1.13e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 97ms/step - loss: 0.3030 - mae: 0.4135\n",
      "Epoch 22/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 86ms/step - loss: 0.2807 - mae: 0.4008\n",
      "Epoch 23/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 83ms/step - loss: 0.3282 - mae: 0.4645\n",
      "Epoch 24/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 2.48e+00\n",
      "      Variable 1 norm: 1.14e+01\n",
      "      Variable 2 norm: 1.13e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 89ms/step - loss: 0.3612 - mae: 0.5022\n",
      "Epoch 25/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 84ms/step - loss: 0.3670 - mae: 0.5047\n",
      "Epoch 26/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 81ms/step - loss: 0.3573 - mae: 0.4745\n",
      "Epoch 27/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 2.49e+00\n",
      "      Variable 1 norm: 1.14e+01\n",
      "      Variable 2 norm: 1.13e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 89ms/step - loss: 0.3481 - mae: 0.4525\n",
      "Epoch 28/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 83ms/step - loss: 0.3265 - mae: 0.4297\n",
      "Epoch 29/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 83ms/step - loss: 0.3567 - mae: 0.4590\n",
      "Epoch 30/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 2.50e+00\n",
      "      Variable 1 norm: 1.15e+01\n",
      "      Variable 2 norm: 1.14e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 89ms/step - loss: 0.4224 - mae: 0.5242\n",
      "Epoch 31/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 83ms/step - loss: 0.4649 - mae: 0.5592\n",
      "Epoch 32/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 79ms/step - loss: 0.4705 - mae: 0.5368\n",
      "Epoch 33/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 2.51e+00\n",
      "      Variable 1 norm: 1.15e+01\n",
      "      Variable 2 norm: 1.14e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 90ms/step - loss: 0.4544 - mae: 0.5199\n",
      "Epoch 34/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 85ms/step - loss: 0.7018 - mae: 0.6920\n",
      "Epoch 35/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 83ms/step - loss: 0.7039 - mae: 0.6753\n",
      "Epoch 36/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 2.52e+00\n",
      "      Variable 1 norm: 1.16e+01\n",
      "      Variable 2 norm: 1.14e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 88ms/step - loss: 0.8396 - mae: 0.6563\n",
      "Epoch 37/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 81ms/step - loss: 1.2747 - mae: 0.8128\n",
      "Epoch 38/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 278ms/step - loss: 0.9794 - mae: 0.7016\n",
      "Epoch 39/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 2.53e+00\n",
      "      Variable 1 norm: 1.16e+01\n",
      "      Variable 2 norm: 1.14e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 126ms/step - loss: 0.9206 - mae: 0.7357\n",
      "Epoch 40/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 89ms/step - loss: 1.1531 - mae: 0.8272\n",
      "Epoch 41/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 83ms/step - loss: 0.7879 - mae: 0.6844\n",
      "Epoch 42/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 2.55e+00\n",
      "      Variable 1 norm: 1.17e+01\n",
      "      Variable 2 norm: 1.14e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 87ms/step - loss: 1.0366 - mae: 0.7065\n",
      "Epoch 43/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 81ms/step - loss: 1.0721 - mae: 0.7106\n",
      "Epoch 44/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 81ms/step - loss: 0.8674 - mae: 0.6662\n",
      "Epoch 45/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 2.57e+00\n",
      "      Variable 1 norm: 1.18e+01\n",
      "      Variable 2 norm: 1.15e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 91ms/step - loss: 0.9391 - mae: 0.8092\n",
      "Epoch 46/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 86ms/step - loss: 0.9569 - mae: 0.8177\n",
      "Epoch 47/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 82ms/step - loss: 0.8652 - mae: 0.7452\n",
      "Epoch 48/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 2.59e+00\n",
      "      Variable 1 norm: 1.19e+01\n",
      "      Variable 2 norm: 1.15e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 91ms/step - loss: 0.8828 - mae: 0.6916\n",
      "Epoch 49/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 80ms/step - loss: 0.8905 - mae: 0.7056\n",
      "Epoch 50/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 80ms/step - loss: 0.8394 - mae: 0.6881\n",
      "Epoch 51/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 2.61e+00\n",
      "      Variable 1 norm: 1.20e+01\n",
      "      Variable 2 norm: 1.15e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 91ms/step - loss: 0.8388 - mae: 0.7338\n",
      "Epoch 52/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 83ms/step - loss: 0.8404 - mae: 0.7409\n",
      "Epoch 53/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 83ms/step - loss: 0.8244 - mae: 0.6793\n",
      "Epoch 54/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 2.63e+00\n",
      "      Variable 1 norm: 1.21e+01\n",
      "      Variable 2 norm: 1.15e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 92ms/step - loss: 0.8282 - mae: 0.6662\n",
      "Epoch 55/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 87ms/step - loss: 0.8281 - mae: 0.6459\n",
      "Epoch 56/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 82ms/step - loss: 0.8144 - mae: 0.6803\n",
      "Epoch 57/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 2.66e+00\n",
      "      Variable 1 norm: 1.22e+01\n",
      "      Variable 2 norm: 1.16e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 91ms/step - loss: 0.8450 - mae: 0.7158\n",
      "Epoch 58/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 85ms/step - loss: 0.8510 - mae: 0.7507\n",
      "Epoch 59/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 84ms/step - loss: 0.8986 - mae: 0.8068\n",
      "Epoch 60/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 2.69e+00\n",
      "      Variable 1 norm: 1.23e+01\n",
      "      Variable 2 norm: 1.16e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 94ms/step - loss: 0.8864 - mae: 0.7882\n",
      "Epoch 61/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 82ms/step - loss: 0.8216 - mae: 0.7030\n",
      "Epoch 62/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 84ms/step - loss: 0.8884 - mae: 0.6694\n",
      "Epoch 63/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 2.72e+00\n",
      "      Variable 1 norm: 1.25e+01\n",
      "      Variable 2 norm: 1.16e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 92ms/step - loss: 0.9240 - mae: 0.6652\n",
      "Epoch 64/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 84ms/step - loss: 0.8842 - mae: 0.6369\n",
      "Epoch 65/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 87ms/step - loss: 0.8672 - mae: 0.6527\n",
      "Epoch 66/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 2.76e+00\n",
      "      Variable 1 norm: 1.26e+01\n",
      "      Variable 2 norm: 1.16e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 92ms/step - loss: 0.8724 - mae: 0.6837\n",
      "Epoch 67/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 84ms/step - loss: 0.8499 - mae: 0.6984\n",
      "Epoch 68/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 83ms/step - loss: 0.8298 - mae: 0.6969\n",
      "Epoch 69/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 2.80e+00\n",
      "      Variable 1 norm: 1.27e+01\n",
      "      Variable 2 norm: 1.16e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 92ms/step - loss: 0.8451 - mae: 0.7065\n",
      "Epoch 70/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 84ms/step - loss: 0.8556 - mae: 0.7182\n",
      "Epoch 71/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 83ms/step - loss: 0.8417 - mae: 0.7138\n",
      "Epoch 72/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 2.85e+00\n",
      "      Variable 1 norm: 1.29e+01\n",
      "      Variable 2 norm: 1.17e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 94ms/step - loss: 0.8286 - mae: 0.7148\n",
      "Epoch 73/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 84ms/step - loss: 0.8320 - mae: 0.7005\n",
      "Epoch 74/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 85ms/step - loss: 0.8373 - mae: 0.6797\n",
      "Epoch 75/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 2.90e+00\n",
      "      Variable 1 norm: 1.30e+01\n",
      "      Variable 2 norm: 1.17e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 92ms/step - loss: 0.8379 - mae: 0.6577\n",
      "Epoch 76/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 85ms/step - loss: 0.8340 - mae: 0.6529\n",
      "Epoch 77/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 89ms/step - loss: 0.8292 - mae: 0.6801\n",
      "Epoch 78/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 2.95e+00\n",
      "      Variable 1 norm: 1.31e+01\n",
      "      Variable 2 norm: 1.17e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 92ms/step - loss: 0.8335 - mae: 0.7263\n",
      "Epoch 79/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 86ms/step - loss: 0.8453 - mae: 0.7492\n",
      "Epoch 80/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 83ms/step - loss: 0.8396 - mae: 0.7314\n",
      "Epoch 81/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 3.01e+00\n",
      "      Variable 1 norm: 1.32e+01\n",
      "      Variable 2 norm: 1.17e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 98ms/step - loss: 0.8250 - mae: 0.6877\n",
      "Epoch 82/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 84ms/step - loss: 0.8351 - mae: 0.6457\n",
      "Epoch 83/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 85ms/step - loss: 0.8519 - mae: 0.6427\n",
      "Epoch 84/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 3.06e+00\n",
      "      Variable 1 norm: 1.34e+01\n",
      "      Variable 2 norm: 1.17e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 91ms/step - loss: 0.8500 - mae: 0.6487\n",
      "Epoch 85/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 84ms/step - loss: 0.8333 - mae: 0.6604\n",
      "Epoch 86/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 135ms/step - loss: 0.8230 - mae: 0.6884\n",
      "Epoch 87/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 3.12e+00\n",
      "      Variable 1 norm: 1.35e+01\n",
      "      Variable 2 norm: 1.17e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 292ms/step - loss: 0.8310 - mae: 0.7131\n",
      "Epoch 88/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 451ms/step - loss: 0.8361 - mae: 0.7248\n",
      "Epoch 89/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 152ms/step - loss: 0.8305 - mae: 0.7179\n",
      "Epoch 90/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 3.18e+00\n",
      "      Variable 1 norm: 1.35e+01\n",
      "      Variable 2 norm: 1.17e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 106ms/step - loss: 0.8241 - mae: 0.6974\n",
      "Epoch 91/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 86ms/step - loss: 0.8252 - mae: 0.6759\n",
      "Epoch 92/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 86ms/step - loss: 0.8277 - mae: 0.6676\n",
      "Epoch 93/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 3.24e+00\n",
      "      Variable 1 norm: 1.36e+01\n",
      "      Variable 2 norm: 1.18e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 93ms/step - loss: 0.8252 - mae: 0.6719\n",
      "Epoch 94/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 84ms/step - loss: 0.8224 - mae: 0.6861\n",
      "Epoch 95/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 85ms/step - loss: 0.8237 - mae: 0.7011\n",
      "Epoch 96/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 3.30e+00\n",
      "      Variable 1 norm: 1.37e+01\n",
      "      Variable 2 norm: 1.18e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 100ms/step - loss: 0.8293 - mae: 0.7123\n",
      "Epoch 97/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 86ms/step - loss: 0.8279 - mae: 0.7097\n",
      "Epoch 98/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 88ms/step - loss: 0.8219 - mae: 0.6949\n",
      "Epoch 99/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 3.35e+00\n",
      "      Variable 1 norm: 1.37e+01\n",
      "      Variable 2 norm: 1.18e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 94ms/step - loss: 0.8214 - mae: 0.6780\n",
      "Epoch 100/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 84ms/step - loss: 0.8242 - mae: 0.6691\n",
      "Epoch 101/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 89ms/step - loss: 0.8234 - mae: 0.6696\n",
      "Epoch 102/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 3.41e+00\n",
      "      Variable 1 norm: 1.38e+01\n",
      "      Variable 2 norm: 1.18e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 98ms/step - loss: 0.8204 - mae: 0.6787\n",
      "Epoch 103/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 85ms/step - loss: 0.8201 - mae: 0.6936\n",
      "Epoch 104/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 84ms/step - loss: 0.8220 - mae: 0.7033\n",
      "Epoch 105/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 3.47e+00\n",
      "      Variable 1 norm: 1.39e+01\n",
      "      Variable 2 norm: 1.18e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 94ms/step - loss: 0.8218 - mae: 0.7012\n",
      "Epoch 106/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 84ms/step - loss: 0.8205 - mae: 0.6910\n",
      "Epoch 107/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 92ms/step - loss: 0.8203 - mae: 0.6827\n",
      "Epoch 108/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 3.53e+00\n",
      "      Variable 1 norm: 1.39e+01\n",
      "      Variable 2 norm: 1.18e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 92ms/step - loss: 0.8198 - mae: 0.6826\n",
      "Epoch 109/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 83ms/step - loss: 0.8190 - mae: 0.6856\n",
      "Epoch 110/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 87ms/step - loss: 0.8195 - mae: 0.6898\n",
      "Epoch 111/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 3.59e+00\n",
      "      Variable 1 norm: 1.40e+01\n",
      "      Variable 2 norm: 1.18e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 93ms/step - loss: 0.8195 - mae: 0.6908\n",
      "Epoch 112/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 86ms/step - loss: 0.8191 - mae: 0.6882\n",
      "Epoch 113/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 85ms/step - loss: 0.8190 - mae: 0.6855\n",
      "Epoch 114/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 3.66e+00\n",
      "      Variable 1 norm: 1.40e+01\n",
      "      Variable 2 norm: 1.19e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 94ms/step - loss: 0.8188 - mae: 0.6850\n",
      "Epoch 115/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 87ms/step - loss: 0.8186 - mae: 0.6883\n",
      "Epoch 116/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 86ms/step - loss: 0.8186 - mae: 0.6912\n",
      "Epoch 117/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 3.73e+00\n",
      "      Variable 1 norm: 1.41e+01\n",
      "      Variable 2 norm: 1.19e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 93ms/step - loss: 0.8186 - mae: 0.6924\n",
      "Epoch 118/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 85ms/step - loss: 0.8186 - mae: 0.6913\n",
      "Epoch 119/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 84ms/step - loss: 0.8187 - mae: 0.6890\n",
      "Epoch 120/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 3.80e+00\n",
      "      Variable 1 norm: 1.41e+01\n",
      "      Variable 2 norm: 1.19e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 92ms/step - loss: 0.8190 - mae: 0.6847\n",
      "Epoch 121/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 85ms/step - loss: 0.8189 - mae: 0.6827\n",
      "Epoch 122/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 84ms/step - loss: 0.8185 - mae: 0.6839\n",
      "Epoch 123/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 3.87e+00\n",
      "      Variable 1 norm: 1.42e+01\n",
      "      Variable 2 norm: 1.19e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 95ms/step - loss: 0.8185 - mae: 0.6868\n",
      "Epoch 124/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 88ms/step - loss: 0.8188 - mae: 0.6901\n",
      "Epoch 125/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 102ms/step - loss: 0.8193 - mae: 0.6901\n",
      "Epoch 126/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 3.95e+00\n",
      "      Variable 1 norm: 1.42e+01\n",
      "      Variable 2 norm: 1.19e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 94ms/step - loss: 0.8193 - mae: 0.6881\n",
      "Epoch 127/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 87ms/step - loss: 0.8191 - mae: 0.6856\n",
      "Epoch 128/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 87ms/step - loss: 0.8193 - mae: 0.6826\n",
      "Epoch 129/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 4.03e+00\n",
      "      Variable 1 norm: 1.43e+01\n",
      "      Variable 2 norm: 1.19e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 99ms/step - loss: 0.8191 - mae: 0.6801\n",
      "Epoch 130/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 84ms/step - loss: 0.8190 - mae: 0.6821\n",
      "Epoch 131/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 84ms/step - loss: 0.8189 - mae: 0.6852\n",
      "Epoch 132/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 4.11e+00\n",
      "      Variable 1 norm: 1.44e+01\n",
      "      Variable 2 norm: 1.19e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 92ms/step - loss: 0.8195 - mae: 0.6894\n",
      "Epoch 133/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 84ms/step - loss: 0.8199 - mae: 0.6947\n",
      "Epoch 134/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 85ms/step - loss: 0.8200 - mae: 0.6942\n",
      "Epoch 135/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 4.19e+00\n",
      "      Variable 1 norm: 1.44e+01\n",
      "      Variable 2 norm: 1.20e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 98ms/step - loss: 0.8196 - mae: 0.6876\n",
      "Epoch 136/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 97ms/step - loss: 0.8198 - mae: 0.6805\n",
      "Epoch 137/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 105ms/step - loss: 0.8209 - mae: 0.6795\n",
      "Epoch 138/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 4.28e+00\n",
      "      Variable 1 norm: 1.45e+01\n",
      "      Variable 2 norm: 1.20e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 101ms/step - loss: 0.8206 - mae: 0.6849\n",
      "Epoch 139/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 96ms/step - loss: 0.8211 - mae: 0.6901\n",
      "Epoch 140/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 92ms/step - loss: 0.8214 - mae: 0.6970\n",
      "Epoch 141/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 4.37e+00\n",
      "      Variable 1 norm: 1.46e+01\n",
      "      Variable 2 norm: 1.20e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 94ms/step - loss: 0.8217 - mae: 0.6897\n",
      "Epoch 142/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 111ms/step - loss: 0.8218 - mae: 0.6895\n",
      "Epoch 143/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 89ms/step - loss: 0.8219 - mae: 0.6858\n",
      "Epoch 144/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 4.45e+00\n",
      "      Variable 1 norm: 1.46e+01\n",
      "      Variable 2 norm: 1.20e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 96ms/step - loss: 0.8210 - mae: 0.6888\n",
      "Epoch 145/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 87ms/step - loss: 0.8197 - mae: 0.6894\n",
      "Epoch 146/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 92ms/step - loss: 0.8195 - mae: 0.6951\n",
      "Epoch 147/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 4.54e+00\n",
      "      Variable 1 norm: 1.47e+01\n",
      "      Variable 2 norm: 1.20e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 100ms/step - loss: 0.8198 - mae: 0.6930\n",
      "Epoch 148/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 89ms/step - loss: 0.8211 - mae: 0.6826\n",
      "Epoch 149/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 85ms/step - loss: 0.8229 - mae: 0.6818\n",
      "Epoch 150/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 4.63e+00\n",
      "      Variable 1 norm: 1.48e+01\n",
      "      Variable 2 norm: 1.20e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 100ms/step - loss: 0.8226 - mae: 0.6781\n",
      "Epoch 151/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 95ms/step - loss: 0.8217 - mae: 0.6829\n",
      "Epoch 152/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 86ms/step - loss: 0.8221 - mae: 0.6922\n",
      "Epoch 153/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 4.71e+00\n",
      "      Variable 1 norm: 1.48e+01\n",
      "      Variable 2 norm: 1.20e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 98ms/step - loss: 0.8241 - mae: 0.6968\n",
      "Epoch 154/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 86ms/step - loss: 0.8279 - mae: 0.7035\n",
      "Epoch 155/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 85ms/step - loss: 0.8308 - mae: 0.7026\n",
      "Epoch 156/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 4.79e+00\n",
      "      Variable 1 norm: 1.48e+01\n",
      "      Variable 2 norm: 1.21e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 94ms/step - loss: 0.8320 - mae: 0.6980\n",
      "Epoch 157/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 87ms/step - loss: 0.8310 - mae: 0.6892\n",
      "Epoch 158/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 86ms/step - loss: 0.8300 - mae: 0.6757\n",
      "Epoch 159/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 4.87e+00\n",
      "      Variable 1 norm: 1.49e+01\n",
      "      Variable 2 norm: 1.21e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 95ms/step - loss: 0.8302 - mae: 0.6712\n",
      "Epoch 160/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 87ms/step - loss: 0.8291 - mae: 0.6788\n",
      "Epoch 161/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 87ms/step - loss: 0.8275 - mae: 0.6891\n",
      "Epoch 162/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 4.96e+00\n",
      "      Variable 1 norm: 1.49e+01\n",
      "      Variable 2 norm: 1.21e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 101ms/step - loss: 0.8293 - mae: 0.6990\n",
      "Epoch 163/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 84ms/step - loss: 0.8294 - mae: 0.7035\n",
      "Epoch 164/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 83ms/step - loss: 0.8280 - mae: 0.6986\n",
      "Epoch 165/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 5.05e+00\n",
      "      Variable 1 norm: 1.50e+01\n",
      "      Variable 2 norm: 1.21e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 181ms/step - loss: 0.8254 - mae: 0.6849\n",
      "Epoch 166/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 140ms/step - loss: 0.8231 - mae: 0.6831\n",
      "Epoch 167/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 289ms/step - loss: 0.8215 - mae: 0.6858\n",
      "Epoch 168/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 5.13e+00\n",
      "      Variable 1 norm: 1.50e+01\n",
      "      Variable 2 norm: 1.21e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 173ms/step - loss: 0.8233 - mae: 0.6919\n",
      "Epoch 169/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 90ms/step - loss: 0.8244 - mae: 0.6951\n",
      "Epoch 170/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 87ms/step - loss: 0.8235 - mae: 0.6899\n",
      "Epoch 171/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 5.22e+00\n",
      "      Variable 1 norm: 1.50e+01\n",
      "      Variable 2 norm: 1.21e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 94ms/step - loss: 0.8227 - mae: 0.6815\n",
      "Epoch 172/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 85ms/step - loss: 0.8220 - mae: 0.6841\n",
      "Epoch 173/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 85ms/step - loss: 0.8215 - mae: 0.6903\n",
      "Epoch 174/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 5.30e+00\n",
      "      Variable 1 norm: 1.50e+01\n",
      "      Variable 2 norm: 1.21e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 98ms/step - loss: 0.8217 - mae: 0.6938\n",
      "Epoch 175/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 84ms/step - loss: 0.8217 - mae: 0.6946\n",
      "Epoch 176/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 84ms/step - loss: 0.8210 - mae: 0.6857\n",
      "Epoch 177/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 5.39e+00\n",
      "      Variable 1 norm: 1.51e+01\n",
      "      Variable 2 norm: 1.21e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 93ms/step - loss: 0.8207 - mae: 0.6816\n",
      "Epoch 178/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 84ms/step - loss: 0.8203 - mae: 0.6886\n",
      "Epoch 179/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 85ms/step - loss: 0.8200 - mae: 0.6926\n",
      "Epoch 180/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 5.49e+00\n",
      "      Variable 1 norm: 1.51e+01\n",
      "      Variable 2 norm: 1.22e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 97ms/step - loss: 0.8193 - mae: 0.6888\n",
      "Epoch 181/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 86ms/step - loss: 0.8202 - mae: 0.6837\n",
      "Epoch 182/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 88ms/step - loss: 0.8190 - mae: 0.6839\n",
      "Epoch 183/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 5.59e+00\n",
      "      Variable 1 norm: 1.51e+01\n",
      "      Variable 2 norm: 1.22e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 100ms/step - loss: 0.8191 - mae: 0.6903\n",
      "Epoch 184/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 85ms/step - loss: 0.8192 - mae: 0.6900\n",
      "Epoch 185/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 87ms/step - loss: 0.8192 - mae: 0.6884\n",
      "Epoch 186/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 5.69e+00\n",
      "      Variable 1 norm: 1.52e+01\n",
      "      Variable 2 norm: 1.22e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 96ms/step - loss: 0.8196 - mae: 0.6904\n",
      "Epoch 187/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 83ms/step - loss: 0.8202 - mae: 0.6942\n",
      "Epoch 188/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 85ms/step - loss: 0.8188 - mae: 0.6821\n",
      "Epoch 189/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 5.80e+00\n",
      "      Variable 1 norm: 1.52e+01\n",
      "      Variable 2 norm: 1.22e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 93ms/step - loss: 0.8209 - mae: 0.6789\n",
      "Epoch 190/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 85ms/step - loss: 0.8188 - mae: 0.6862\n",
      "Epoch 191/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 91ms/step - loss: 0.8194 - mae: 0.6938\n",
      "Epoch 192/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 5.90e+00\n",
      "      Variable 1 norm: 1.52e+01\n",
      "      Variable 2 norm: 1.22e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 91ms/step - loss: 0.8188 - mae: 0.6890\n",
      "Epoch 193/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 84ms/step - loss: 0.8194 - mae: 0.6787\n",
      "Epoch 194/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 85ms/step - loss: 0.8203 - mae: 0.6913\n",
      "Epoch 195/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 6.01e+00\n",
      "      Variable 1 norm: 1.53e+01\n",
      "      Variable 2 norm: 1.22e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 94ms/step - loss: 0.8204 - mae: 0.7013\n",
      "Epoch 196/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 86ms/step - loss: 0.8195 - mae: 0.6855\n",
      "Epoch 197/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 92ms/step - loss: 0.8214 - mae: 0.6783\n",
      "Epoch 198/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 6.11e+00\n",
      "      Variable 1 norm: 1.53e+01\n",
      "      Variable 2 norm: 1.23e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 99ms/step - loss: 0.8198 - mae: 0.6877\n",
      "Epoch 199/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 83ms/step - loss: 0.8231 - mae: 0.6938\n",
      "Epoch 200/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 85ms/step - loss: 0.8227 - mae: 0.6877\n",
      "Epoch 201/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 6.22e+00\n",
      "      Variable 1 norm: 1.54e+01\n",
      "      Variable 2 norm: 1.23e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 111ms/step - loss: 0.8201 - mae: 0.6805\n",
      "Epoch 202/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 87ms/step - loss: 0.8225 - mae: 0.6852\n",
      "Epoch 203/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 85ms/step - loss: 0.8191 - mae: 0.6927\n",
      "Epoch 204/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 6.33e+00\n",
      "      Variable 1 norm: 1.55e+01\n",
      "      Variable 2 norm: 1.23e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 106ms/step - loss: 0.8242 - mae: 0.6961\n",
      "Epoch 205/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 85ms/step - loss: 0.8228 - mae: 0.6887\n",
      "Epoch 206/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 83ms/step - loss: 0.8205 - mae: 0.6779\n",
      "Epoch 207/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 6.44e+00\n",
      "      Variable 1 norm: 1.55e+01\n",
      "      Variable 2 norm: 1.23e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 93ms/step - loss: 0.8229 - mae: 0.6828\n",
      "Epoch 208/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 89ms/step - loss: 0.8197 - mae: 0.6944\n",
      "Epoch 209/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 86ms/step - loss: 0.8255 - mae: 0.7012\n",
      "Epoch 210/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 6.55e+00\n",
      "      Variable 1 norm: 1.56e+01\n",
      "      Variable 2 norm: 1.23e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 97ms/step - loss: 0.8260 - mae: 0.6923\n",
      "Epoch 211/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 89ms/step - loss: 0.8207 - mae: 0.6749\n",
      "Epoch 212/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 84ms/step - loss: 0.8294 - mae: 0.6778\n",
      "Epoch 213/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 6.67e+00\n",
      "      Variable 1 norm: 1.56e+01\n",
      "      Variable 2 norm: 1.23e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 102ms/step - loss: 0.8319 - mae: 0.6888\n",
      "Epoch 214/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 87ms/step - loss: 0.8224 - mae: 0.6959\n",
      "Epoch 215/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 85ms/step - loss: 0.8245 - mae: 0.6981\n",
      "Epoch 216/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 6.78e+00\n",
      "      Variable 1 norm: 1.56e+01\n",
      "      Variable 2 norm: 1.24e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 95ms/step - loss: 0.8310 - mae: 0.6962\n",
      "Epoch 217/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 85ms/step - loss: 0.8253 - mae: 0.6894\n",
      "Epoch 218/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 84ms/step - loss: 0.8205 - mae: 0.6850\n",
      "Epoch 219/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 6.90e+00\n",
      "      Variable 1 norm: 1.57e+01\n",
      "      Variable 2 norm: 1.24e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 97ms/step - loss: 0.8234 - mae: 0.6855\n",
      "Epoch 220/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 85ms/step - loss: 0.8218 - mae: 0.6849\n",
      "Epoch 221/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 87ms/step - loss: 0.8205 - mae: 0.6851\n",
      "Epoch 222/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 7.02e+00\n",
      "      Variable 1 norm: 1.57e+01\n",
      "      Variable 2 norm: 1.24e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 94ms/step - loss: 0.8205 - mae: 0.6894\n",
      "Epoch 223/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 88ms/step - loss: 0.8198 - mae: 0.6955\n",
      "Epoch 224/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 89ms/step - loss: 0.8198 - mae: 0.6918\n",
      "Epoch 225/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 7.12e+00\n",
      "      Variable 1 norm: 1.57e+01\n",
      "      Variable 2 norm: 1.24e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 121ms/step - loss: 0.8194 - mae: 0.6824\n",
      "Epoch 226/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 88ms/step - loss: 0.8198 - mae: 0.6808\n",
      "Epoch 227/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 86ms/step - loss: 0.8192 - mae: 0.6869\n",
      "Epoch 228/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 7.23e+00\n",
      "      Variable 1 norm: 1.58e+01\n",
      "      Variable 2 norm: 1.24e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 95ms/step - loss: 0.8197 - mae: 0.6934\n",
      "Epoch 229/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 86ms/step - loss: 0.8195 - mae: 0.6919\n",
      "Epoch 230/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 89ms/step - loss: 0.8193 - mae: 0.6876\n",
      "Epoch 231/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 7.33e+00\n",
      "      Variable 1 norm: 1.58e+01\n",
      "      Variable 2 norm: 1.24e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 93ms/step - loss: 0.8194 - mae: 0.6855\n",
      "Epoch 232/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 85ms/step - loss: 0.8192 - mae: 0.6849\n",
      "Epoch 233/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 86ms/step - loss: 0.8190 - mae: 0.6858\n",
      "Epoch 234/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 7.44e+00\n",
      "      Variable 1 norm: 1.58e+01\n",
      "      Variable 2 norm: 1.24e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 98ms/step - loss: 0.8194 - mae: 0.6893\n",
      "Epoch 235/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 86ms/step - loss: 0.8205 - mae: 0.6917\n",
      "Epoch 236/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 93ms/step - loss: 0.8208 - mae: 0.6930\n",
      "Epoch 237/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 7.54e+00\n",
      "      Variable 1 norm: 1.59e+01\n",
      "      Variable 2 norm: 1.25e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 95ms/step - loss: 0.8204 - mae: 0.6941\n",
      "Epoch 238/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 82ms/step - loss: 0.8196 - mae: 0.6887\n",
      "Epoch 239/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 189ms/step - loss: 0.8203 - mae: 0.6816\n",
      "Epoch 240/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 7.63e+00\n",
      "      Variable 1 norm: 1.59e+01\n",
      "      Variable 2 norm: 1.25e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 786ms/step - loss: 0.8200 - mae: 0.6833\n",
      "Epoch 241/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 319ms/step - loss: 0.8205 - mae: 0.6946\n",
      "Epoch 242/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 101ms/step - loss: 0.8209 - mae: 0.6985\n",
      "Epoch 243/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 7.72e+00\n",
      "      Variable 1 norm: 1.59e+01\n",
      "      Variable 2 norm: 1.25e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 104ms/step - loss: 0.8201 - mae: 0.6916\n",
      "Epoch 244/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 87ms/step - loss: 0.8210 - mae: 0.6818\n",
      "Epoch 245/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 86ms/step - loss: 0.8220 - mae: 0.6822\n",
      "Epoch 246/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 7.80e+00\n",
      "      Variable 1 norm: 1.59e+01\n",
      "      Variable 2 norm: 1.25e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 93ms/step - loss: 0.8222 - mae: 0.6865\n",
      "Epoch 247/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 89ms/step - loss: 0.8206 - mae: 0.6863\n",
      "Epoch 248/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 94ms/step - loss: 0.8205 - mae: 0.6898\n",
      "Epoch 249/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 7.87e+00\n",
      "      Variable 1 norm: 1.59e+01\n",
      "      Variable 2 norm: 1.25e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 144ms/step - loss: 0.8208 - mae: 0.6962\n",
      "Epoch 250/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 89ms/step - loss: 0.8203 - mae: 0.6937\n",
      "Epoch 251/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 87ms/step - loss: 0.8198 - mae: 0.6835\n",
      "Epoch 252/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 7.93e+00\n",
      "      Variable 1 norm: 1.60e+01\n",
      "      Variable 2 norm: 1.25e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 170ms/step - loss: 0.8193 - mae: 0.6823\n",
      "Epoch 253/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 228ms/step - loss: 0.8198 - mae: 0.6876\n",
      "Epoch 254/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 87ms/step - loss: 0.8198 - mae: 0.6891\n",
      "Epoch 255/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 8.00e+00\n",
      "      Variable 1 norm: 1.60e+01\n",
      "      Variable 2 norm: 1.25e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 116ms/step - loss: 0.8199 - mae: 0.6892\n",
      "Epoch 256/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 92ms/step - loss: 0.8197 - mae: 0.6879\n",
      "Epoch 257/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 85ms/step - loss: 0.8196 - mae: 0.6851\n",
      "Epoch 258/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 8.06e+00\n",
      "      Variable 1 norm: 1.60e+01\n",
      "      Variable 2 norm: 1.26e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 95ms/step - loss: 0.8195 - mae: 0.6855\n",
      "Epoch 259/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 84ms/step - loss: 0.8196 - mae: 0.6900\n",
      "Epoch 260/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 89ms/step - loss: 0.8198 - mae: 0.6901\n",
      "Epoch 261/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 8.13e+00\n",
      "      Variable 1 norm: 1.60e+01\n",
      "      Variable 2 norm: 1.26e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 103ms/step - loss: 0.8200 - mae: 0.6856\n",
      "Epoch 262/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 90ms/step - loss: 0.8200 - mae: 0.6848\n",
      "Epoch 263/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 90ms/step - loss: 0.8197 - mae: 0.6884\n",
      "Epoch 264/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 8.20e+00\n",
      "      Variable 1 norm: 1.60e+01\n",
      "      Variable 2 norm: 1.26e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 94ms/step - loss: 0.8196 - mae: 0.6910\n",
      "Epoch 265/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 86ms/step - loss: 0.8193 - mae: 0.6892\n",
      "Epoch 266/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 90ms/step - loss: 0.8194 - mae: 0.6852\n",
      "Epoch 267/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 8.27e+00\n",
      "      Variable 1 norm: 1.60e+01\n",
      "      Variable 2 norm: 1.26e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 93ms/step - loss: 0.8194 - mae: 0.6845\n",
      "Epoch 268/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 90ms/step - loss: 0.8194 - mae: 0.6874\n",
      "Epoch 269/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 90ms/step - loss: 0.8194 - mae: 0.6900\n",
      "Epoch 270/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 8.34e+00\n",
      "      Variable 1 norm: 1.60e+01\n",
      "      Variable 2 norm: 1.26e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 102ms/step - loss: 0.8192 - mae: 0.6884\n",
      "Epoch 271/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 87ms/step - loss: 0.8191 - mae: 0.6846\n",
      "Epoch 272/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 94ms/step - loss: 0.8191 - mae: 0.6857\n",
      "Epoch 273/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 8.41e+00\n",
      "      Variable 1 norm: 1.61e+01\n",
      "      Variable 2 norm: 1.26e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 94ms/step - loss: 0.8191 - mae: 0.6886\n",
      "Epoch 274/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 90ms/step - loss: 0.8192 - mae: 0.6885\n",
      "Epoch 275/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 85ms/step - loss: 0.8194 - mae: 0.6846\n",
      "Epoch 276/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 8.47e+00\n",
      "      Variable 1 norm: 1.61e+01\n",
      "      Variable 2 norm: 1.26e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 94ms/step - loss: 0.8193 - mae: 0.6860\n",
      "Epoch 277/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 91ms/step - loss: 0.8191 - mae: 0.6872\n",
      "Epoch 278/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 88ms/step - loss: 0.8189 - mae: 0.6883\n",
      "Epoch 279/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 8.55e+00\n",
      "      Variable 1 norm: 1.61e+01\n",
      "      Variable 2 norm: 1.26e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 94ms/step - loss: 0.8190 - mae: 0.6879\n",
      "Epoch 280/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 85ms/step - loss: 0.8191 - mae: 0.6845\n",
      "Epoch 281/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 87ms/step - loss: 0.8191 - mae: 0.6882\n",
      "Epoch 282/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 8.63e+00\n",
      "      Variable 1 norm: 1.61e+01\n",
      "      Variable 2 norm: 1.27e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 97ms/step - loss: 0.8189 - mae: 0.6887\n",
      "Epoch 283/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 91ms/step - loss: 0.8190 - mae: 0.6870\n",
      "Epoch 284/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 85ms/step - loss: 0.8192 - mae: 0.6862\n",
      "Epoch 285/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 8.71e+00\n",
      "      Variable 1 norm: 1.61e+01\n",
      "      Variable 2 norm: 1.27e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 98ms/step - loss: 0.8191 - mae: 0.6865\n",
      "Epoch 286/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 122ms/step - loss: 0.8190 - mae: 0.6893\n",
      "Epoch 287/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 341ms/step - loss: 0.8189 - mae: 0.6869\n",
      "Epoch 288/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 8.78e+00\n",
      "      Variable 1 norm: 1.61e+01\n",
      "      Variable 2 norm: 1.27e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 913ms/step - loss: 0.8190 - mae: 0.6852\n",
      "Epoch 289/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 279ms/step - loss: 0.8189 - mae: 0.6885\n",
      "Epoch 290/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 106ms/step - loss: 0.8188 - mae: 0.6883\n",
      "Epoch 291/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 8.85e+00\n",
      "      Variable 1 norm: 1.61e+01\n",
      "      Variable 2 norm: 1.27e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 99ms/step - loss: 0.8188 - mae: 0.6868\n",
      "Epoch 292/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 91ms/step - loss: 0.8189 - mae: 0.6862\n",
      "Epoch 293/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 89ms/step - loss: 0.8190 - mae: 0.6862\n",
      "Epoch 294/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 8.92e+00\n",
      "      Variable 1 norm: 1.62e+01\n",
      "      Variable 2 norm: 1.27e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 97ms/step - loss: 0.8189 - mae: 0.6881\n",
      "Epoch 295/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 87ms/step - loss: 0.8189 - mae: 0.6867\n",
      "Epoch 296/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 89ms/step - loss: 0.8189 - mae: 0.6862\n",
      "Epoch 297/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 8.98e+00\n",
      "      Variable 1 norm: 1.62e+01\n",
      "      Variable 2 norm: 1.28e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 97ms/step - loss: 0.8189 - mae: 0.6885\n",
      "Epoch 298/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 338ms/step - loss: 0.8189 - mae: 0.6871\n",
      "Epoch 299/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 158ms/step - loss: 0.8188 - mae: 0.6868\n",
      "Epoch 300/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.05e+00\n",
      "      Variable 1 norm: 1.62e+01\n",
      "      Variable 2 norm: 1.28e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 100ms/step - loss: 0.8188 - mae: 0.6875\n",
      "🏁 GradientMonitoringCallback: Training completed!\n",
      "   📊 Final stats - Total calls: 300, Gradient checks: 100, Fallbacks: 0\n",
      "   ✅ Gradient monitoring completed successfully!\n",
      "\n",
      "--- Phase 2: Fine-tuning ---\n",
      "🚀 GradientMonitoringCallback: Training started!\n",
      "Epoch 1/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 6s/step - loss: 0.8188 - mae: 0.6867\n",
      "Epoch 2/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 191ms/step - loss: 0.8204 - mae: 0.6923\n",
      "Epoch 3/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.05e+00\n",
      "      Variable 1 norm: 1.62e+01\n",
      "      Variable 2 norm: 1.28e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 121ms/step - loss: 0.8188 - mae: 0.6882\n",
      "Epoch 4/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 94ms/step - loss: 0.8191 - mae: 0.6850\n",
      "Epoch 5/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 91ms/step - loss: 0.8196 - mae: 0.6843\n",
      "Epoch 6/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.06e+00\n",
      "      Variable 1 norm: 1.62e+01\n",
      "      Variable 2 norm: 1.28e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 92ms/step - loss: 0.8192 - mae: 0.6850\n",
      "Epoch 7/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 85ms/step - loss: 0.8187 - mae: 0.6867\n",
      "Epoch 8/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 83ms/step - loss: 0.8188 - mae: 0.6886\n",
      "Epoch 9/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.07e+00\n",
      "      Variable 1 norm: 1.62e+01\n",
      "      Variable 2 norm: 1.28e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 103ms/step - loss: 0.8192 - mae: 0.6899\n",
      "Epoch 10/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 106ms/step - loss: 0.8191 - mae: 0.6898\n",
      "Epoch 11/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 97ms/step - loss: 0.8188 - mae: 0.6887\n",
      "Epoch 12/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.08e+00\n",
      "      Variable 1 norm: 1.62e+01\n",
      "      Variable 2 norm: 1.28e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 99ms/step - loss: 0.8187 - mae: 0.6873\n",
      "Epoch 13/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 91ms/step - loss: 0.8187 - mae: 0.6861\n",
      "Epoch 14/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 91ms/step - loss: 0.8189 - mae: 0.6853\n",
      "Epoch 15/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.09e+00\n",
      "      Variable 1 norm: 1.62e+01\n",
      "      Variable 2 norm: 1.28e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 94ms/step - loss: 0.8189 - mae: 0.6852\n",
      "Epoch 16/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 88ms/step - loss: 0.8187 - mae: 0.6857\n",
      "Epoch 17/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 201ms/step - loss: 0.8186 - mae: 0.6867\n",
      "Epoch 18/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.09e+00\n",
      "      Variable 1 norm: 1.62e+01\n",
      "      Variable 2 norm: 1.28e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 328ms/step - loss: 0.8187 - mae: 0.6878\n",
      "Epoch 19/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 265ms/step - loss: 0.8188 - mae: 0.6886\n",
      "Epoch 20/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 93ms/step - loss: 0.8188 - mae: 0.6886\n",
      "Epoch 21/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.10e+00\n",
      "      Variable 1 norm: 1.62e+01\n",
      "      Variable 2 norm: 1.28e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 98ms/step - loss: 0.8187 - mae: 0.6882\n",
      "Epoch 22/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 93ms/step - loss: 0.8186 - mae: 0.6874\n",
      "Epoch 23/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 89ms/step - loss: 0.8186 - mae: 0.6867\n",
      "Epoch 24/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.11e+00\n",
      "      Variable 1 norm: 1.62e+01\n",
      "      Variable 2 norm: 1.28e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 93ms/step - loss: 0.8187 - mae: 0.6862\n",
      "Epoch 25/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 85ms/step - loss: 0.8187 - mae: 0.6861\n",
      "Epoch 26/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 85ms/step - loss: 0.8186 - mae: 0.6862\n",
      "Epoch 27/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.12e+00\n",
      "      Variable 1 norm: 1.62e+01\n",
      "      Variable 2 norm: 1.28e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 102ms/step - loss: 0.8186 - mae: 0.6866\n",
      "Epoch 28/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 126ms/step - loss: 0.8186 - mae: 0.6872\n",
      "Epoch 29/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 99ms/step - loss: 0.8186 - mae: 0.6878\n",
      "Epoch 30/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.13e+00\n",
      "      Variable 1 norm: 1.62e+01\n",
      "      Variable 2 norm: 1.28e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 105ms/step - loss: 0.8186 - mae: 0.6879\n",
      "Epoch 31/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 87ms/step - loss: 0.8186 - mae: 0.6877\n",
      "Epoch 32/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 86ms/step - loss: 0.8185 - mae: 0.6872\n",
      "Epoch 33/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.13e+00\n",
      "      Variable 1 norm: 1.62e+01\n",
      "      Variable 2 norm: 1.28e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 100ms/step - loss: 0.8185 - mae: 0.6867\n",
      "Epoch 34/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 88ms/step - loss: 0.8185 - mae: 0.6865\n",
      "Epoch 35/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 89ms/step - loss: 0.8185 - mae: 0.6865\n",
      "Epoch 36/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.14e+00\n",
      "      Variable 1 norm: 1.62e+01\n",
      "      Variable 2 norm: 1.28e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 94ms/step - loss: 0.8185 - mae: 0.6868\n",
      "Epoch 37/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 87ms/step - loss: 0.8185 - mae: 0.6870\n",
      "Epoch 38/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 85ms/step - loss: 0.8185 - mae: 0.6873\n",
      "Epoch 39/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.15e+00\n",
      "      Variable 1 norm: 1.62e+01\n",
      "      Variable 2 norm: 1.28e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 99ms/step - loss: 0.8185 - mae: 0.6874\n",
      "Epoch 40/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 85ms/step - loss: 0.8185 - mae: 0.6873\n",
      "Epoch 41/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 84ms/step - loss: 0.8184 - mae: 0.6872\n",
      "Epoch 42/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.15e+00\n",
      "      Variable 1 norm: 1.63e+01\n",
      "      Variable 2 norm: 1.28e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 110ms/step - loss: 0.8184 - mae: 0.6871\n",
      "Epoch 43/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 90ms/step - loss: 0.8184 - mae: 0.6869\n",
      "Epoch 44/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 95ms/step - loss: 0.8184 - mae: 0.6867\n",
      "Epoch 45/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.16e+00\n",
      "      Variable 1 norm: 1.63e+01\n",
      "      Variable 2 norm: 1.28e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 94ms/step - loss: 0.8184 - mae: 0.6866\n",
      "Epoch 46/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 84ms/step - loss: 0.8184 - mae: 0.6868\n",
      "Epoch 47/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 83ms/step - loss: 0.8184 - mae: 0.6870\n",
      "Epoch 48/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.17e+00\n",
      "      Variable 1 norm: 1.63e+01\n",
      "      Variable 2 norm: 1.28e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 92ms/step - loss: 0.8184 - mae: 0.6873\n",
      "Epoch 49/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 84ms/step - loss: 0.8183 - mae: 0.6874\n",
      "Epoch 50/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 88ms/step - loss: 0.8183 - mae: 0.6873\n",
      "Epoch 51/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.18e+00\n",
      "      Variable 1 norm: 1.63e+01\n",
      "      Variable 2 norm: 1.28e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 93ms/step - loss: 0.8183 - mae: 0.6871\n",
      "Epoch 52/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 84ms/step - loss: 0.8183 - mae: 0.6868\n",
      "Epoch 53/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 84ms/step - loss: 0.8183 - mae: 0.6867\n",
      "Epoch 54/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.18e+00\n",
      "      Variable 1 norm: 1.63e+01\n",
      "      Variable 2 norm: 1.28e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 95ms/step - loss: 0.8183 - mae: 0.6870\n",
      "Epoch 55/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 84ms/step - loss: 0.8182 - mae: 0.6873\n",
      "Epoch 56/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 90ms/step - loss: 0.8182 - mae: 0.6874\n",
      "Epoch 57/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.19e+00\n",
      "      Variable 1 norm: 1.63e+01\n",
      "      Variable 2 norm: 1.28e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 98ms/step - loss: 0.8182 - mae: 0.6873\n",
      "Epoch 58/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 84ms/step - loss: 0.8182 - mae: 0.6871\n",
      "Epoch 59/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 86ms/step - loss: 0.8182 - mae: 0.6870\n",
      "Epoch 60/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.20e+00\n",
      "      Variable 1 norm: 1.63e+01\n",
      "      Variable 2 norm: 1.28e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 94ms/step - loss: 0.8181 - mae: 0.6868\n",
      "Epoch 61/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 84ms/step - loss: 0.8181 - mae: 0.6866\n",
      "Epoch 62/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 87ms/step - loss: 0.8181 - mae: 0.6867\n",
      "Epoch 63/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.20e+00\n",
      "      Variable 1 norm: 1.63e+01\n",
      "      Variable 2 norm: 1.28e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 92ms/step - loss: 0.8180 - mae: 0.6870\n",
      "Epoch 64/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 84ms/step - loss: 0.8180 - mae: 0.6873\n",
      "Epoch 65/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 85ms/step - loss: 0.8180 - mae: 0.6873\n",
      "Epoch 66/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.21e+00\n",
      "      Variable 1 norm: 1.63e+01\n",
      "      Variable 2 norm: 1.28e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 94ms/step - loss: 0.8179 - mae: 0.6871\n",
      "Epoch 67/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 87ms/step - loss: 0.8178 - mae: 0.6866\n",
      "Epoch 68/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 91ms/step - loss: 0.8178 - mae: 0.6864\n",
      "Epoch 69/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.22e+00\n",
      "      Variable 1 norm: 1.63e+01\n",
      "      Variable 2 norm: 1.28e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 95ms/step - loss: 0.8177 - mae: 0.6866\n",
      "Epoch 70/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 94ms/step - loss: 0.8176 - mae: 0.6870\n",
      "Epoch 71/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 86ms/step - loss: 0.8175 - mae: 0.6872\n",
      "Epoch 72/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.22e+00\n",
      "      Variable 1 norm: 1.63e+01\n",
      "      Variable 2 norm: 1.28e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 93ms/step - loss: 0.8174 - mae: 0.6870\n",
      "Epoch 73/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 85ms/step - loss: 0.8173 - mae: 0.6865\n",
      "Epoch 74/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 87ms/step - loss: 0.8172 - mae: 0.6862\n",
      "Epoch 75/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.23e+00\n",
      "      Variable 1 norm: 1.63e+01\n",
      "      Variable 2 norm: 1.28e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 93ms/step - loss: 0.8171 - mae: 0.6861\n",
      "Epoch 76/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 84ms/step - loss: 0.8170 - mae: 0.6864\n",
      "Epoch 77/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 88ms/step - loss: 0.8169 - mae: 0.6865\n",
      "Epoch 78/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.24e+00\n",
      "      Variable 1 norm: 1.63e+01\n",
      "      Variable 2 norm: 1.28e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 95ms/step - loss: 0.8167 - mae: 0.6864\n",
      "Epoch 79/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 97ms/step - loss: 0.8166 - mae: 0.6862\n",
      "Epoch 80/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 84ms/step - loss: 0.8164 - mae: 0.6860\n",
      "Epoch 81/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.24e+00\n",
      "      Variable 1 norm: 1.63e+01\n",
      "      Variable 2 norm: 1.28e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 92ms/step - loss: 0.8163 - mae: 0.6859\n",
      "Epoch 82/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 86ms/step - loss: 0.8160 - mae: 0.6860\n",
      "Epoch 83/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 83ms/step - loss: 0.8158 - mae: 0.6861\n",
      "Epoch 84/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.25e+00\n",
      "      Variable 1 norm: 1.63e+01\n",
      "      Variable 2 norm: 1.28e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 96ms/step - loss: 0.8155 - mae: 0.6861\n",
      "Epoch 85/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 84ms/step - loss: 0.8153 - mae: 0.6858\n",
      "Epoch 86/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 87ms/step - loss: 0.8150 - mae: 0.6851\n",
      "Epoch 87/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.25e+00\n",
      "      Variable 1 norm: 1.63e+01\n",
      "      Variable 2 norm: 1.28e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 93ms/step - loss: 0.8147 - mae: 0.6844\n",
      "Epoch 88/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 84ms/step - loss: 0.8144 - mae: 0.6840\n",
      "Epoch 89/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 87ms/step - loss: 0.8140 - mae: 0.6840\n",
      "Epoch 90/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.26e+00\n",
      "      Variable 1 norm: 1.63e+01\n",
      "      Variable 2 norm: 1.28e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 103ms/step - loss: 0.8135 - mae: 0.6842\n",
      "Epoch 91/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 85ms/step - loss: 0.8130 - mae: 0.6846\n",
      "Epoch 92/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 85ms/step - loss: 0.8124 - mae: 0.6845\n",
      "Epoch 93/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.26e+00\n",
      "      Variable 1 norm: 1.63e+01\n",
      "      Variable 2 norm: 1.28e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 92ms/step - loss: 0.8118 - mae: 0.6842\n",
      "Epoch 94/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 85ms/step - loss: 0.8110 - mae: 0.6830\n",
      "Epoch 95/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 89ms/step - loss: 0.8102 - mae: 0.6824\n",
      "Epoch 96/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.27e+00\n",
      "      Variable 1 norm: 1.63e+01\n",
      "      Variable 2 norm: 1.28e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 93ms/step - loss: 0.8092 - mae: 0.6808\n",
      "Epoch 97/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 89ms/step - loss: 0.8080 - mae: 0.6800\n",
      "Epoch 98/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 85ms/step - loss: 0.8066 - mae: 0.6808\n",
      "Epoch 99/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.27e+00\n",
      "      Variable 1 norm: 1.63e+01\n",
      "      Variable 2 norm: 1.28e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 96ms/step - loss: 0.8049 - mae: 0.6782\n",
      "Epoch 100/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 89ms/step - loss: 0.8033 - mae: 0.6811\n",
      "Epoch 101/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 92ms/step - loss: 0.8012 - mae: 0.6772\n",
      "Epoch 102/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.28e+00\n",
      "      Variable 1 norm: 1.63e+01\n",
      "      Variable 2 norm: 1.28e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 96ms/step - loss: 0.7992 - mae: 0.6748\n",
      "Epoch 103/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 85ms/step - loss: 0.7970 - mae: 0.6750\n",
      "Epoch 104/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 85ms/step - loss: 0.7946 - mae: 0.6698\n",
      "Epoch 105/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.28e+00\n",
      "      Variable 1 norm: 1.63e+01\n",
      "      Variable 2 norm: 1.28e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 93ms/step - loss: 0.7923 - mae: 0.6704\n",
      "Epoch 106/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 83ms/step - loss: 0.7903 - mae: 0.6707\n",
      "Epoch 107/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 83ms/step - loss: 0.7884 - mae: 0.6682\n",
      "Epoch 108/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.29e+00\n",
      "      Variable 1 norm: 1.63e+01\n",
      "      Variable 2 norm: 1.28e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 95ms/step - loss: 0.7871 - mae: 0.6683\n",
      "Epoch 109/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 86ms/step - loss: 0.7860 - mae: 0.6671\n",
      "Epoch 110/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 86ms/step - loss: 0.7856 - mae: 0.6665\n",
      "Epoch 111/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.30e+00\n",
      "      Variable 1 norm: 1.63e+01\n",
      "      Variable 2 norm: 1.28e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 95ms/step - loss: 0.7849 - mae: 0.6654\n",
      "Epoch 112/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 89ms/step - loss: 0.7839 - mae: 0.6629\n",
      "Epoch 113/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 89ms/step - loss: 0.7831 - mae: 0.6605\n",
      "Epoch 114/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.30e+00\n",
      "      Variable 1 norm: 1.63e+01\n",
      "      Variable 2 norm: 1.28e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 92ms/step - loss: 0.7826 - mae: 0.6597\n",
      "Epoch 115/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 86ms/step - loss: 0.7823 - mae: 0.6587\n",
      "Epoch 116/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 84ms/step - loss: 0.7824 - mae: 0.6579\n",
      "Epoch 117/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.31e+00\n",
      "      Variable 1 norm: 1.63e+01\n",
      "      Variable 2 norm: 1.28e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 94ms/step - loss: 0.7823 - mae: 0.6572\n",
      "Epoch 118/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 83ms/step - loss: 0.7825 - mae: 0.6566\n",
      "Epoch 119/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 97ms/step - loss: 0.7831 - mae: 0.6573\n",
      "Epoch 120/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.31e+00\n",
      "      Variable 1 norm: 1.63e+01\n",
      "      Variable 2 norm: 1.28e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 249ms/step - loss: 0.7836 - mae: 0.6593\n",
      "Epoch 121/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 206ms/step - loss: 0.7871 - mae: 0.6669\n",
      "Epoch 122/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 192ms/step - loss: 0.7881 - mae: 0.6674\n",
      "Epoch 123/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.31e+00\n",
      "      Variable 1 norm: 1.63e+01\n",
      "      Variable 2 norm: 1.28e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 121ms/step - loss: 0.7878 - mae: 0.6642\n",
      "Epoch 124/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 88ms/step - loss: 0.7877 - mae: 0.6617\n",
      "Epoch 125/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 85ms/step - loss: 0.7869 - mae: 0.6600\n",
      "Epoch 126/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.32e+00\n",
      "      Variable 1 norm: 1.63e+01\n",
      "      Variable 2 norm: 1.28e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 95ms/step - loss: 0.7858 - mae: 0.6602\n",
      "Epoch 127/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 85ms/step - loss: 0.7849 - mae: 0.6610\n",
      "Epoch 128/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 97ms/step - loss: 0.7842 - mae: 0.6616\n",
      "Epoch 129/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.32e+00\n",
      "      Variable 1 norm: 1.63e+01\n",
      "      Variable 2 norm: 1.28e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 99ms/step - loss: 0.7842 - mae: 0.6624\n",
      "Epoch 130/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 89ms/step - loss: 0.7856 - mae: 0.6642\n",
      "Epoch 131/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 90ms/step - loss: 0.7873 - mae: 0.6654\n",
      "Epoch 132/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.32e+00\n",
      "      Variable 1 norm: 1.63e+01\n",
      "      Variable 2 norm: 1.28e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 96ms/step - loss: 0.7906 - mae: 0.6684\n",
      "Epoch 133/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 89ms/step - loss: 0.7914 - mae: 0.6704\n",
      "Epoch 134/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 90ms/step - loss: 0.7894 - mae: 0.6697\n",
      "Epoch 135/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.32e+00\n",
      "      Variable 1 norm: 1.64e+01\n",
      "      Variable 2 norm: 1.28e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 97ms/step - loss: 0.7901 - mae: 0.6698\n",
      "Epoch 136/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 85ms/step - loss: 0.7892 - mae: 0.6679\n",
      "Epoch 137/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 87ms/step - loss: 0.7858 - mae: 0.6639\n",
      "Epoch 138/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.32e+00\n",
      "      Variable 1 norm: 1.64e+01\n",
      "      Variable 2 norm: 1.28e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 95ms/step - loss: 0.7838 - mae: 0.6572\n",
      "Epoch 139/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 85ms/step - loss: 0.7835 - mae: 0.6525\n",
      "Epoch 140/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 89ms/step - loss: 0.7837 - mae: 0.6512\n",
      "Epoch 141/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.33e+00\n",
      "      Variable 1 norm: 1.64e+01\n",
      "      Variable 2 norm: 1.28e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 95ms/step - loss: 0.7835 - mae: 0.6517\n",
      "Epoch 142/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 104ms/step - loss: 0.7831 - mae: 0.6535\n",
      "Epoch 143/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 90ms/step - loss: 0.7828 - mae: 0.6562\n",
      "Epoch 144/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.33e+00\n",
      "      Variable 1 norm: 1.64e+01\n",
      "      Variable 2 norm: 1.28e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 99ms/step - loss: 0.7844 - mae: 0.6627\n",
      "Epoch 145/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 88ms/step - loss: 0.7834 - mae: 0.6650\n",
      "Epoch 146/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 83ms/step - loss: 0.7838 - mae: 0.6676\n",
      "Epoch 147/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.33e+00\n",
      "      Variable 1 norm: 1.64e+01\n",
      "      Variable 2 norm: 1.28e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 98ms/step - loss: 0.7846 - mae: 0.6682\n",
      "Epoch 148/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 84ms/step - loss: 0.7859 - mae: 0.6669\n",
      "Epoch 149/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 85ms/step - loss: 0.7875 - mae: 0.6635\n",
      "Epoch 150/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.33e+00\n",
      "      Variable 1 norm: 1.64e+01\n",
      "      Variable 2 norm: 1.28e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 93ms/step - loss: 0.7893 - mae: 0.6589\n",
      "Epoch 151/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 90ms/step - loss: 0.7908 - mae: 0.6545\n",
      "Epoch 152/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 90ms/step - loss: 0.7920 - mae: 0.6515\n",
      "Epoch 153/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.33e+00\n",
      "      Variable 1 norm: 1.64e+01\n",
      "      Variable 2 norm: 1.28e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 105ms/step - loss: 0.7929 - mae: 0.6504\n",
      "Epoch 154/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 90ms/step - loss: 0.7934 - mae: 0.6518\n",
      "Epoch 155/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 86ms/step - loss: 0.7932 - mae: 0.6550\n",
      "Epoch 156/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.33e+00\n",
      "      Variable 1 norm: 1.64e+01\n",
      "      Variable 2 norm: 1.28e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 97ms/step - loss: 0.7922 - mae: 0.6580\n",
      "Epoch 157/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 84ms/step - loss: 0.7912 - mae: 0.6602\n",
      "Epoch 158/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 84ms/step - loss: 0.7908 - mae: 0.6623\n",
      "Epoch 159/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.33e+00\n",
      "      Variable 1 norm: 1.64e+01\n",
      "      Variable 2 norm: 1.29e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 97ms/step - loss: 0.7904 - mae: 0.6632\n",
      "Epoch 160/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 85ms/step - loss: 0.7896 - mae: 0.6629\n",
      "Epoch 161/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 83ms/step - loss: 0.7889 - mae: 0.6625\n",
      "Epoch 162/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.33e+00\n",
      "      Variable 1 norm: 1.64e+01\n",
      "      Variable 2 norm: 1.29e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 99ms/step - loss: 0.7884 - mae: 0.6626\n",
      "Epoch 163/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 92ms/step - loss: 0.7880 - mae: 0.6628\n",
      "Epoch 164/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 84ms/step - loss: 0.7880 - mae: 0.6625\n",
      "Epoch 165/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.33e+00\n",
      "      Variable 1 norm: 1.64e+01\n",
      "      Variable 2 norm: 1.29e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 97ms/step - loss: 0.7880 - mae: 0.6615\n",
      "Epoch 166/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 83ms/step - loss: 0.7879 - mae: 0.6601\n",
      "Epoch 167/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 93ms/step - loss: 0.7877 - mae: 0.6588\n",
      "Epoch 168/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.33e+00\n",
      "      Variable 1 norm: 1.64e+01\n",
      "      Variable 2 norm: 1.29e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 96ms/step - loss: 0.7871 - mae: 0.6576\n",
      "Epoch 169/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 93ms/step - loss: 0.7868 - mae: 0.6583\n",
      "Epoch 170/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 91ms/step - loss: 0.7865 - mae: 0.6589\n",
      "Epoch 171/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.33e+00\n",
      "      Variable 1 norm: 1.64e+01\n",
      "      Variable 2 norm: 1.29e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 97ms/step - loss: 0.7856 - mae: 0.6589\n",
      "Epoch 172/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 85ms/step - loss: 0.7850 - mae: 0.6591\n",
      "Epoch 173/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 86ms/step - loss: 0.7852 - mae: 0.6584\n",
      "Epoch 174/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.33e+00\n",
      "      Variable 1 norm: 1.64e+01\n",
      "      Variable 2 norm: 1.29e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 96ms/step - loss: 0.7855 - mae: 0.6563\n",
      "Epoch 175/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 87ms/step - loss: 0.7857 - mae: 0.6530\n",
      "Epoch 176/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 86ms/step - loss: 0.7863 - mae: 0.6502\n",
      "Epoch 177/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.34e+00\n",
      "      Variable 1 norm: 1.64e+01\n",
      "      Variable 2 norm: 1.29e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 96ms/step - loss: 0.7868 - mae: 0.6494\n",
      "Epoch 178/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 88ms/step - loss: 0.7867 - mae: 0.6509\n",
      "Epoch 179/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 87ms/step - loss: 0.7863 - mae: 0.6549\n",
      "Epoch 180/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.34e+00\n",
      "      Variable 1 norm: 1.64e+01\n",
      "      Variable 2 norm: 1.29e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 94ms/step - loss: 0.7863 - mae: 0.6598\n",
      "Epoch 181/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 91ms/step - loss: 0.7865 - mae: 0.6641\n",
      "Epoch 182/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 87ms/step - loss: 0.7867 - mae: 0.6661\n",
      "Epoch 183/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.34e+00\n",
      "      Variable 1 norm: 1.64e+01\n",
      "      Variable 2 norm: 1.29e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 93ms/step - loss: 0.7867 - mae: 0.6657\n",
      "Epoch 184/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 89ms/step - loss: 0.7869 - mae: 0.6636\n",
      "Epoch 185/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 84ms/step - loss: 0.7871 - mae: 0.6610\n",
      "Epoch 186/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.34e+00\n",
      "      Variable 1 norm: 1.64e+01\n",
      "      Variable 2 norm: 1.29e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 95ms/step - loss: 0.7873 - mae: 0.6588\n",
      "Epoch 187/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 87ms/step - loss: 0.7875 - mae: 0.6577\n",
      "Epoch 188/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 85ms/step - loss: 0.7879 - mae: 0.6574\n",
      "Epoch 189/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.34e+00\n",
      "      Variable 1 norm: 1.64e+01\n",
      "      Variable 2 norm: 1.29e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 99ms/step - loss: 0.7882 - mae: 0.6573\n",
      "Epoch 190/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 83ms/step - loss: 0.7885 - mae: 0.6580\n",
      "Epoch 191/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 98ms/step - loss: 0.7887 - mae: 0.6596\n",
      "Epoch 192/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.34e+00\n",
      "      Variable 1 norm: 1.64e+01\n",
      "      Variable 2 norm: 1.29e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 93ms/step - loss: 0.7890 - mae: 0.6617\n",
      "Epoch 193/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 91ms/step - loss: 0.7894 - mae: 0.6638\n",
      "Epoch 194/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 86ms/step - loss: 0.7899 - mae: 0.6658\n",
      "Epoch 195/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.34e+00\n",
      "      Variable 1 norm: 1.64e+01\n",
      "      Variable 2 norm: 1.29e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 98ms/step - loss: 0.7906 - mae: 0.6672\n",
      "Epoch 196/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 87ms/step - loss: 0.7912 - mae: 0.6679\n",
      "Epoch 197/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 85ms/step - loss: 0.7917 - mae: 0.6686\n",
      "Epoch 198/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.34e+00\n",
      "      Variable 1 norm: 1.64e+01\n",
      "      Variable 2 norm: 1.29e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 96ms/step - loss: 0.7920 - mae: 0.6695\n",
      "Epoch 199/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 92ms/step - loss: 0.7923 - mae: 0.6711\n",
      "Epoch 200/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 88ms/step - loss: 0.7925 - mae: 0.6726\n",
      "Epoch 201/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.34e+00\n",
      "      Variable 1 norm: 1.64e+01\n",
      "      Variable 2 norm: 1.29e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 93ms/step - loss: 0.7928 - mae: 0.6733\n",
      "Epoch 202/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 86ms/step - loss: 0.7928 - mae: 0.6725\n",
      "Epoch 203/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 87ms/step - loss: 0.7926 - mae: 0.6705\n",
      "Epoch 204/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.35e+00\n",
      "      Variable 1 norm: 1.64e+01\n",
      "      Variable 2 norm: 1.29e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 95ms/step - loss: 0.7923 - mae: 0.6680\n",
      "Epoch 205/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 88ms/step - loss: 0.7919 - mae: 0.6662\n",
      "Epoch 206/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 84ms/step - loss: 0.7916 - mae: 0.6655\n",
      "Epoch 207/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.35e+00\n",
      "      Variable 1 norm: 1.64e+01\n",
      "      Variable 2 norm: 1.29e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 94ms/step - loss: 0.7915 - mae: 0.6664\n",
      "Epoch 208/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 86ms/step - loss: 0.7917 - mae: 0.6696\n",
      "Epoch 209/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 87ms/step - loss: 0.7921 - mae: 0.6749\n",
      "Epoch 210/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.35e+00\n",
      "      Variable 1 norm: 1.64e+01\n",
      "      Variable 2 norm: 1.29e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 94ms/step - loss: 0.7934 - mae: 0.6815\n",
      "Epoch 211/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 91ms/step - loss: 0.7950 - mae: 0.6873\n",
      "Epoch 212/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 87ms/step - loss: 0.7962 - mae: 0.6904\n",
      "Epoch 213/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.35e+00\n",
      "      Variable 1 norm: 1.64e+01\n",
      "      Variable 2 norm: 1.29e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 97ms/step - loss: 0.7959 - mae: 0.6893\n",
      "Epoch 214/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 93ms/step - loss: 0.7944 - mae: 0.6837\n",
      "Epoch 215/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 162ms/step - loss: 0.7932 - mae: 0.6754\n",
      "Epoch 216/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.35e+00\n",
      "      Variable 1 norm: 1.65e+01\n",
      "      Variable 2 norm: 1.29e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 186ms/step - loss: 0.7927 - mae: 0.6665\n",
      "Epoch 217/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 382ms/step - loss: 0.7928 - mae: 0.6588\n",
      "Epoch 218/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 211ms/step - loss: 0.7930 - mae: 0.6541\n",
      "Epoch 219/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.36e+00\n",
      "      Variable 1 norm: 1.65e+01\n",
      "      Variable 2 norm: 1.29e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 103ms/step - loss: 0.7929 - mae: 0.6531\n",
      "Epoch 220/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 89ms/step - loss: 0.7924 - mae: 0.6558\n",
      "Epoch 221/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 85ms/step - loss: 0.7914 - mae: 0.6613\n",
      "Epoch 222/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.36e+00\n",
      "      Variable 1 norm: 1.65e+01\n",
      "      Variable 2 norm: 1.29e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 95ms/step - loss: 0.7909 - mae: 0.6677\n",
      "Epoch 223/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 84ms/step - loss: 0.7908 - mae: 0.6725\n",
      "Epoch 224/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 87ms/step - loss: 0.7907 - mae: 0.6739\n",
      "Epoch 225/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.36e+00\n",
      "      Variable 1 norm: 1.65e+01\n",
      "      Variable 2 norm: 1.29e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 93ms/step - loss: 0.7902 - mae: 0.6716\n",
      "Epoch 226/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 85ms/step - loss: 0.7895 - mae: 0.6660\n",
      "Epoch 227/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 87ms/step - loss: 0.7891 - mae: 0.6594\n",
      "Epoch 228/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.37e+00\n",
      "      Variable 1 norm: 1.65e+01\n",
      "      Variable 2 norm: 1.29e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 97ms/step - loss: 0.7893 - mae: 0.6535\n",
      "Epoch 229/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 89ms/step - loss: 0.7890 - mae: 0.6507\n",
      "Epoch 230/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 84ms/step - loss: 0.7880 - mae: 0.6518\n",
      "Epoch 231/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.37e+00\n",
      "      Variable 1 norm: 1.65e+01\n",
      "      Variable 2 norm: 1.29e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 93ms/step - loss: 0.7874 - mae: 0.6558\n",
      "Epoch 232/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 84ms/step - loss: 0.7873 - mae: 0.6616\n",
      "Epoch 233/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 83ms/step - loss: 0.7875 - mae: 0.6674\n",
      "Epoch 234/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.37e+00\n",
      "      Variable 1 norm: 1.65e+01\n",
      "      Variable 2 norm: 1.29e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 98ms/step - loss: 0.7878 - mae: 0.6708\n",
      "Epoch 235/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 90ms/step - loss: 0.7881 - mae: 0.6717\n",
      "Epoch 236/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 92ms/step - loss: 0.7884 - mae: 0.6704\n",
      "Epoch 237/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.37e+00\n",
      "      Variable 1 norm: 1.65e+01\n",
      "      Variable 2 norm: 1.29e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 165ms/step - loss: 0.7881 - mae: 0.6668\n",
      "Epoch 238/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 87ms/step - loss: 0.7880 - mae: 0.6631\n",
      "Epoch 239/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 89ms/step - loss: 0.7881 - mae: 0.6605\n",
      "Epoch 240/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.38e+00\n",
      "      Variable 1 norm: 1.65e+01\n",
      "      Variable 2 norm: 1.29e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 92ms/step - loss: 0.7879 - mae: 0.6593\n",
      "Epoch 241/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 85ms/step - loss: 0.7876 - mae: 0.6603\n",
      "Epoch 242/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 89ms/step - loss: 0.7873 - mae: 0.6628\n",
      "Epoch 243/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.38e+00\n",
      "      Variable 1 norm: 1.65e+01\n",
      "      Variable 2 norm: 1.29e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 92ms/step - loss: 0.7873 - mae: 0.6662\n",
      "Epoch 244/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 86ms/step - loss: 0.7872 - mae: 0.6686\n",
      "Epoch 245/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 86ms/step - loss: 0.7872 - mae: 0.6694\n",
      "Epoch 246/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.39e+00\n",
      "      Variable 1 norm: 1.65e+01\n",
      "      Variable 2 norm: 1.29e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 98ms/step - loss: 0.7871 - mae: 0.6678\n",
      "Epoch 247/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 88ms/step - loss: 0.7872 - mae: 0.6640\n",
      "Epoch 248/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 86ms/step - loss: 0.7870 - mae: 0.6592\n",
      "Epoch 249/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.39e+00\n",
      "      Variable 1 norm: 1.65e+01\n",
      "      Variable 2 norm: 1.29e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 99ms/step - loss: 0.7871 - mae: 0.6556\n",
      "Epoch 250/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 86ms/step - loss: 0.7869 - mae: 0.6548\n",
      "Epoch 251/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 86ms/step - loss: 0.7871 - mae: 0.6580\n",
      "Epoch 252/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.39e+00\n",
      "      Variable 1 norm: 1.65e+01\n",
      "      Variable 2 norm: 1.29e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 99ms/step - loss: 0.7872 - mae: 0.6638\n",
      "Epoch 253/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 86ms/step - loss: 0.7879 - mae: 0.6696\n",
      "Epoch 254/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 87ms/step - loss: 0.7882 - mae: 0.6727\n",
      "Epoch 255/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.39e+00\n",
      "      Variable 1 norm: 1.65e+01\n",
      "      Variable 2 norm: 1.29e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 97ms/step - loss: 0.7884 - mae: 0.6728\n",
      "Epoch 256/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 87ms/step - loss: 0.7883 - mae: 0.6699\n",
      "Epoch 257/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 92ms/step - loss: 0.7876 - mae: 0.6642\n",
      "Epoch 258/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.40e+00\n",
      "      Variable 1 norm: 1.66e+01\n",
      "      Variable 2 norm: 1.29e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 94ms/step - loss: 0.7878 - mae: 0.6574\n",
      "Epoch 259/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 85ms/step - loss: 0.7886 - mae: 0.6531\n",
      "Epoch 260/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 87ms/step - loss: 0.7898 - mae: 0.6521\n",
      "Epoch 261/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.40e+00\n",
      "      Variable 1 norm: 1.66e+01\n",
      "      Variable 2 norm: 1.29e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 102ms/step - loss: 0.7899 - mae: 0.6522\n",
      "Epoch 262/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 87ms/step - loss: 0.7892 - mae: 0.6546\n",
      "Epoch 263/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 89ms/step - loss: 0.7887 - mae: 0.6595\n",
      "Epoch 264/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.40e+00\n",
      "      Variable 1 norm: 1.66e+01\n",
      "      Variable 2 norm: 1.29e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 95ms/step - loss: 0.7895 - mae: 0.6664\n",
      "Epoch 265/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 85ms/step - loss: 0.7906 - mae: 0.6711\n",
      "Epoch 266/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 86ms/step - loss: 0.7926 - mae: 0.6742\n",
      "Epoch 267/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.40e+00\n",
      "      Variable 1 norm: 1.66e+01\n",
      "      Variable 2 norm: 1.29e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 92ms/step - loss: 0.7961 - mae: 0.6756\n",
      "Epoch 268/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 86ms/step - loss: 0.8009 - mae: 0.6742\n",
      "Epoch 269/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 86ms/step - loss: 0.8073 - mae: 0.6693\n",
      "Epoch 270/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.41e+00\n",
      "      Variable 1 norm: 1.66e+01\n",
      "      Variable 2 norm: 1.29e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 94ms/step - loss: 0.8162 - mae: 0.6633\n",
      "Epoch 271/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 88ms/step - loss: 0.8268 - mae: 0.6580\n",
      "Epoch 272/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 86ms/step - loss: 0.8387 - mae: 0.6534\n",
      "Epoch 273/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.41e+00\n",
      "      Variable 1 norm: 1.66e+01\n",
      "      Variable 2 norm: 1.29e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 97ms/step - loss: 0.8500 - mae: 0.6497\n",
      "Epoch 274/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 89ms/step - loss: 0.8588 - mae: 0.6462\n",
      "Epoch 275/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 92ms/step - loss: 0.8640 - mae: 0.6414\n",
      "Epoch 276/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.41e+00\n",
      "      Variable 1 norm: 1.66e+01\n",
      "      Variable 2 norm: 1.29e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 92ms/step - loss: 0.8664 - mae: 0.6376\n",
      "Epoch 277/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 81ms/step - loss: 0.8676 - mae: 0.6353\n",
      "Epoch 278/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 84ms/step - loss: 0.8679 - mae: 0.6353\n",
      "Epoch 279/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.42e+00\n",
      "      Variable 1 norm: 1.66e+01\n",
      "      Variable 2 norm: 1.29e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 91ms/step - loss: 0.8672 - mae: 0.6371\n",
      "Epoch 280/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 90ms/step - loss: 0.8664 - mae: 0.6406\n",
      "Epoch 281/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 89ms/step - loss: 0.8662 - mae: 0.6461\n",
      "Epoch 282/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.42e+00\n",
      "      Variable 1 norm: 1.66e+01\n",
      "      Variable 2 norm: 1.29e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 96ms/step - loss: 0.8665 - mae: 0.6529\n",
      "Epoch 283/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 87ms/step - loss: 0.8669 - mae: 0.6597\n",
      "Epoch 284/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 97ms/step - loss: 0.8663 - mae: 0.6652\n",
      "Epoch 285/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.42e+00\n",
      "      Variable 1 norm: 1.66e+01\n",
      "      Variable 2 norm: 1.29e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 93ms/step - loss: 0.8637 - mae: 0.6676\n",
      "Epoch 286/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 89ms/step - loss: 0.8590 - mae: 0.6667\n",
      "Epoch 287/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 90ms/step - loss: 0.8527 - mae: 0.6631\n",
      "Epoch 288/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.42e+00\n",
      "      Variable 1 norm: 1.66e+01\n",
      "      Variable 2 norm: 1.29e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 95ms/step - loss: 0.8458 - mae: 0.6580\n",
      "Epoch 289/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 84ms/step - loss: 0.8394 - mae: 0.6522\n",
      "Epoch 290/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 84ms/step - loss: 0.8341 - mae: 0.6487\n",
      "Epoch 291/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.43e+00\n",
      "      Variable 1 norm: 1.66e+01\n",
      "      Variable 2 norm: 1.29e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 94ms/step - loss: 0.8294 - mae: 0.6486\n",
      "Epoch 292/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 91ms/step - loss: 0.8249 - mae: 0.6539\n",
      "Epoch 293/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 86ms/step - loss: 0.8211 - mae: 0.6629\n",
      "Epoch 294/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.43e+00\n",
      "      Variable 1 norm: 1.66e+01\n",
      "      Variable 2 norm: 1.29e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 98ms/step - loss: 0.8186 - mae: 0.6739\n",
      "Epoch 295/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 83ms/step - loss: 0.8171 - mae: 0.6839\n",
      "Epoch 296/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 84ms/step - loss: 0.8159 - mae: 0.6903\n",
      "Epoch 297/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.43e+00\n",
      "      Variable 1 norm: 1.66e+01\n",
      "      Variable 2 norm: 1.29e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 94ms/step - loss: 0.8142 - mae: 0.6925\n",
      "Epoch 298/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 88ms/step - loss: 0.8125 - mae: 0.6912\n",
      "Epoch 299/300\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 88ms/step - loss: 0.8111 - mae: 0.6885\n",
      "Epoch 300/300\n",
      "   📋 Optimizer type: Adam\n",
      "   📈 Number of trainable variables: 18\n",
      "   ❌ Optimizer doesn't have get_gradients, using variable norms\n",
      "      Variable 0 norm: 9.44e+00\n",
      "      Variable 1 norm: 1.67e+01\n",
      "      Variable 2 norm: 1.29e+01\n",
      "   🔬 Checking 18 variable norms...\n",
      "   ✅ All variable norms are within acceptable range\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 95ms/step - loss: 0.8099 - mae: 0.6861\n",
      "🏁 GradientMonitoringCallback: Training completed!\n",
      "   📊 Final stats - Total calls: 300, Gradient checks: 100, Fallbacks: 0\n",
      "   ✅ Gradient monitoring completed successfully!\n",
      "X_mean:None, X_std:None, y_mean:None, y_std:None\n"
     ]
    }
   ],
   "source": [
    "# overfit small\n",
    "model, X_mean, X_std, y_mean, y_std = train_model(train_data[:3,...],epochs1=300, epochs2=300, validation_split=0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-25T20:33:47.345288Z",
     "start_time": "2025-05-25T20:32:28.208977Z"
    }
   },
   "id": "71d257d5cc4ad709",
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred deltas\n",
      "[[[ 0.6200853  -1.4685496 ]\n",
      "  [ 0.6323009  -1.6576537 ]\n",
      "  [ 0.5777124  -1.6909549 ]\n",
      "  [ 0.64099866 -1.8064063 ]\n",
      "  [ 0.82729065 -1.7919458 ]]]\n",
      "pred deltas\n",
      "[[[ 0.62008464 -1.4685491 ]\n",
      "  [ 0.6322974  -1.6576431 ]\n",
      "  [ 0.57770175 -1.690933  ]\n",
      "  [ 0.6409859  -1.8063818 ]\n",
      "  [ 0.8272821  -1.7919288 ]]]\n",
      "pred deltas\n",
      "[[[ 0.62008655 -1.46855   ]\n",
      "  [ 0.632305   -1.6576673 ]\n",
      "  [ 0.5777259  -1.6909825 ]\n",
      "  [ 0.64101493 -1.8064376 ]\n",
      "  [ 0.827302   -1.7919668 ]]]\n",
      "pred deltas\n",
      "[[[ 0.6200856  -1.4685488 ]\n",
      "  [ 0.63230103 -1.6576543 ]\n",
      "  [ 0.5777122  -1.6909554 ]\n",
      "  [ 0.6409991  -1.8064065 ]\n",
      "  [ 0.82729053 -1.7919452 ]]]\n",
      "pred deltas\n",
      "[[[ 0.62008476 -1.468549  ]\n",
      "  [ 0.63229895 -1.6576478 ]\n",
      "  [ 0.5777066  -1.6909428 ]\n",
      "  [ 0.6409916  -1.8063927 ]\n",
      "  [ 0.82728565 -1.7919362 ]]]\n",
      "pred deltas\n",
      "[[[ 0.91566324 -1.9718213 ]\n",
      "  [ 0.78065526 -1.9445078 ]\n",
      "  [ 0.5907213  -1.898335  ]\n",
      "  [ 0.6161543  -1.6380386 ]\n",
      "  [ 0.56206137 -1.2853904 ]]]\n",
      "pred deltas\n",
      "[[[ 0.8650169  -1.8976893 ]\n",
      "  [ 0.68931925 -1.851831  ]\n",
      "  [ 0.5557005  -1.762901  ]\n",
      "  [ 0.53604174 -1.4387001 ]\n",
      "  [ 0.41924828 -1.0471971 ]]]\n",
      "pred deltas\n",
      "[[[ 0.6732021  -1.4748757 ]\n",
      "  [ 0.34694368 -1.155703  ]\n",
      "  [ 0.14370145 -0.90848356]\n",
      "  [ 0.11815567 -0.533158  ]\n",
      "  [ 0.1123042  -0.2331634 ]]]\n",
      "pred deltas\n",
      "[[[ 0.6200627 -1.4680324]\n",
      "  [ 0.632405  -1.6567202]\n",
      "  [ 0.5759225 -1.6911054]\n",
      "  [ 0.6477403 -1.8162241]\n",
      "  [ 0.8389109 -1.815144 ]]]\n",
      "pred deltas\n",
      "[[[ 0.29637617 -0.82083845]\n",
      "  [ 0.08101125 -0.5895137 ]\n",
      "  [-0.09745353 -0.2598946 ]\n",
      "  [-0.14002858  0.0917284 ]\n",
      "  [-0.14102344  0.26298267]]]\n",
      "pred deltas\n",
      "[[[ 0.61556137 -1.4482327 ]\n",
      "  [ 0.6237068  -1.6235718 ]\n",
      "  [ 0.5515717  -1.6278344 ]\n",
      "  [ 0.60825646 -1.7199516 ]\n",
      "  [ 0.81335115 -1.7376913 ]]]\n",
      "pred deltas\n",
      "[[[ 0.5624123  -1.2927456 ]\n",
      "  [ 0.26221693 -0.9981413 ]\n",
      "  [ 0.08137453 -0.7371777 ]\n",
      "  [ 0.04178505 -0.33698434]\n",
      "  [ 0.02471285 -0.04913585]]]\n",
      "pred deltas\n",
      "[[[ 0.612388   -1.4445769 ]\n",
      "  [ 0.6209633  -1.6181312 ]\n",
      "  [ 0.5454043  -1.6133565 ]\n",
      "  [ 0.5949292  -1.6953213 ]\n",
      "  [ 0.80251324 -1.7156062 ]]]\n",
      "pred deltas\n",
      "[[[ 0.60957533 -1.4406409 ]\n",
      "  [ 0.61828005 -1.6122656 ]\n",
      "  [ 0.5376559  -1.595929  ]\n",
      "  [ 0.578415   -1.6648469 ]\n",
      "  [ 0.7854128  -1.6822627 ]]]\n",
      "pred deltas\n",
      "[[[ 0.6045744 -1.4352069]\n",
      "  [ 0.6138269 -1.6036376]\n",
      "  [ 0.5247958 -1.566767 ]\n",
      "  [ 0.5516332 -1.6169325]\n",
      "  [ 0.7455888 -1.6131275]]]\n",
      "pred deltas\n",
      "[[[ 0.60136366 -1.4298828 ]\n",
      "  [ 0.61039674 -1.5984675 ]\n",
      "  [ 0.51539016 -1.5517247 ]\n",
      "  [ 0.5460762  -1.5985605 ]\n",
      "  [ 0.7407533  -1.587655  ]]]\n",
      "pred deltas\n",
      "[[[ 0.59268326 -1.4143836 ]\n",
      "  [ 0.6036844  -1.5855261 ]\n",
      "  [ 0.5042709  -1.5237185 ]\n",
      "  [ 0.5486061  -1.5795188 ]\n",
      "  [ 0.7492503  -1.574105  ]]]\n",
      "pred deltas\n",
      "[[[ 0.59268    -1.4143761 ]\n",
      "  [ 0.6036799  -1.5855179 ]\n",
      "  [ 0.50425524 -1.5236957 ]\n",
      "  [ 0.54858124 -1.5794737 ]\n",
      "  [ 0.7492387  -1.5740803 ]]]\n",
      "pred deltas\n",
      "[[[ 1.0725788 -2.1823568]\n",
      "  [ 1.0256709 -2.2741008]\n",
      "  [ 0.8389723 -2.1465743]\n",
      "  [ 0.8467482 -1.9949327]\n",
      "  [ 0.8734373 -1.7357664]]]\n",
      "pred deltas\n",
      "[[[ 0.5881731  -1.4032834 ]\n",
      "  [ 0.6045077  -1.5837908 ]\n",
      "  [ 0.51353717 -1.5259657 ]\n",
      "  [ 0.57687044 -1.6088712 ]\n",
      "  [ 0.7863562  -1.6203978 ]]]\n",
      "pred deltas\n",
      "[[[ 0.58817387 -1.4032848 ]\n",
      "  [ 0.6045066  -1.5837882 ]\n",
      "  [ 0.5135324  -1.5259588 ]\n",
      "  [ 0.5768564  -1.6088539 ]\n",
      "  [ 0.78633577 -1.6203718 ]]]\n",
      "pred deltas\n",
      "[[[ 0.83206314 -1.8259654 ]\n",
      "  [ 0.8150417  -1.9508216 ]\n",
      "  [ 0.7204338  -1.8412945 ]\n",
      "  [ 0.78793216 -1.9371601 ]\n",
      "  [ 0.90243214 -1.7961485 ]]]\n",
      "pred deltas\n",
      "[[[ 0.5863584  -1.4012798 ]\n",
      "  [ 0.6091409  -1.5925319 ]\n",
      "  [ 0.52627724 -1.5423436 ]\n",
      "  [ 0.6007618  -1.6379826 ]\n",
      "  [ 0.8119375  -1.6519727 ]]]\n",
      "pred deltas\n",
      "[[[ 0.81053704 -1.7323866 ]\n",
      "  [ 0.563081   -1.4979457 ]\n",
      "  [ 0.28944153 -1.2347353 ]\n",
      "  [ 0.24112907 -0.8809337 ]\n",
      "  [ 0.23771133 -0.56066746]]]\n",
      "pred deltas\n",
      "[[[ 0.6218876  -1.4610896 ]\n",
      "  [ 0.68060076 -1.7151728 ]\n",
      "  [ 0.6088135  -1.6789036 ]\n",
      "  [ 0.6741867  -1.7554986 ]\n",
      "  [ 0.86047226 -1.7207669 ]]]\n",
      "pred deltas\n",
      "[[[ 0.5861643  -1.4002467 ]\n",
      "  [ 0.61391073 -1.5998094 ]\n",
      "  [ 0.53588676 -1.5558082 ]\n",
      "  [ 0.6163689  -1.6583652 ]\n",
      "  [ 0.82550156 -1.6698864 ]]]\n",
      "pred deltas\n",
      "[[[ 0.83141255 -1.8235801 ]\n",
      "  [ 0.8165121  -1.9516441 ]\n",
      "  [ 0.73041415 -1.8550632 ]\n",
      "  [ 0.7946006  -1.9452791 ]\n",
      "  [ 0.9008783  -1.7949903 ]]]\n",
      "pred deltas\n",
      "[[[ 0.5859099  -1.3975492 ]\n",
      "  [ 0.62036    -1.6089182 ]\n",
      "  [ 0.54656625 -1.5700972 ]\n",
      "  [ 0.63665026 -1.6792488 ]\n",
      "  [ 0.83894193 -1.6813141 ]]]\n",
      "pred deltas\n",
      "[[[ 0.58591294 -1.3975654 ]\n",
      "  [ 0.62034106 -1.608898  ]\n",
      "  [ 0.5465383  -1.5700705 ]\n",
      "  [ 0.6365617  -1.6791514 ]\n",
      "  [ 0.8388921  -1.681261  ]]]\n",
      "pred deltas\n",
      "[[[ 0.8830082 -1.8824117]\n",
      "  [ 0.8826507 -2.0485542]\n",
      "  [ 0.7145862 -1.8894374]\n",
      "  [ 0.7228769 -1.777077 ]\n",
      "  [ 0.7274608 -1.4634421]]]\n",
      "pred deltas\n",
      "[[[ 0.8105426  -1.7324001 ]\n",
      "  [ 0.5630904  -1.4979682 ]\n",
      "  [ 0.28944445 -1.2347596 ]\n",
      "  [ 0.24113281 -0.88096184]\n",
      "  [ 0.23772202 -0.5607049 ]]]\n",
      "[[3169.68478022 1681.01695721]\n",
      " [3170.31708113 1679.35930352]\n",
      " [3170.89479355 1677.6683486 ]\n",
      " [3171.53579215 1675.8619421 ]\n",
      " [3172.36308292 1674.06999664]]\n",
      "[[3169.59927323 1681.91109158]\n",
      " [3170.12526704 1681.34556357]\n",
      " [3170.64292666 1680.78882564]\n",
      " [3171.15207496 1680.24129589]\n",
      " [3171.65193589 1679.70370715]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/j6ybhf8906j4bmnp171wtn680000gn/T/ipykernel_22241/3735350324.py:9: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "  cmap1 = plt.cm.get_cmap('viridis', 50)\n",
      "/var/folders/wv/j6ybhf8906j4bmnp171wtn680000gn/T/ipykernel_22241/3735350324.py:10: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "  cmap2 = plt.cm.get_cmap('plasma', 50)\n"
     ]
    }
   ],
   "source": [
    "# visualize regular prediction\n",
    "\n",
    "# model = load_model()\n",
    "\n",
    "# Parameters\n",
    "Tobs = 50\n",
    "Tpred = 60\n",
    "\n",
    "data = train_data[1]\n",
    "\n",
    "# Select a test scenario (can use any valid index)\n",
    "test_scenario = data.copy()  # shape (agents, time_steps, features)\n",
    "\n",
    "\n",
    "# Forecast future positions\n",
    "predicted_positions = forecast_positions(test_scenario, Tobs, Tpred, model, X_mean, X_std, y_mean, y_std)\n",
    "\n",
    "# Create combined matrix of past observed + predicted for ego agent (agent 0)\n",
    "ego_past = test_scenario[0, :Tobs, :2]               # shape (Tobs, 2)\n",
    "ego_future = predicted_positions[0]                  # shape (Tpred, 2)\n",
    "\n",
    "print(ego_future[:5])\n",
    "print(test_scenario[0, Tobs:Tobs+5, :2])\n",
    "ego_full = np.concatenate([ego_past, ego_future], axis=0)  # shape (Tobs + Tpred, 2)\n",
    "\n",
    "# Create updated scenario with predicted ego and original others\n",
    "updated_scenario = test_scenario.copy()\n",
    "updated_scenario[0, :Tobs+Tpred, :2] = ego_full  # Replace ego trajectory\n",
    "\n",
    "# Visualize\n",
    "make_gif(updated_scenario, data, name='lstm2')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-25T20:36:58.699272Z",
     "start_time": "2025-05-25T20:36:50.586860Z"
    }
   },
   "id": "86ac55f8c53f2860",
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model, X_mean, X_std, y_mean, y_std = train_model(train_data,epochs1=2, epochs2=0)\n",
    "\n",
    "# Save the model \n",
    "save_model(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-25T20:10:02.335523Z",
     "start_time": "2025-05-25T20:10:02.328104Z"
    }
   },
   "id": "345640e8afcba2fb",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# visualize regular prediction\n",
    "\n",
    "# model = load_model()\n",
    "\n",
    "# Parameters\n",
    "Tobs = 50\n",
    "Tpred = 60\n",
    "\n",
    "data = train_data[0]\n",
    "\n",
    "# Select a test scenario (can use any valid index)\n",
    "test_scenario = data.copy()  # shape (agents, time_steps, features)\n",
    "\n",
    "\n",
    "# Forecast future positions\n",
    "predicted_positions = forecast_positions(test_scenario, Tobs, Tpred, model, X_mean, X_std, y_mean, y_std)\n",
    "\n",
    "# Create combined matrix of past observed + predicted for ego agent (agent 0)\n",
    "ego_past = test_scenario[0, :Tobs, :2]               # shape (Tobs, 2)\n",
    "ego_future = predicted_positions[0]                  # shape (Tpred, 2)\n",
    "\n",
    "print(ego_future[:5])\n",
    "ego_full = np.concatenate([ego_past, ego_future], axis=0)  # shape (Tobs + Tpred, 2)\n",
    "\n",
    "# Create updated scenario with predicted ego and original others\n",
    "updated_scenario = test_scenario.copy()\n",
    "updated_scenario[0, :Tobs+Tpred, :2] = ego_full  # Replace ego trajectory\n",
    "\n",
    "# Visualize\n",
    "make_gif(updated_scenario, data, name='lstm2')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-05-25T20:10:02.328516Z"
    }
   },
   "id": "1e102a35fb1e91d7",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# visualize prediction\n",
    "\n",
    "# model = load_model()\n",
    "\n",
    "# Parameters\n",
    "Tobs = 50\n",
    "Tpred = 60\n",
    "\n",
    "data = train_data[0]\n",
    "\n",
    "# Select a test scenario (can use any valid index)\n",
    "test_scenario = data.copy()  # shape (agents, time_steps, features)\n",
    "\n",
    "\n",
    "# Forecast future positions\n",
    "predicted_positions = finetune_forecast_positions(test_scenario, Tobs, Tpred, model, X_mean, X_std, y_mean, y_std)\n",
    "\n",
    "# Create combined matrix of past observed + predicted for ego agent (agent 0)\n",
    "ego_past = test_scenario[0, :Tobs, :2]               # shape (Tobs, 2)\n",
    "ego_future = predicted_positions[0]                  # shape (Tpred, 2)\n",
    "ego_full = np.concatenate([ego_past, ego_future], axis=0)  # shape (Tobs + Tpred, 2)\n",
    "\n",
    "# Create updated scenario with predicted ego and original others\n",
    "updated_scenario = test_scenario.copy()\n",
    "updated_scenario[0, :Tobs+Tpred, :2] = ego_full  # Replace ego trajectory\n",
    "\n",
    "# Visualize\n",
    "make_gif(updated_scenario, data, name='lstm2')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-05-25T20:10:02.328610Z"
    }
   },
   "id": "afd67ae1834bd217",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "def evaluate_mse(train_data, model, Tobs=50, Tpred=60):\n",
    "    \"\"\"\n",
    "    Computes LSTM prediction for ego agent and evaluates MSE with progress reporting.\n",
    "    \"\"\"\n",
    "    N = train_data.shape[0]\n",
    "    mse_list = []\n",
    "    valid_scenarios = 0\n",
    "    \n",
    "    print(f\"Evaluating {N} scenarios...\")\n",
    "    \n",
    "    # Progress reporting variables\n",
    "    report_interval = max(1, N // 10)  # Report at 10% intervals\n",
    "    \n",
    "    for i in range(N):\n",
    "        # Progress reporting\n",
    "        if i % report_interval == 0 or i == N-1:\n",
    "            print(f\"Processing scenario {i+1}/{N} ({(i+1)/N*100:.1f}%)\")\n",
    "        \n",
    "        scenario_data = train_data[i]\n",
    "        ego_agent_data = scenario_data[0]\n",
    "        ground_truth = ego_agent_data[Tobs:Tobs+Tpred, :2]\n",
    "        \n",
    "        # Skip if ground truth contains all zeros (padded)\n",
    "        if np.all(ground_truth == 0):\n",
    "            continue\n",
    "            \n",
    "        valid_scenarios += 1\n",
    "        \n",
    "        # Forecast future positions\n",
    "        predicted_positions = forecast_positions(\n",
    "            ego_agent_data[np.newaxis, :, :],\n",
    "            Tobs, Tpred, model, X_mean, X_std, y_mean, y_std\n",
    "        )\n",
    "        \n",
    "        # Compute MSE\n",
    "        mse = mean_squared_error(ground_truth, predicted_positions[0])\n",
    "        mse_list.append(mse)\n",
    "        \n",
    "        # Occasional MSE reporting\n",
    "        if i % report_interval == 0:\n",
    "            print(f\"  Current scenario MSE: {mse:.4f}\")\n",
    "    \n",
    "    # Final results\n",
    "    if mse_list:\n",
    "        overall_mse = np.mean(mse_list)\n",
    "        print(f\"Evaluation complete: {valid_scenarios} valid scenarios\")\n",
    "        print(f\"Mean Squared Error (MSE): {overall_mse:.4f}\")\n",
    "        print(f\"Min MSE: {np.min(mse_list):.4f}, Max MSE: {np.max(mse_list):.4f}\")\n",
    "        return overall_mse\n",
    "    else:\n",
    "        print(\"No valid scenarios for evaluation.\")\n",
    "        return None"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-05-25T20:10:02.328661Z"
    }
   },
   "id": "11f6401b403c51b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Evaluate on training data\n",
    "evaluate_mse(train_data, model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-05-25T20:10:02.328695Z"
    }
   },
   "id": "c75caee9a06e8acb",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def generate_submission(data, output_csv, Tobs=50, Tpred=60):\n",
    "    \"\"\"\n",
    "    Applies forecasting and generates a submission CSV with format:\n",
    "    index,x,y where index is auto-generated and matches submission key.\n",
    "    \n",
    "    Args:\n",
    "        data (np.ndarray): Test data of shape (num_scenarios, 50, 50, 6).\n",
    "        output_csv (str): Output CSV file path.\n",
    "        Tobs (int): Observed time steps (default 50).\n",
    "        Tpred (int): Prediction time steps (default 60).\n",
    "    \"\"\"\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    for i in range(data.shape[0]):\n",
    "        scenario_data = data[i]            # Shape: (50, 50, 6)\n",
    "        ego_agent_data = scenario_data[0]  # Shape: (50, 6)\n",
    "\n",
    "        # Predict future positions for the ego agent\n",
    "        predicted_positions = finetune_forecast_positions(\n",
    "            ego_agent_data[np.newaxis, :, :], Tobs, Tpred, model\n",
    "        )  # Shape: (1, 60, 2)\n",
    "\n",
    "        # Append 60 predictions (x, y) for this scenario\n",
    "        predictions.extend(predicted_positions[0])  # Shape: (60, 2)\n",
    "\n",
    "    # Create DataFrame without explicit ID\n",
    "    submission_df = pd.DataFrame(predictions, columns=[\"x\", \"y\"])\n",
    "    submission_df.index.name = 'index'  # Match Kaggle format\n",
    "\n",
    "    # Save CSV with index\n",
    "    submission_df.to_csv(output_csv)\n",
    "    print(f\"Submission file '{output_csv}' saved with shape {submission_df.shape}\")\n",
    "\n",
    "generate_submission(test_data, 'lstm_submission.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-05-25T20:10:02.328728Z"
    }
   },
   "id": "907c41122d6ed7b9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
