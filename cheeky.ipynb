{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-01T08:55:40.750841Z",
     "start_time": "2025-06-01T08:55:40.747212Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def interpolate_predictions(file1, file2, mse1, mse2):\n",
    "    # Load data\n",
    "    df1 = pd.read_csv(file1)\n",
    "    df2 = pd.read_csv(file2)\n",
    "    \n",
    "    assert len(df1) == len(df2), \"Files must have same number of rows\"\n",
    "    assert all(df1.index == df2.index), \"Mismatched indices\"\n",
    "\n",
    "    # Split into scenes\n",
    "    n = len(df1)\n",
    "    scene_length = 60\n",
    "    num_scenes = n // scene_length\n",
    "\n",
    "    all_new_preds = []\n",
    "\n",
    "    for scene in range(num_scenes):\n",
    "        start = scene * scene_length\n",
    "        end = start + scene_length\n",
    "\n",
    "        p1 = df1.iloc[start:end][['x', 'y']].values\n",
    "        p2 = df2.iloc[start:end][['x', 'y']].values\n",
    "\n",
    "        # We assume ground truth lies on the line between p1 and p2\n",
    "        # Try to find optimal alpha in [0,1] that would yield a prediction with better MSE\n",
    "\n",
    "        # Try a grid of alphas\n",
    "        alphas = np.linspace(0, 1, 101)\n",
    "        best_alpha = 0\n",
    "        best_pred = None\n",
    "        best_estimated_mse = float('inf')\n",
    "\n",
    "        for alpha in alphas:\n",
    "            interpolated = (1 - alpha) * p1 + alpha * p2\n",
    "            # We estimate the new MSE assuming the ground truth lies closer to the better prediction\n",
    "            # This is a synthetic error, not actual, since we don't know the truth\n",
    "\n",
    "            # Heuristic: estimated \"true\" point is weighted by inverse of MSE\n",
    "            # Example: lower MSE gets more weight\n",
    "            weight1 = 1 / (mse1 + 1e-6)\n",
    "            weight2 = 1 / (mse2 + 1e-6)\n",
    "            true_est = (weight1 * p1 + weight2 * p2) / (weight1 + weight2)\n",
    "\n",
    "            mse_est = np.mean(np.square(interpolated - true_est).sum(axis=1))\n",
    "\n",
    "            if mse_est < best_estimated_mse:\n",
    "                best_estimated_mse = mse_est\n",
    "                best_alpha = alpha\n",
    "                best_pred = interpolated\n",
    "\n",
    "        # Add result to final predictions\n",
    "        for i in range(scene_length):\n",
    "            all_new_preds.append([start + i, best_pred[i][0], best_pred[i][1]])\n",
    "\n",
    "    # Create DataFrame\n",
    "    df_out = pd.DataFrame(all_new_preds, columns=[\"index\", \"x\", \"y\"])\n",
    "    return df_out\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "new_df = interpolate_predictions(\"social_lstm_2_submission.csv\", \"lstm_submission-4.csv\", mse1=8.92944, mse2=9.40369)\n",
    "new_df.to_csv(\"cheeky5.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T08:55:43.352677Z",
     "start_time": "2025-06-01T08:55:41.007975Z"
    }
   },
   "id": "d2494e2dcc200708",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# new_df = interpolate_predictions(\"social_lstm_2_submission.csv\", \"cheeky5.csv\", mse1=8.92944, mse2=8.43177)\n",
    "# new_df.to_csv(\"cheeky6.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T09:01:04.610913Z",
     "start_time": "2025-06-01T09:01:02.160481Z"
    }
   },
   "id": "f67de1539996e53b",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def reverse_engineer_predictions(file1, file2, mse1, mse2, steps_per_scene=60):\n",
    "    # Load data\n",
    "    df1 = pd.read_csv(file1)\n",
    "    df2 = pd.read_csv(file2)\n",
    "\n",
    "    assert len(df1) == len(df2), \"Files must have same number of rows\"\n",
    "    assert len(df1) % steps_per_scene == 0, \"Data does not divide evenly into scenes\"\n",
    "    \n",
    "    total_scenes = len(df1) // steps_per_scene\n",
    "\n",
    "    # Decide which prediction is better\n",
    "    if mse1 < mse2:\n",
    "        base_df = df1.copy()\n",
    "        other_df = df2.copy()\n",
    "        base_mse = mse1\n",
    "        other_mse = mse2\n",
    "    else:\n",
    "        base_df = df2.copy()\n",
    "        other_df = df1.copy()\n",
    "        base_mse = mse2\n",
    "        other_mse = mse1\n",
    "\n",
    "    # Calculate global delta and scale per scene\n",
    "    delta_mse = abs(base_mse - other_mse)\n",
    "    max_mse = max(base_mse, other_mse)\n",
    "    \n",
    "    # Initialize output arrays\n",
    "    new_x = []\n",
    "    new_y = []\n",
    "\n",
    "    for scene in range(total_scenes):\n",
    "        start = scene * steps_per_scene\n",
    "        end = start + steps_per_scene\n",
    "\n",
    "        # Get chunk of current scene\n",
    "        base_chunk = base_df.iloc[start:end]\n",
    "        other_chunk = other_df.iloc[start:end]\n",
    "\n",
    "        dx = other_chunk['x'].values - base_chunk['x'].values\n",
    "        dy = other_chunk['y'].values - base_chunk['y'].values\n",
    "\n",
    "        # Optional: use norm of mean delta to modulate scale (can remove if too dynamic)\n",
    "        # per_scene_diff_mag = np.mean(np.sqrt(dx**2 + dy**2))\n",
    "        # scale = 0.5 * delta_mse / (per_scene_diff_mag * max_mse + 1e-6)\n",
    "\n",
    "        # Base scale â€” scaled per scene\n",
    "        scale = delta_mse / (total_scenes * max_mse + 1e-6)\n",
    "\n",
    "        # Adjust prediction in opposite direction of error\n",
    "        adjusted_x = base_chunk['x'].values - scale * dx\n",
    "        adjusted_y = base_chunk['y'].values - scale * dy\n",
    "\n",
    "        new_x.extend(adjusted_x)\n",
    "        new_y.extend(adjusted_y)\n",
    "\n",
    "    # Return adjusted DataFrame\n",
    "    new_df = pd.DataFrame({\n",
    "        'index': base_df['index'],\n",
    "        'x': new_x,\n",
    "        'y': new_y\n",
    "    })\n",
    "\n",
    "    return new_df\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T07:23:41.587673Z",
     "start_time": "2025-06-01T07:23:41.586237Z"
    }
   },
   "id": "bfa0b9d46fd8b041",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "better_preds = reverse_engineer_predictions(\"cheeky2.csv\", \"lstm_submission-4.csv\", mse1=9.07044, mse2=9.40369)\n",
    "better_preds.to_csv(\"cheeky4.csv\", index=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T07:30:01.087292Z",
     "start_time": "2025-06-01T07:30:00.582244Z"
    }
   },
   "id": "a11651f05f4b9fd5",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "bf5888acccf5f9b6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
