{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69207995c2b68685",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "use social pooling layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4708aa47ab62fb01",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Input: (batch_size, 50_agents, 50_timesteps, 6_features)\n",
    "\n",
    "1. Per-agent encoding:\n",
    "   - Each agent's 50-step trajectory → LSTM encoder\n",
    "   - Output: (batch_size, 50_agents, hidden_dim)\n",
    "\n",
    "2. Social pooling:\n",
    "   - Attention mechanism across all agents\n",
    "   - Pool information from all agents for each agent\n",
    "   - Output: (batch_size, 50_agents, pooled_dim)\n",
    "\n",
    "3. Decoding (ego-focused):\n",
    "   - Ego's encoded features + social context → LSTM decoder\n",
    "   - Output: (batch_size, 60_timesteps, 6_features) for ego only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12ecbb45022667b6",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T19:23:45.837481Z",
     "start_time": "2025-06-01T19:23:42.331129Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mehul/Documents/UCSD Classes/Deep Learning/AgroverseComp/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T19:23:46.875072Z",
     "start_time": "2025-06-01T19:23:45.836736Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data's shape (10000, 50, 110, 6)\n",
      "test_data's shape (2100, 50, 50, 6)\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "import numpy as np\n",
    "train_file = np.load('data/train.npz')\n",
    "train_data = train_file['data']\n",
    "print(\"train_data's shape\", train_data.shape)\n",
    "test_file = np.load('data/test_input.npz')\n",
    "test_data = test_file['data']\n",
    "print(\"test_data's shape\", test_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List, Optional\n",
    "from joblib import Parallel, delayed\n",
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "def rotate_trajectory(trajectory: np.ndarray, angle: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Rotate trajectory by given angle around the origin.\n",
    "    Updates position, velocity, and heading.\n",
    "    \n",
    "    Args:\n",
    "        trajectory: Shape (timesteps, 6) where columns are [pos_x, pos_y, vel_x, vel_y, heading, obj_type]\n",
    "        angle: Rotation angle in radians\n",
    "        \n",
    "    Returns:\n",
    "        Rotated trajectory of same shape\n",
    "    \"\"\"\n",
    "    if trajectory.shape[-1] < 6:\n",
    "        raise ValueError(\"Trajectory must have 6 features\")\n",
    "        \n",
    "    cos_a, sin_a = np.cos(angle), np.sin(angle)\n",
    "    rotation_matrix = np.array([[cos_a, -sin_a],\n",
    "                               [sin_a, cos_a]])\n",
    "    \n",
    "    rotated_trajectory = trajectory.copy()\n",
    "    \n",
    "    # Rotate positions (columns 0, 1)\n",
    "    pos_xy = trajectory[:, :2]  # position_x, position_y\n",
    "    rotated_pos = pos_xy @ rotation_matrix.T\n",
    "    rotated_trajectory[:, :2] = rotated_pos\n",
    "    \n",
    "    # Rotate velocities (columns 2, 3)\n",
    "    vel_xy = trajectory[:, 2:4]  # velocity_x, velocity_y\n",
    "    rotated_vel = vel_xy @ rotation_matrix.T\n",
    "    rotated_trajectory[:, 2:4] = rotated_vel\n",
    "    \n",
    "    # Update heading (column 4)\n",
    "    rotated_trajectory[:, 4] = trajectory[:, 4] + angle\n",
    "    \n",
    "    # Keep object_type unchanged (column 5)\n",
    "    \n",
    "    return rotated_trajectory\n",
    "\n",
    "def translate_trajectory(trajectory: np.ndarray, dx: float, dy: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Translate trajectory by given offsets.\n",
    "    Only affects position coordinates.\n",
    "    \n",
    "    Args:\n",
    "        trajectory: Shape (timesteps, 6)\n",
    "        dx: Translation in x direction\n",
    "        dy: Translation in y direction\n",
    "        \n",
    "    Returns:\n",
    "        Translated trajectory of same shape\n",
    "    \"\"\"\n",
    "    if trajectory.shape[-1] < 6:\n",
    "        raise ValueError(\"Trajectory must have 6 features\")\n",
    "        \n",
    "    translated_trajectory = trajectory.copy()\n",
    "    translated_trajectory[:, 0] += dx  # position_x\n",
    "    translated_trajectory[:, 1] += dy  # position_y\n",
    "    \n",
    "    return translated_trajectory\n",
    "\n",
    "def flip_horizontal(trajectory: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Flip trajectory horizontally (mirror across y-axis).\n",
    "    Updates position, velocity, and heading.\n",
    "    \"\"\"\n",
    "    if trajectory.shape[-1] < 6:\n",
    "        raise ValueError(\"Trajectory must have 6 features\")\n",
    "        \n",
    "    flipped = trajectory.copy()\n",
    "    flipped[:, 0] = -flipped[:, 0]  # Negate position_x\n",
    "    flipped[:, 2] = -flipped[:, 2]  # Negate velocity_x\n",
    "    flipped[:, 4] = np.pi - flipped[:, 4]  # Mirror heading across y-axis\n",
    "    \n",
    "    return flipped\n",
    "\n",
    "def flip_vertical(trajectory: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Flip trajectory vertically (mirror across x-axis).\n",
    "    Updates position, velocity, and heading.\n",
    "    \"\"\"\n",
    "    if trajectory.shape[-1] < 6:\n",
    "        raise ValueError(\"Trajectory must have 6 features\")\n",
    "        \n",
    "    flipped = trajectory.copy()\n",
    "    flipped[:, 1] = -flipped[:, 1]  # Negate position_y\n",
    "    flipped[:, 3] = -flipped[:, 3]  # Negate velocity_y\n",
    "    flipped[:, 4] = -flipped[:, 4]  # Mirror heading across x-axis\n",
    "    \n",
    "    return flipped\n",
    "\n",
    "def augment_single_sample(sample: np.ndarray, \n",
    "                         rotation_range: float = np.pi/4,\n",
    "                         translation_range: float = 5.0,\n",
    "                         flip_prob: float = 0.5) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Apply random augmentations to a single sample.\n",
    "    \n",
    "    Args:\n",
    "        sample: Shape (agents, timesteps, 6) for single scene\n",
    "        rotation_range: Maximum rotation angle in radians\n",
    "        translation_range: Maximum translation distance\n",
    "        flip_prob: Probability of applying flips\n",
    "        \n",
    "    Returns:\n",
    "        Augmented sample of same shape\n",
    "    \"\"\"\n",
    "    if len(sample.shape) != 3 or sample.shape[2] != 6:\n",
    "        raise ValueError(f\"Expected shape (agents, timesteps, 6), got {sample.shape}\")\n",
    "    \n",
    "    augmented = sample.copy()\n",
    "    \n",
    "    # Random rotation, translation parameters (same for all agents in scene)\n",
    "    angle = np.random.uniform(-rotation_range, rotation_range)\n",
    "    dx = np.random.uniform(-translation_range, translation_range)\n",
    "    dy = np.random.uniform(-translation_range, translation_range)\n",
    "    \n",
    "    # Random flip decisions (same for all agents in scene)\n",
    "    do_horizontal_flip = np.random.random() < flip_prob\n",
    "    do_vertical_flip = np.random.random() < flip_prob\n",
    "    \n",
    "    for agent_idx in range(sample.shape[0]):\n",
    "        trajectory = sample[agent_idx, :, :]  # Shape (timesteps, 6)\n",
    "        \n",
    "        # Skip if trajectory is all zeros (padding/inactive agent)\n",
    "        if np.all(trajectory[:, :2] == 0):  # Check if positions are all zero\n",
    "            continue\n",
    "            \n",
    "        # Apply transformations\n",
    "        trajectory = rotate_trajectory(trajectory, angle)\n",
    "        trajectory = translate_trajectory(trajectory, dx, dy)\n",
    "        \n",
    "        if do_horizontal_flip:\n",
    "            trajectory = flip_horizontal(trajectory)\n",
    "        \n",
    "        if do_vertical_flip:\n",
    "            trajectory = flip_vertical(trajectory)\n",
    "            \n",
    "        augmented[agent_idx, :, :] = trajectory\n",
    "    \n",
    "    return augmented\n",
    "\n",
    "def augment_batch_worker(batch_data: np.ndarray,\n",
    "                        rotation_range: float,\n",
    "                        translation_range: float,\n",
    "                        flip_prob: float) -> np.ndarray:\n",
    "    \"\"\"Process a batch of samples for parallel augmentation.\"\"\"\n",
    "    return np.array([augment_single_sample(sample,\n",
    "                                          rotation_range,\n",
    "                                          translation_range,\n",
    "                                          flip_prob)\n",
    "                     for sample in batch_data])\n",
    "\n",
    "def augment_dataset(data: np.ndarray, \n",
    "                   num_augmentations: int = 5,\n",
    "                   rotation_range: float = np.pi/4,\n",
    "                   translation_range: float = 5.0,\n",
    "                   flip_prob: float = 0.5,\n",
    "                   return_original: bool = True,\n",
    "                   n_jobs: Optional[int] = None,\n",
    "                   batch_size: Optional[int] = None) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Augment entire dataset.\n",
    "    \n",
    "    Args:\n",
    "        data: Shape (scenes, agents, timesteps, 6) dataset\n",
    "        num_augmentations: Number of augmented versions per sample\n",
    "        rotation_range: Maximum rotation angle in radians\n",
    "        translation_range: Maximum translation distance\n",
    "        flip_prob: Probability of applying flips\n",
    "        return_original: Whether to include original data in output\n",
    "        n_jobs: Number of parallel jobs\n",
    "        batch_size: Size of batches for parallel processing\n",
    "        \n",
    "    Returns:\n",
    "        Augmented dataset with shape (scenes*(1+num_augmentations), agents, timesteps, 6)\n",
    "    \"\"\"\n",
    "    if len(data.shape) != 4 or data.shape[3] != 6:\n",
    "        raise ValueError(f\"Expected shape (scenes, agents, timesteps, 6), got {data.shape}\")\n",
    "    \n",
    "    original_size = data.shape[0]\n",
    "    augmented_samples = []\n",
    "    \n",
    "    if return_original:\n",
    "        augmented_samples.append(data)\n",
    "\n",
    "    if n_jobs is None:\n",
    "        n_jobs = min(os.cpu_count(), 8)\n",
    "    elif n_jobs == -1:\n",
    "        n_jobs = os.cpu_count()\n",
    "\n",
    "    if batch_size is None:\n",
    "        batch_size = max(1, original_size // n_jobs)\n",
    "\n",
    "    print(f\"Using {n_jobs} jobs with batch size {batch_size}\")\n",
    "    \n",
    "    for aug_idx in tqdm(range(num_augmentations), desc=\"Augmentations\"):\n",
    "        batches = [data[i:i + batch_size] for i in range(0, original_size, batch_size)]\n",
    "\n",
    "        augmented_batches = Parallel(n_jobs=n_jobs)(\n",
    "            delayed(augment_batch_worker)(\n",
    "                batch,\n",
    "                rotation_range,\n",
    "                translation_range,\n",
    "                flip_prob\n",
    "            ) for batch in tqdm(batches, desc=f\"Processing aug {aug_idx + 1}\", leave=False)\n",
    "        )\n",
    "\n",
    "        augmented_batch = np.concatenate(augmented_batches, axis=0)\n",
    "        augmented_samples.append(augmented_batch)\n",
    "\n",
    "    return np.concatenate(augmented_samples, axis=0)\n",
    "\n",
    "def visualize_augmentations(original_sample: np.ndarray, \n",
    "                           num_examples: int = 4,\n",
    "                           agent_idx: int = 0) -> None:\n",
    "    \"\"\"\n",
    "    Visualize original and augmented trajectories for comparison.\n",
    "    \n",
    "    Args:\n",
    "        original_sample: Single sample of shape (agents, timesteps, 6)\n",
    "        num_examples: Number of augmented examples to show\n",
    "        agent_idx: Which agent's trajectory to visualize\n",
    "    \"\"\"\n",
    "    if len(original_sample.shape) != 3 or original_sample.shape[2] != 6:\n",
    "        raise ValueError(f\"Expected shape (agents, timesteps, 6), got {original_sample.shape}\")\n",
    "    \n",
    "    if agent_idx >= original_sample.shape[0]:\n",
    "        raise ValueError(f\"agent_idx {agent_idx} >= number of agents {original_sample.shape[0]}\")\n",
    "    \n",
    "    fig, axes = plt.subplots(1, num_examples + 1, figsize=(15, 3))\n",
    "    \n",
    "    # Extract original trajectory (position_x, position_y)\n",
    "    orig_traj = original_sample[agent_idx, :, :2]  # Shape (timesteps, 2)\n",
    "    \n",
    "    # Plot original\n",
    "    axes[0].plot(orig_traj[:, 0], orig_traj[:, 1], 'b-o', markersize=3)\n",
    "    axes[0].set_title('Original')\n",
    "    axes[0].grid(True)\n",
    "    axes[0].axis('equal')\n",
    "    \n",
    "    # Plot augmented versions\n",
    "    for i in range(num_examples):\n",
    "        aug_sample = augment_single_sample(original_sample)\n",
    "        aug_traj = aug_sample[agent_idx, :, :2]  # Shape (timesteps, 2)\n",
    "        \n",
    "        axes[i + 1].plot(aug_traj[:, 0], aug_traj[:, 1], 'r-o', markersize=3)\n",
    "        axes[i + 1].set_title(f'Augmented {i + 1}')\n",
    "        axes[i + 1].grid(True)\n",
    "        axes[i + 1].axis('equal')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T19:23:46.903436Z",
     "start_time": "2025-06-01T19:23:46.886568Z"
    }
   },
   "id": "ec139c13ea8edd77",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1500x300 with 5 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_augmentations(train_data[0], agent_idx=0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T19:23:47.179125Z",
     "start_time": "2025-06-01T19:23:46.897412Z"
    }
   },
   "id": "68345685a4090686",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14d9241b88b2e2e4",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T19:23:47.193074Z",
     "start_time": "2025-06-01T19:23:47.178309Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input, RepeatVector, TimeDistributed, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9df4b831ee8e017",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T19:23:47.195180Z",
     "start_time": "2025-06-01T19:23:47.189958Z"
    }
   },
   "outputs": [],
   "source": [
    "def standardize_data_dimensions(scenario_data):\n",
    "    \"\"\"\n",
    "    Standardize position data by centering a single scenario at the origin.\n",
    "    \n",
    "    :param scenario_data: numpy array of shape (50, 110, 6)\n",
    "                         where dimensions are [position_x, position_y, velocity_x, velocity_y, heading, object_type]\n",
    "    :returns: tuple of (standardized_data, min_values)\n",
    "             - standardized_data: same shape as input with centered positions\n",
    "             - min_values: array of shape (2,) containing [min_x, min_y] for this scenario\n",
    "    \"\"\"\n",
    "    # Copy the data to avoid modifying the original\n",
    "    standardized_data = scenario_data.copy()\n",
    "    \n",
    "    # Extract position data (first 2 dimensions)\n",
    "    positions = scenario_data[:, :, :2]  # Shape: (50, 110, 2)\n",
    "    \n",
    "    # Create mask for non-zero positions (to ignore padding)\n",
    "    # We consider a position valid if it's not (0,0) or if the object_type is not 0\n",
    "    object_types = scenario_data[:, :, 5]  # Shape: (50, 110)\n",
    "    valid_mask = (positions[:, :, 0] != 0) | (positions[:, :, 1] != 0) | (object_types != 0)\n",
    "    \n",
    "    # Find min values across all valid positions in this scenario\n",
    "    if np.any(valid_mask):\n",
    "        valid_positions = positions[valid_mask]  # Shape: (num_valid_points, 2)\n",
    "        min_x = np.min(valid_positions[:, 0])\n",
    "        min_y = np.min(valid_positions[:, 1])\n",
    "    else:\n",
    "        # If no valid positions found, use 0 as min values\n",
    "        min_x = 0\n",
    "        min_y = 0\n",
    "    \n",
    "    # Store min values\n",
    "    min_values = np.array([min_x, min_y])\n",
    "    \n",
    "    # Standardize positions by subtracting min values\n",
    "    # Only modify non-zero positions to preserve padding\n",
    "    for agent_idx in range(scenario_data.shape[0]):\n",
    "        for time_idx in range(scenario_data.shape[1]):\n",
    "            if valid_mask[agent_idx, time_idx]:\n",
    "                standardized_data[agent_idx, time_idx, 0] -= min_x  # position_x\n",
    "                standardized_data[agent_idx, time_idx, 1] -= min_y  # position_y\n",
    "    \n",
    "    return standardized_data, min_values\n",
    "\n",
    "\n",
    "def denormalize_predictions(predictions, min_values):\n",
    "    \"\"\"\n",
    "    Helper function to add back the min values to predicted positions.\n",
    "    \n",
    "    :param predictions: predicted data with standardized positions, shape (50, 110, 6) or similar\n",
    "    :param min_values: array of shape (2,) containing [min_x, min_y] for this scenario\n",
    "    :returns: predictions with original coordinate system restored\n",
    "    \"\"\"\n",
    "    denormalized = predictions.copy()\n",
    "    \n",
    "    # Add back the min values to restore original coordinate system\n",
    "    # Assuming predictions have position_x and position_y as first two dimensions\n",
    "    denormalized[:, :, 0] += min_values[0]  # position_x\n",
    "    denormalized[:, :, 1] += min_values[1]  # position_y\n",
    "    \n",
    "    return denormalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db886a7f7f3d9df5",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T19:23:47.433549Z",
     "start_time": "2025-06-01T19:23:47.194686Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "from tqdm import tqdm\n",
    "\n",
    "def standardize_single_scene(scene_data):\n",
    "    \"\"\"\n",
    "    Wrapper function to standardize a single scene and return both standardized data and min values.\n",
    "    This function will be called in parallel.\n",
    "    \"\"\"\n",
    "    standardized_scene, min_vals = standardize_data_dimensions(scene_data)\n",
    "    return standardized_scene, min_vals\n",
    "\n",
    "def parallel_standardize_training_data(train_data, n_jobs=-1, verbose=True):\n",
    "    \"\"\"\n",
    "    Parallelize the standardization of training data across all scenes.\n",
    "    \n",
    "    :param train_data: numpy array of shape (10000, 50, 110, 6)\n",
    "    :param n_jobs: number of parallel jobs (-1 uses all available cores)\n",
    "    :param verbose: whether to show progress bar\n",
    "    :returns: tuple of (standardized_data, min_values_array)\n",
    "    \"\"\"\n",
    "    print(f\"Standardizing {train_data.shape[0]} scenes using {multiprocessing.cpu_count() if n_jobs == -1 else n_jobs} cores...\")\n",
    "    \n",
    "    # Use joblib to parallelize the processing\n",
    "    if verbose:\n",
    "        # With progress bar\n",
    "        results = Parallel(n_jobs=n_jobs)(\n",
    "            delayed(standardize_single_scene)(train_data[i]) \n",
    "            for i in tqdm(range(train_data.shape[0]), desc=\"Processing scenes\")\n",
    "        )\n",
    "    else:\n",
    "        # Without progress bar\n",
    "        results = Parallel(n_jobs=n_jobs)(\n",
    "            delayed(standardize_single_scene)(train_data[i]) \n",
    "            for i in range(train_data.shape[0])\n",
    "        )\n",
    "    \n",
    "    # Unpack results\n",
    "    standardized_scenes, min_values_list = zip(*results)\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    standardized_data = np.array(standardized_scenes)\n",
    "    min_values_array = np.array(min_values_list)\n",
    "    \n",
    "    print(f\"Standardization complete!\")\n",
    "    print(f\"Standardized data shape: {standardized_data.shape}\")\n",
    "    print(f\"Min values shape: {min_values_array.shape}\")\n",
    "    \n",
    "    return standardized_data, min_values_array"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# full agents are too much data\n",
    "num_agents = 15\n",
    "train_data = train_data[:, :num_agents, :, :]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T19:23:47.477019Z",
     "start_time": "2025-06-01T19:23:47.198564Z"
    }
   },
   "id": "e6c232753299d794",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented data not found. Running augmentation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting chunk 1/5...\n",
      "Using 8 jobs with batch size 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Augmentations:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "Processing aug 1:   0%|          | 0/80 [00:00<?, ?it/s]\u001B[A\u001B[A\n",
      "\n",
      "Processing aug 1:  10%|█         | 8/80 [00:00<00:00, 78.96it/s]\u001B[A\u001B[A\n",
      "\n",
      "Processing aug 1:  20%|██        | 16/80 [00:00<00:01, 48.59it/s]\u001B[A\u001B[A\n",
      "\n",
      "Processing aug 1:  30%|███       | 24/80 [00:00<00:01, 52.25it/s]\u001B[A\u001B[A\n",
      "\n",
      "Processing aug 1:  50%|█████     | 40/80 [00:00<00:00, 78.89it/s]\u001B[A\u001B[A\n",
      "\n",
      "Processing aug 1:  90%|█████████ | 72/80 [00:00<00:00, 136.38it/s]\u001B[A\u001B[A\n",
      "\n",
      "                                                                  \u001B[A\u001B[A\n",
      "Augmentations:  50%|█████     | 1/2 [00:01<00:01,  1.02s/it]\u001B[A\n",
      "\n",
      "Processing aug 2:   0%|          | 0/80 [00:00<?, ?it/s]\u001B[A\u001B[A\n",
      "\n",
      "Processing aug 2:  60%|██████    | 48/80 [00:00<00:00, 308.52it/s]\u001B[A\u001B[A\n",
      "\n",
      "                                                                  \u001B[A\u001B[A\n",
      "Augmentations: 100%|██████████| 2/2 [00:01<00:00,  1.15it/s]\u001B[A\n",
      "Processing chunks:  20%|██        | 1/5 [00:02<00:08,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting chunk 2/5...\n",
      "Using 8 jobs with batch size 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Augmentations:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "Processing aug 1:   0%|          | 0/80 [00:00<?, ?it/s]\u001B[A\u001B[A\n",
      "\n",
      "Processing aug 1:  40%|████      | 32/80 [00:00<00:00, 264.67it/s]\u001B[A\u001B[A\n",
      "\n",
      "Processing aug 1: 100%|██████████| 80/80 [00:00<00:00, 220.31it/s]\u001B[A\u001B[A\n",
      "\n",
      "                                                                  \u001B[A\u001B[A\n",
      "Augmentations:  50%|█████     | 1/2 [00:00<00:00,  1.28it/s]\u001B[A\n",
      "\n",
      "Processing aug 2:   0%|          | 0/80 [00:00<?, ?it/s]\u001B[A\u001B[A\n",
      "\n",
      "Processing aug 2:  60%|██████    | 48/80 [00:00<00:00, 289.14it/s]\u001B[A\u001B[A\n",
      "\n",
      "Processing aug 2: 100%|██████████| 80/80 [00:00<00:00, 279.61it/s]\u001B[A\u001B[A\n",
      "\n",
      "                                                                  \u001B[A\u001B[A\n",
      "Augmentations: 100%|██████████| 2/2 [00:01<00:00,  1.39it/s]\u001B[A\n",
      "Processing chunks:  40%|████      | 2/5 [00:03<00:05,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting chunk 3/5...\n",
      "Using 8 jobs with batch size 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Augmentations:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "Processing aug 1:   0%|          | 0/80 [00:00<?, ?it/s]\u001B[A\u001B[A\n",
      "\n",
      "Processing aug 1:  40%|████      | 32/80 [00:00<00:00, 223.78it/s]\u001B[A\u001B[A\n",
      "\n",
      "Processing aug 1: 100%|██████████| 80/80 [00:00<00:00, 222.36it/s]\u001B[A\u001B[A\n",
      "\n",
      "                                                                  \u001B[A\u001B[A\n",
      "Augmentations:  50%|█████     | 1/2 [00:00<00:00,  1.26it/s]\u001B[A\n",
      "\n",
      "Processing aug 2:   0%|          | 0/80 [00:00<?, ?it/s]\u001B[A\u001B[A\n",
      "\n",
      "Processing aug 2:  60%|██████    | 48/80 [00:00<00:00, 319.09it/s]\u001B[A\u001B[A\n",
      "\n",
      "Processing aug 2: 100%|██████████| 80/80 [00:00<00:00, 192.95it/s]\u001B[A\u001B[A\n",
      "\n",
      "                                                                  \u001B[A\u001B[A\n",
      "Augmentations: 100%|██████████| 2/2 [00:01<00:00,  1.16it/s]\u001B[A\n",
      "Processing chunks:  60%|██████    | 3/5 [00:05<00:03,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting chunk 4/5...\n",
      "Using 8 jobs with batch size 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Augmentations:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "Processing aug 1:   0%|          | 0/80 [00:00<?, ?it/s]\u001B[A\u001B[A\n",
      "\n",
      "Processing aug 1:  40%|████      | 32/80 [00:00<00:00, 289.48it/s]\u001B[A\u001B[A\n",
      "\n",
      "Processing aug 1:  76%|███████▋  | 61/80 [00:00<00:00, 274.42it/s]\u001B[A\u001B[A\n",
      "\n",
      "                                                                  \u001B[A\u001B[A\n",
      "Augmentations:  50%|█████     | 1/2 [00:00<00:00,  1.23it/s]\u001B[A\n",
      "\n",
      "Processing aug 2:   0%|          | 0/80 [00:00<?, ?it/s]\u001B[A\u001B[A\n",
      "\n",
      "Processing aug 2:  60%|██████    | 48/80 [00:00<00:00, 293.53it/s]\u001B[A\u001B[A\n",
      "\n",
      "Processing aug 2: 100%|██████████| 80/80 [00:00<00:00, 255.33it/s]\u001B[A\u001B[A\n",
      "\n",
      "                                                                  \u001B[A\u001B[A\n",
      "Augmentations: 100%|██████████| 2/2 [00:01<00:00,  1.30it/s]\u001B[A\n",
      "Processing chunks:  80%|████████  | 4/5 [00:07<00:01,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting chunk 5/5...\n",
      "Using 8 jobs with batch size 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Augmentations:   0%|          | 0/2 [00:00<?, ?it/s]\u001B[A\n",
      "\n",
      "Processing aug 1:   0%|          | 0/80 [00:00<?, ?it/s]\u001B[A\u001B[A\n",
      "\n",
      "Processing aug 1:  40%|████      | 32/80 [00:00<00:00, 248.60it/s]\u001B[A\u001B[A\n",
      "\n",
      "Processing aug 1: 100%|██████████| 80/80 [00:00<00:00, 254.05it/s]\u001B[A\u001B[A\n",
      "\n",
      "                                                                  \u001B[A\u001B[A\n",
      "Augmentations:  50%|█████     | 1/2 [00:00<00:00,  1.28it/s]\u001B[A\n",
      "\n",
      "Processing aug 2:   0%|          | 0/80 [00:00<?, ?it/s]\u001B[A\u001B[A\n",
      "\n",
      "Processing aug 2:  60%|██████    | 48/80 [00:00<00:00, 287.65it/s]\u001B[A\u001B[A\n",
      "\n",
      "                                                                  \u001B[A\u001B[A\n",
      "Augmentations: 100%|██████████| 2/2 [00:01<00:00,  1.34it/s]\u001B[A\n",
      "Processing chunks: 100%|██████████| 5/5 [00:09<00:00,  1.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmentation done. Preparing to dump to pickle...\n",
      "Original shape: (10000, 15, 110, 6)\n",
      "Augmented shape: (30000, 15, 110, 6)\n"
     ]
    }
   ],
   "source": [
    "# Main execution code\n",
    "augmented_pkl_path = Path(\"augmented_train.pkl\")\n",
    "\n",
    "# Check if the file exists\n",
    "if augmented_pkl_path.exists():\n",
    "    print(\"Loading augmented data from pickle file...\")\n",
    "    with open(augmented_pkl_path, \"rb\") as f:\n",
    "        augmented_train = pickle.load(f)\n",
    "else:\n",
    "    print(\"Augmented data not found. Running augmentation...\")\n",
    "    # Split into 5 chunks\n",
    "    num_chunks = 5\n",
    "    chunk_size = len(train_data) // num_chunks\n",
    "    chunks = [train_data[i * chunk_size: (i + 1) * chunk_size] for i in range(num_chunks - 1)]\n",
    "    chunks.append(train_data[(num_chunks - 1) * chunk_size:])  # last chunk\n",
    "    \n",
    "    augmented_chunks = []\n",
    "    for i, chunk in enumerate(tqdm(chunks, desc=\"Processing chunks\")):\n",
    "        print(f\"Augmenting chunk {i+1}/{num_chunks}...\")\n",
    "        augmented_chunk = augment_dataset(\n",
    "            chunk, \n",
    "            num_augmentations=2,\n",
    "            n_jobs=-1,\n",
    "            batch_size=25\n",
    "        )\n",
    "        augmented_chunks.append(augmented_chunk)\n",
    "        \n",
    "        # Force cleanup\n",
    "        del augmented_chunk\n",
    "        del chunk\n",
    "        gc.collect()\n",
    "    \n",
    "    # Concatenate all augmented chunks\n",
    "    augmented_train = np.concatenate(augmented_chunks, axis=0)\n",
    "    print(\"Augmentation done. Preparing to dump to pickle...\")\n",
    "    \n",
    "    # Uncomment to save to pickle\n",
    "    # with open(augmented_pkl_path, \"wb\") as f:\n",
    "    #     pickle.dump(augmented_train, f)\n",
    "    # print(\"Dumped to pickle.\")\n",
    "\n",
    "print(f\"Original shape: {train_data.shape}\")\n",
    "print(f\"Augmented shape: {augmented_train.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T19:23:59.437756Z",
     "start_time": "2025-06-01T19:23:47.203286Z"
    }
   },
   "id": "a031bfd1adf1010e",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c81976ba8c9b548",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T19:24:38.628853Z",
     "start_time": "2025-06-01T19:23:59.472387Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardizing 30000 scenes using 8 cores...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing scenes: 100%|██████████| 30000/30000 [00:21<00:00, 1408.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardization complete!\n",
      "Standardized data shape: (30000, 15, 110, 6)\n",
      "Min values shape: (30000, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": "(30000, 15, 110, 6)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standardized_train_data, min_values = parallel_standardize_training_data(\n",
    "    augmented_train, \n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    verbose=True\n",
    ")\n",
    "standardized_train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "made it\n"
     ]
    }
   ],
   "source": [
    "print('made it')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T19:24:38.672636Z",
     "start_time": "2025-06-01T19:24:38.614422Z"
    }
   },
   "id": "79a611dceec09135",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d49a92ffcd8f00c",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T19:24:39.049828Z",
     "start_time": "2025-06-01T19:24:38.629563Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, RepeatVector, TimeDistributed\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "\n",
    "class AgentSelector(layers.Layer):\n",
    "    \"\"\"Custom layer to select agent data - serializable\"\"\"\n",
    "    def __init__(self, agent_idx, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.agent_idx = agent_idx\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return inputs[:, self.agent_idx, :, :]\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"agent_idx\": self.agent_idx})\n",
    "        return config\n",
    "\n",
    "class StackEncodings(layers.Layer):\n",
    "    \"\"\"Custom layer to stack encodings - serializable\"\"\"\n",
    "    def call(self, inputs):\n",
    "        return tf.stack(inputs, axis=1)\n",
    "\n",
    "class SocialAttentionPooling(layers.Layer):\n",
    "    \"\"\"Attention-based social pooling using all 6 features for attention computation\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        # input_shape: [(batch, encoding_dim), (batch, num_agents, encoding_dim), (batch, num_agents, timesteps, 6)]\n",
    "        encoding_dim = input_shape[0][-1]\n",
    "        \n",
    "        # Linear projections for attention\n",
    "        self.query_projection = Dense(encoding_dim, name='query_proj')\n",
    "        self.key_projection = Dense(encoding_dim, name='key_proj')  \n",
    "        self.value_projection = Dense(encoding_dim, name='value_proj')\n",
    "        \n",
    "        # Feature attention network to process 6D features\n",
    "        self.feature_attention = Dense(64, activation='relu', name='feature_attention')\n",
    "        self.feature_to_encoding = Dense(encoding_dim, name='feature_to_encoding')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        ego_encoding, all_encodings, all_raw_features = inputs\n",
    "        \n",
    "        # Project encodings to attention space\n",
    "        query = self.query_projection(ego_encoding)  # (batch, encoding_dim)\n",
    "        keys = self.key_projection(all_encodings)    # (batch, num_agents, encoding_dim)\n",
    "        values = self.value_projection(all_encodings) # (batch, num_agents, encoding_dim)\n",
    "        \n",
    "        # Process raw 6D features for additional attention signal\n",
    "        # Take mean across timesteps to get representative features per agent\n",
    "        agent_features = tf.reduce_mean(all_raw_features, axis=2)  # (batch, num_agents, 6)\n",
    "        \n",
    "        # Transform 6D features to encoding space\n",
    "        feature_attention = self.feature_attention(agent_features)  # (batch, num_agents, 64)\n",
    "        feature_keys = self.feature_to_encoding(feature_attention)  # (batch, num_agents, encoding_dim)\n",
    "        \n",
    "        # Compute attention scores using both semantic and feature information\n",
    "        query_expanded = tf.expand_dims(query, axis=1)  # (batch, 1, encoding_dim)\n",
    "        \n",
    "        # Semantic attention scores\n",
    "        semantic_scores = tf.reduce_sum(query_expanded * keys, axis=-1)  # (batch, num_agents)\n",
    "        \n",
    "        # Feature-based attention scores  \n",
    "        feature_scores = tf.reduce_sum(query_expanded * feature_keys, axis=-1)  # (batch, num_agents)\n",
    "        \n",
    "        # Combine both attention signals\n",
    "        combined_scores = semantic_scores + feature_scores  # (batch, num_agents)\n",
    "        attention_weights = tf.nn.softmax(combined_scores, axis=-1)  # (batch, num_agents)\n",
    "        \n",
    "        # Apply attention weights\n",
    "        attention_weights_expanded = tf.expand_dims(attention_weights, axis=-1)  # (batch, num_agents, 1)\n",
    "        social_context = tf.reduce_sum(values * attention_weights_expanded, axis=1)  # (batch, encoding_dim)\n",
    "        \n",
    "        return social_context\n",
    "\n",
    "class MultiHeadSocialAttention(layers.Layer):\n",
    "    \"\"\"Multi-head attention for social pooling with 6D feature integration\"\"\"\n",
    "    def __init__(self, num_heads=4, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        encoding_dim = input_shape[0][-1]\n",
    "        assert encoding_dim % self.num_heads == 0\n",
    "        \n",
    "        self.head_dim = encoding_dim // self.num_heads\n",
    "        self.encoding_dim = encoding_dim\n",
    "        \n",
    "        # Projections for each head\n",
    "        self.query_projections = [Dense(self.head_dim, name=f'query_head_{i}') \n",
    "                                 for i in range(self.num_heads)]\n",
    "        self.key_projections = [Dense(self.head_dim, name=f'key_head_{i}') \n",
    "                               for i in range(self.num_heads)]\n",
    "        self.value_projections = [Dense(self.head_dim, name=f'value_head_{i}') \n",
    "                                 for i in range(self.num_heads)]\n",
    "        \n",
    "        # Feature processing for each head\n",
    "        self.feature_projections = [Dense(self.head_dim, name=f'feature_head_{i}')\n",
    "                                   for i in range(self.num_heads)]\n",
    "        \n",
    "        self.output_projection = Dense(encoding_dim, name='output_proj')\n",
    "        \n",
    "        # 6D feature processing\n",
    "        self.feature_processor = Dense(encoding_dim, activation='relu', name='feature_processor')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        ego_encoding, all_encodings, all_raw_features = inputs\n",
    "        \n",
    "        # Process 6D features\n",
    "        agent_features = tf.reduce_mean(all_raw_features, axis=2)  # (batch, num_agents, 6)\n",
    "        processed_features = self.feature_processor(agent_features)  # (batch, num_agents, encoding_dim)\n",
    "        \n",
    "        head_outputs = []\n",
    "        \n",
    "        for i in range(self.num_heads):\n",
    "            # Project to query, key, value for this head\n",
    "            query = self.query_projections[i](ego_encoding)  # (batch, head_dim)\n",
    "            keys = self.key_projections[i](all_encodings)    # (batch, num_agents, head_dim)\n",
    "            values = self.value_projections[i](all_encodings) # (batch, num_agents, head_dim)\n",
    "            \n",
    "            # Add feature-based keys\n",
    "            feature_keys = self.feature_projections[i](processed_features)  # (batch, num_agents, head_dim)\n",
    "            combined_keys = keys + feature_keys\n",
    "            \n",
    "            # Compute attention\n",
    "            query_expanded = tf.expand_dims(query, axis=1)\n",
    "            attention_scores = tf.reduce_sum(query_expanded * combined_keys, axis=-1) / tf.sqrt(float(self.head_dim))\n",
    "            attention_weights = tf.nn.softmax(attention_scores, axis=-1)\n",
    "            \n",
    "            # Apply attention\n",
    "            attention_weights_expanded = tf.expand_dims(attention_weights, axis=-1)\n",
    "            head_output = tf.reduce_sum(values * attention_weights_expanded, axis=1)\n",
    "            head_outputs.append(head_output)\n",
    "        \n",
    "        # Concatenate heads and project\n",
    "        concatenated = tf.concat(head_outputs, axis=-1)\n",
    "        social_context = self.output_projection(concatenated)\n",
    "        \n",
    "        return social_context\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"num_heads\": self.num_heads})\n",
    "        return config\n",
    "\n",
    "class PositionAwareSocialAttention(layers.Layer):\n",
    "    \"\"\"Attention that uses all 6 dimensions of agent data\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        # input_shape: [(batch, encoding_dim), (batch, num_agents, encoding_dim), (batch, num_agents, timesteps, 6)]\n",
    "        encoding_dim = input_shape[0][-1]\n",
    "        \n",
    "        # Encoding projections\n",
    "        self.query_projection = Dense(encoding_dim, name='query_proj')\n",
    "        self.key_projection = Dense(encoding_dim, name='key_proj')\n",
    "        self.value_projection = Dense(encoding_dim, name='value_proj')\n",
    "        \n",
    "        # 6D feature processing layers\n",
    "        self.position_encoder = Dense(64, activation='relu', name='position_encoder')  # for x,y\n",
    "        self.velocity_encoder = Dense(64, activation='relu', name='velocity_encoder')  # for vx,vy\n",
    "        self.heading_encoder = Dense(32, activation='relu', name='heading_encoder')    # for heading\n",
    "        self.object_type_encoder = Dense(32, activation='relu', name='object_type_encoder')  # for object_type\n",
    "        \n",
    "        # Combine all feature encodings\n",
    "        self.feature_combiner = Dense(encoding_dim, activation='relu', name='feature_combiner')\n",
    "        \n",
    "        # Final attention combination\n",
    "        self.attention_combiner = Dense(1, name='attention_combiner')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        ego_encoding, all_encodings, all_raw_features = inputs\n",
    "        \n",
    "        # Standard attention components\n",
    "        query = self.query_projection(ego_encoding)\n",
    "        keys = self.key_projection(all_encodings)\n",
    "        values = self.value_projection(all_encodings)\n",
    "        \n",
    "        # Process 6D features - take mean across timesteps for representative features\n",
    "        agent_features = tf.reduce_mean(all_raw_features, axis=2)  # (batch, num_agents, 6)\n",
    "        \n",
    "        # Split and encode different feature types\n",
    "        positions = agent_features[:, :, :2]      # x, y\n",
    "        velocities = agent_features[:, :, 2:4]    # vx, vy  \n",
    "        headings = agent_features[:, :, 4:5]      # heading\n",
    "        object_types = agent_features[:, :, 5:6]  # object_type\n",
    "        \n",
    "        # Encode each feature type\n",
    "        pos_encoded = self.position_encoder(positions)      # (batch, num_agents, 64)\n",
    "        vel_encoded = self.velocity_encoder(velocities)     # (batch, num_agents, 64)\n",
    "        head_encoded = self.heading_encoder(headings)       # (batch, num_agents, 32)\n",
    "        type_encoded = self.object_type_encoder(object_types)  # (batch, num_agents, 32)\n",
    "        \n",
    "        # Combine all feature encodings\n",
    "        combined_features = tf.concat([pos_encoded, vel_encoded, head_encoded, type_encoded], axis=-1)\n",
    "        feature_context = self.feature_combiner(combined_features)  # (batch, num_agents, encoding_dim)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        query_expanded = tf.expand_dims(query, axis=1)\n",
    "        semantic_scores = tf.reduce_sum(query_expanded * keys, axis=-1, keepdims=True)  # (batch, num_agents, 1)\n",
    "        feature_scores = tf.reduce_sum(query_expanded * feature_context, axis=-1, keepdims=True)  # (batch, num_agents, 1)\n",
    "        \n",
    "        # Combine both types of attention\n",
    "        combined_scores = tf.concat([semantic_scores, feature_scores], axis=-1)  # (batch, num_agents, 2)\n",
    "        attention_scores = self.attention_combiner(combined_scores)  # (batch, num_agents, 1)\n",
    "        attention_scores = tf.squeeze(attention_scores, axis=-1)  # (batch, num_agents)\n",
    "        \n",
    "        attention_weights = tf.nn.softmax(attention_scores, axis=-1)\n",
    "        \n",
    "        # Apply attention\n",
    "        attention_weights_expanded = tf.expand_dims(attention_weights, axis=-1)\n",
    "        social_context = tf.reduce_sum(values * attention_weights_expanded, axis=1)\n",
    "        \n",
    "        return social_context\n",
    "\n",
    "def create_social_lstm_with_attention(input_dim=6, output_dim=2, timesteps_in=50, timesteps_out=60, num_agents=50, lstm_units=128, num_layers=2, lr=0.001, attention_type='simple'):\n",
    "    \"\"\"\n",
    "    Social LSTM with attention-based pooling using all 6 input dimensions\n",
    "    attention_type: 'simple', 'multihead', or 'position_aware'\n",
    "    \"\"\"\n",
    "    \n",
    "    # Input: (batch_size, num_agents, timesteps_in, input_dim)\n",
    "    all_agents_input = Input(shape=(num_agents, timesteps_in, input_dim), name='all_agents_data')\n",
    "    \n",
    "    # Create shared encoder layers\n",
    "    encoder_lstms = []\n",
    "    for layer_idx in range(num_layers):\n",
    "        encoder_lstms.append(LSTM(lstm_units, return_sequences=True, name=f'shared_encoder_lstm_{layer_idx}'))\n",
    "    final_encoder = LSTM(lstm_units, name='shared_final_encoder')\n",
    "    \n",
    "    # Encode each agent using shared weights\n",
    "    agent_encodings = []\n",
    "    \n",
    "    for i in range(num_agents):\n",
    "        agent_data = AgentSelector(agent_idx=i, name=f'agent_{i}_data')(all_agents_input)\n",
    "        \n",
    "        # Apply shared encoder layers\n",
    "        x = agent_data\n",
    "        for encoder_lstm in encoder_lstms:\n",
    "            x = encoder_lstm(x)\n",
    "        encoded = final_encoder(x)\n",
    "        agent_encodings.append(encoded)\n",
    "    \n",
    "    # Stack all encodings\n",
    "    all_encodings = StackEncodings(name='stack_encodings')(agent_encodings)\n",
    "    ego_encoding = agent_encodings[0]  # First agent is ego\n",
    "    \n",
    "    # Apply attention-based social pooling with 6D features\n",
    "    if attention_type == 'simple':\n",
    "        social_context = SocialAttentionPooling(name='social_attention')(\n",
    "            [ego_encoding, all_encodings, all_agents_input])\n",
    "    elif attention_type == 'multihead':\n",
    "        social_context = MultiHeadSocialAttention(num_heads=4, name='multihead_attention')(\n",
    "            [ego_encoding, all_encodings, all_agents_input])\n",
    "    elif attention_type == 'position_aware':\n",
    "        social_context = PositionAwareSocialAttention(name='position_attention')(\n",
    "            [ego_encoding, all_encodings, all_agents_input])\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown attention_type: {attention_type}\")\n",
    "    \n",
    "    # Combine ego encoding with social context\n",
    "    combined_encoding = layers.Concatenate(name='combine_ego_social')([ego_encoding, social_context])\n",
    "    \n",
    "    # Decoder for ego agent\n",
    "    x = RepeatVector(timesteps_out, name='repeat_for_decode')(combined_encoding)\n",
    "    \n",
    "    for layer_idx in range(num_layers):\n",
    "        x = LSTM(lstm_units, return_sequences=True, name=f'decoder_lstm_{layer_idx}')(x)\n",
    "    \n",
    "    # Output layers\n",
    "    x = TimeDistributed(Dense(128, activation='relu'), name='decode_dense1')(x)\n",
    "    x = TimeDistributed(Dense(64, activation='relu'), name='decode_dense2')(x)\n",
    "    ego_output = TimeDistributed(Dense(output_dim), name='ego_trajectory')(x)\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs=all_agents_input, outputs=ego_output, name='AttentionSocialLSTM')\n",
    "    \n",
    "    # Compile\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=lr, clipnorm=1.0),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Updated load_model function with attention layer support\n",
    "from tensorflow import keras\n",
    "\n",
    "def load_model(model_path='social_lstm_2.keras'):\n",
    "    custom_objects = {\n",
    "        'AgentSelector': AgentSelector,\n",
    "        'StackEncodings': StackEncodings,\n",
    "        'SocialAttentionPooling': SocialAttentionPooling,\n",
    "        'MultiHeadSocialAttention': MultiHeadSocialAttention,\n",
    "        'PositionAwareSocialAttention': PositionAwareSocialAttention\n",
    "    }\n",
    "    \n",
    "    model = keras.models.load_model(model_path, custom_objects=custom_objects, safe_mode=False)\n",
    "    print(f\"Keras model loaded from {model_path}\")\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T19:24:39.051893Z",
     "start_time": "2025-06-01T19:24:38.664062Z"
    }
   },
   "id": "e0a6a9b5057173bc",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53288ed2ca034f2",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T19:25:00.059200Z",
     "start_time": "2025-06-01T19:25:00.055399Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "562108cbfcfe2700",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T19:25:00.518913Z",
     "start_time": "2025-06-01T19:25:00.273184Z"
    }
   },
   "outputs": [],
   "source": [
    "# Simple attention\n",
    "model = create_social_lstm_with_attention(\n",
    "    num_agents=15, \n",
    "    lstm_units=128, \n",
    "    attention_type='simple'\n",
    ")\n",
    "# \n",
    "# # Multi-head attention\n",
    "# model_multihead = create_social_lstm_with_attention(\n",
    "#     num_agents=15, \n",
    "#     lstm_units=128, \n",
    "#     attention_type='multihead'\n",
    "# )\n",
    "# \n",
    "# # Position-aware attention (requires position inputs)\n",
    "# model_position = create_social_lstm_with_attention(\n",
    "#     num_agents=15, \n",
    "#     lstm_units=128, \n",
    "#     attention_type='position_aware'\n",
    "# )\n",
    "# \n",
    "# print(\"Simple Attention Model:\")\n",
    "# print(model_simple.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b9ad83eea1e33344",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T19:25:00.893246Z",
     "start_time": "2025-06-01T19:25:00.844605Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "\u001B[1mModel: \"AttentionSocialLSTM\"\u001B[0m\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"AttentionSocialLSTM\"</span>\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃\u001B[1m \u001B[0m\u001B[1mLayer (type)       \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape     \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m   Param #\u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mConnected to     \u001B[0m\u001B[1m \u001B[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ all_agents_data     │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m15\u001B[0m, \u001B[38;5;34m50\u001B[0m, \u001B[38;5;34m6\u001B[0m) │          \u001B[38;5;34m0\u001B[0m │ -                 │\n│ (\u001B[38;5;33mInputLayer\u001B[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ agent_0_data        │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m50\u001B[0m, \u001B[38;5;34m6\u001B[0m)     │          \u001B[38;5;34m0\u001B[0m │ all_agents_data[\u001B[38;5;34m…\u001B[0m │\n│ (\u001B[38;5;33mAgentSelector\u001B[0m)     │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ agent_1_data        │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m50\u001B[0m, \u001B[38;5;34m6\u001B[0m)     │          \u001B[38;5;34m0\u001B[0m │ all_agents_data[\u001B[38;5;34m…\u001B[0m │\n│ (\u001B[38;5;33mAgentSelector\u001B[0m)     │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ agent_2_data        │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m50\u001B[0m, \u001B[38;5;34m6\u001B[0m)     │          \u001B[38;5;34m0\u001B[0m │ all_agents_data[\u001B[38;5;34m…\u001B[0m │\n│ (\u001B[38;5;33mAgentSelector\u001B[0m)     │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ agent_3_data        │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m50\u001B[0m, \u001B[38;5;34m6\u001B[0m)     │          \u001B[38;5;34m0\u001B[0m │ all_agents_data[\u001B[38;5;34m…\u001B[0m │\n│ (\u001B[38;5;33mAgentSelector\u001B[0m)     │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ agent_4_data        │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m50\u001B[0m, \u001B[38;5;34m6\u001B[0m)     │          \u001B[38;5;34m0\u001B[0m │ all_agents_data[\u001B[38;5;34m…\u001B[0m │\n│ (\u001B[38;5;33mAgentSelector\u001B[0m)     │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ agent_5_data        │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m50\u001B[0m, \u001B[38;5;34m6\u001B[0m)     │          \u001B[38;5;34m0\u001B[0m │ all_agents_data[\u001B[38;5;34m…\u001B[0m │\n│ (\u001B[38;5;33mAgentSelector\u001B[0m)     │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ agent_6_data        │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m50\u001B[0m, \u001B[38;5;34m6\u001B[0m)     │          \u001B[38;5;34m0\u001B[0m │ all_agents_data[\u001B[38;5;34m…\u001B[0m │\n│ (\u001B[38;5;33mAgentSelector\u001B[0m)     │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ agent_7_data        │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m50\u001B[0m, \u001B[38;5;34m6\u001B[0m)     │          \u001B[38;5;34m0\u001B[0m │ all_agents_data[\u001B[38;5;34m…\u001B[0m │\n│ (\u001B[38;5;33mAgentSelector\u001B[0m)     │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ agent_8_data        │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m50\u001B[0m, \u001B[38;5;34m6\u001B[0m)     │          \u001B[38;5;34m0\u001B[0m │ all_agents_data[\u001B[38;5;34m…\u001B[0m │\n│ (\u001B[38;5;33mAgentSelector\u001B[0m)     │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ agent_9_data        │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m50\u001B[0m, \u001B[38;5;34m6\u001B[0m)     │          \u001B[38;5;34m0\u001B[0m │ all_agents_data[\u001B[38;5;34m…\u001B[0m │\n│ (\u001B[38;5;33mAgentSelector\u001B[0m)     │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ agent_10_data       │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m50\u001B[0m, \u001B[38;5;34m6\u001B[0m)     │          \u001B[38;5;34m0\u001B[0m │ all_agents_data[\u001B[38;5;34m…\u001B[0m │\n│ (\u001B[38;5;33mAgentSelector\u001B[0m)     │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ agent_11_data       │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m50\u001B[0m, \u001B[38;5;34m6\u001B[0m)     │          \u001B[38;5;34m0\u001B[0m │ all_agents_data[\u001B[38;5;34m…\u001B[0m │\n│ (\u001B[38;5;33mAgentSelector\u001B[0m)     │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ agent_12_data       │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m50\u001B[0m, \u001B[38;5;34m6\u001B[0m)     │          \u001B[38;5;34m0\u001B[0m │ all_agents_data[\u001B[38;5;34m…\u001B[0m │\n│ (\u001B[38;5;33mAgentSelector\u001B[0m)     │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ agent_13_data       │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m50\u001B[0m, \u001B[38;5;34m6\u001B[0m)     │          \u001B[38;5;34m0\u001B[0m │ all_agents_data[\u001B[38;5;34m…\u001B[0m │\n│ (\u001B[38;5;33mAgentSelector\u001B[0m)     │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ agent_14_data       │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m50\u001B[0m, \u001B[38;5;34m6\u001B[0m)     │          \u001B[38;5;34m0\u001B[0m │ all_agents_data[\u001B[38;5;34m…\u001B[0m │\n│ (\u001B[38;5;33mAgentSelector\u001B[0m)     │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ shared_encoder_lst… │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m50\u001B[0m, \u001B[38;5;34m128\u001B[0m)   │     \u001B[38;5;34m69,120\u001B[0m │ agent_0_data[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m…\u001B[0m │\n│ (\u001B[38;5;33mLSTM\u001B[0m)              │                   │            │ agent_1_data[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m…\u001B[0m │\n│                     │                   │            │ agent_2_data[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m…\u001B[0m │\n│                     │                   │            │ agent_3_data[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m…\u001B[0m │\n│                     │                   │            │ agent_4_data[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m…\u001B[0m │\n│                     │                   │            │ agent_5_data[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m…\u001B[0m │\n│                     │                   │            │ agent_6_data[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m…\u001B[0m │\n│                     │                   │            │ agent_7_data[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m…\u001B[0m │\n│                     │                   │            │ agent_8_data[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m…\u001B[0m │\n│                     │                   │            │ agent_9_data[\u001B[38;5;34m0\u001B[0m][\u001B[38;5;34m…\u001B[0m │\n│                     │                   │            │ agent_10_data[\u001B[38;5;34m0\u001B[0m]… │\n│                     │                   │            │ agent_11_data[\u001B[38;5;34m0\u001B[0m]… │\n│                     │                   │            │ agent_12_data[\u001B[38;5;34m0\u001B[0m]… │\n│                     │                   │            │ agent_13_data[\u001B[38;5;34m0\u001B[0m]… │\n│                     │                   │            │ agent_14_data[\u001B[38;5;34m0\u001B[0m]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ shared_encoder_lst… │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m50\u001B[0m, \u001B[38;5;34m128\u001B[0m)   │    \u001B[38;5;34m131,584\u001B[0m │ shared_encoder_l… │\n│ (\u001B[38;5;33mLSTM\u001B[0m)              │                   │            │ shared_encoder_l… │\n│                     │                   │            │ shared_encoder_l… │\n│                     │                   │            │ shared_encoder_l… │\n│                     │                   │            │ shared_encoder_l… │\n│                     │                   │            │ shared_encoder_l… │\n│                     │                   │            │ shared_encoder_l… │\n│                     │                   │            │ shared_encoder_l… │\n│                     │                   │            │ shared_encoder_l… │\n│                     │                   │            │ shared_encoder_l… │\n│                     │                   │            │ shared_encoder_l… │\n│                     │                   │            │ shared_encoder_l… │\n│                     │                   │            │ shared_encoder_l… │\n│                     │                   │            │ shared_encoder_l… │\n│                     │                   │            │ shared_encoder_l… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ shared_final_encod… │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m128\u001B[0m)       │    \u001B[38;5;34m131,584\u001B[0m │ shared_encoder_l… │\n│ (\u001B[38;5;33mLSTM\u001B[0m)              │                   │            │ shared_encoder_l… │\n│                     │                   │            │ shared_encoder_l… │\n│                     │                   │            │ shared_encoder_l… │\n│                     │                   │            │ shared_encoder_l… │\n│                     │                   │            │ shared_encoder_l… │\n│                     │                   │            │ shared_encoder_l… │\n│                     │                   │            │ shared_encoder_l… │\n│                     │                   │            │ shared_encoder_l… │\n│                     │                   │            │ shared_encoder_l… │\n│                     │                   │            │ shared_encoder_l… │\n│                     │                   │            │ shared_encoder_l… │\n│                     │                   │            │ shared_encoder_l… │\n│                     │                   │            │ shared_encoder_l… │\n│                     │                   │            │ shared_encoder_l… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ stack_encodings     │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m15\u001B[0m, \u001B[38;5;34m128\u001B[0m)   │          \u001B[38;5;34m0\u001B[0m │ shared_final_enc… │\n│ (\u001B[38;5;33mStackEncodings\u001B[0m)    │                   │            │ shared_final_enc… │\n│                     │                   │            │ shared_final_enc… │\n│                     │                   │            │ shared_final_enc… │\n│                     │                   │            │ shared_final_enc… │\n│                     │                   │            │ shared_final_enc… │\n│                     │                   │            │ shared_final_enc… │\n│                     │                   │            │ shared_final_enc… │\n│                     │                   │            │ shared_final_enc… │\n│                     │                   │            │ shared_final_enc… │\n│                     │                   │            │ shared_final_enc… │\n│                     │                   │            │ shared_final_enc… │\n│                     │                   │            │ shared_final_enc… │\n│                     │                   │            │ shared_final_enc… │\n│                     │                   │            │ shared_final_enc… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ social_attention    │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m128\u001B[0m)       │     \u001B[38;5;34m58,304\u001B[0m │ shared_final_enc… │\n│ (\u001B[38;5;33mSocialAttentionPo…\u001B[0m │                   │            │ stack_encodings[\u001B[38;5;34m…\u001B[0m │\n│                     │                   │            │ all_agents_data[\u001B[38;5;34m…\u001B[0m │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ combine_ego_social  │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m256\u001B[0m)       │          \u001B[38;5;34m0\u001B[0m │ shared_final_enc… │\n│ (\u001B[38;5;33mConcatenate\u001B[0m)       │                   │            │ social_attention… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ repeat_for_decode   │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m60\u001B[0m, \u001B[38;5;34m256\u001B[0m)   │          \u001B[38;5;34m0\u001B[0m │ combine_ego_soci… │\n│ (\u001B[38;5;33mRepeatVector\u001B[0m)      │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ decoder_lstm_0      │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m60\u001B[0m, \u001B[38;5;34m128\u001B[0m)   │    \u001B[38;5;34m197,120\u001B[0m │ repeat_for_decod… │\n│ (\u001B[38;5;33mLSTM\u001B[0m)              │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ decoder_lstm_1      │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m60\u001B[0m, \u001B[38;5;34m128\u001B[0m)   │    \u001B[38;5;34m131,584\u001B[0m │ decoder_lstm_0[\u001B[38;5;34m0\u001B[0m… │\n│ (\u001B[38;5;33mLSTM\u001B[0m)              │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ decode_dense1       │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m60\u001B[0m, \u001B[38;5;34m128\u001B[0m)   │     \u001B[38;5;34m16,512\u001B[0m │ decoder_lstm_1[\u001B[38;5;34m0\u001B[0m… │\n│ (\u001B[38;5;33mTimeDistributed\u001B[0m)   │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ decode_dense2       │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m60\u001B[0m, \u001B[38;5;34m64\u001B[0m)    │      \u001B[38;5;34m8,256\u001B[0m │ decode_dense1[\u001B[38;5;34m0\u001B[0m]… │\n│ (\u001B[38;5;33mTimeDistributed\u001B[0m)   │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ ego_trajectory      │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m60\u001B[0m, \u001B[38;5;34m2\u001B[0m)     │        \u001B[38;5;34m130\u001B[0m │ decode_dense2[\u001B[38;5;34m0\u001B[0m]… │\n│ (\u001B[38;5;33mTimeDistributed\u001B[0m)   │                   │            │                   │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ all_agents_data     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ agent_0_data        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ all_agents_data[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AgentSelector</span>)     │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ agent_1_data        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ all_agents_data[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AgentSelector</span>)     │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ agent_2_data        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ all_agents_data[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AgentSelector</span>)     │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ agent_3_data        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ all_agents_data[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AgentSelector</span>)     │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ agent_4_data        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ all_agents_data[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AgentSelector</span>)     │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ agent_5_data        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ all_agents_data[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AgentSelector</span>)     │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ agent_6_data        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ all_agents_data[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AgentSelector</span>)     │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ agent_7_data        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ all_agents_data[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AgentSelector</span>)     │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ agent_8_data        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ all_agents_data[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AgentSelector</span>)     │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ agent_9_data        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ all_agents_data[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AgentSelector</span>)     │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ agent_10_data       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ all_agents_data[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AgentSelector</span>)     │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ agent_11_data       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ all_agents_data[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AgentSelector</span>)     │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ agent_12_data       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ all_agents_data[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AgentSelector</span>)     │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ agent_13_data       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ all_agents_data[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AgentSelector</span>)     │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ agent_14_data       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ all_agents_data[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AgentSelector</span>)     │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ shared_encoder_lst… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">69,120</span> │ agent_0_data[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)              │                   │            │ agent_1_data[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│                     │                   │            │ agent_2_data[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│                     │                   │            │ agent_3_data[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│                     │                   │            │ agent_4_data[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│                     │                   │            │ agent_5_data[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│                     │                   │            │ agent_6_data[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│                     │                   │            │ agent_7_data[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│                     │                   │            │ agent_8_data[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│                     │                   │            │ agent_9_data[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│                     │                   │            │ agent_10_data[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n│                     │                   │            │ agent_11_data[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n│                     │                   │            │ agent_12_data[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n│                     │                   │            │ agent_13_data[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n│                     │                   │            │ agent_14_data[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ shared_encoder_lst… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │ shared_encoder_l… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)              │                   │            │ shared_encoder_l… │\n│                     │                   │            │ shared_encoder_l… │\n│                     │                   │            │ shared_encoder_l… │\n│                     │                   │            │ shared_encoder_l… │\n│                     │                   │            │ shared_encoder_l… │\n│                     │                   │            │ shared_encoder_l… │\n│                     │                   │            │ shared_encoder_l… │\n│                     │                   │            │ shared_encoder_l… │\n│                     │                   │            │ shared_encoder_l… │\n│                     │                   │            │ shared_encoder_l… │\n│                     │                   │            │ shared_encoder_l… │\n│                     │                   │            │ shared_encoder_l… │\n│                     │                   │            │ shared_encoder_l… │\n│                     │                   │            │ shared_encoder_l… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ shared_final_encod… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │ shared_encoder_l… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)              │                   │            │ shared_encoder_l… │\n│                     │                   │            │ shared_encoder_l… │\n│                     │                   │            │ shared_encoder_l… │\n│                     │                   │            │ shared_encoder_l… │\n│                     │                   │            │ shared_encoder_l… │\n│                     │                   │            │ shared_encoder_l… │\n│                     │                   │            │ shared_encoder_l… │\n│                     │                   │            │ shared_encoder_l… │\n│                     │                   │            │ shared_encoder_l… │\n│                     │                   │            │ shared_encoder_l… │\n│                     │                   │            │ shared_encoder_l… │\n│                     │                   │            │ shared_encoder_l… │\n│                     │                   │            │ shared_encoder_l… │\n│                     │                   │            │ shared_encoder_l… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ stack_encodings     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ shared_final_enc… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">StackEncodings</span>)    │                   │            │ shared_final_enc… │\n│                     │                   │            │ shared_final_enc… │\n│                     │                   │            │ shared_final_enc… │\n│                     │                   │            │ shared_final_enc… │\n│                     │                   │            │ shared_final_enc… │\n│                     │                   │            │ shared_final_enc… │\n│                     │                   │            │ shared_final_enc… │\n│                     │                   │            │ shared_final_enc… │\n│                     │                   │            │ shared_final_enc… │\n│                     │                   │            │ shared_final_enc… │\n│                     │                   │            │ shared_final_enc… │\n│                     │                   │            │ shared_final_enc… │\n│                     │                   │            │ shared_final_enc… │\n│                     │                   │            │ shared_final_enc… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ social_attention    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">58,304</span> │ shared_final_enc… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SocialAttentionPo…</span> │                   │            │ stack_encodings[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│                     │                   │            │ all_agents_data[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ combine_ego_social  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ shared_final_enc… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ social_attention… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ repeat_for_decode   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ combine_ego_soci… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">RepeatVector</span>)      │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ decoder_lstm_0      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">197,120</span> │ repeat_for_decod… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)              │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ decoder_lstm_1      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │ decoder_lstm_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)              │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ decode_dense1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │ decoder_lstm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ decode_dense2       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │ decode_dense1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ ego_trajectory      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span> │ decode_dense2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │                   │            │                   │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m744,194\u001B[0m (2.84 MB)\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">744,194</span> (2.84 MB)\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m744,194\u001B[0m (2.84 MB)\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">744,194</span> (2.84 MB)\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee1df27124751b62",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T19:25:01.992254Z",
     "start_time": "2025-06-01T19:25:01.883255Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.src.callbacks import LearningRateScheduler, EarlyStopping, Callback\n",
    "from keras.src.optimizers import Adam\n",
    "from keras import Model\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "class DynamicReduceLROnPlateau(Callback):\n",
    "    def __init__(self, factor=0.1, patience=3, min_lr=1e-7, verbose=1):\n",
    "        super().__init__()\n",
    "        self.factor = factor\n",
    "        self.patience = patience\n",
    "        self.min_lr = min_lr\n",
    "        self.verbose = verbose\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.wait = 0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        current_val_loss = logs.get(\"val_loss\")\n",
    "\n",
    "        if current_val_loss is None:\n",
    "            return  # can't do anything if val_loss isn't available\n",
    "\n",
    "        if current_val_loss < self.best_val_loss - 1e-4:  # a small delta\n",
    "            self.best_val_loss = current_val_loss\n",
    "            self.wait = 0\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                old_lr = float(self.model.optimizer.learning_rate.numpy())\n",
    "                if old_lr > self.min_lr:\n",
    "                    new_lr = max(old_lr * self.factor, self.min_lr)\n",
    "                    self.model.optimizer.learning_rate.assign(new_lr)\n",
    "                    if self.verbose:\n",
    "                        print(f\"\\nEpoch {epoch+1}: val_loss did not improve. Reducing LR from {old_lr:.6f} to {new_lr:.6f}\")\n",
    "                    self.wait = 0  # reset after LR reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99dd9a0ab8e0e651",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T19:25:02.689455Z",
     "start_time": "2025-06-01T19:25:02.658944Z"
    }
   },
   "outputs": [],
   "source": [
    "class LRThresholdCallback(Callback):\n",
    "    def __init__(self, threshold=9e-5):\n",
    "        super().__init__()\n",
    "        self.threshold = threshold\n",
    "        self.should_stop = False\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        lr = float(self.model.optimizer.learning_rate.numpy())\n",
    "        if lr < self.threshold:\n",
    "            print(f\"\\nLearning rate {lr:.6f} < threshold {self.threshold}, moving to Phase 2.\")\n",
    "            self.model.stop_training = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "48e6074507a81432",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T19:25:03.146324Z",
     "start_time": "2025-06-01T19:25:03.097913Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "class GradientMonitoringCallback(Callback):\n",
    "    def __init__(self, clip_min=1e-4, clip_max=1e2, monitor_frequency=3):\n",
    "        \"\"\"\n",
    "        Monitor gradient norms during training\n",
    "        \n",
    "        Args:\n",
    "            clip_min: Minimum threshold for gradient norms\n",
    "            clip_max: Maximum threshold for gradient norms  \n",
    "            monitor_frequency: How often to check gradients (every N batches)\n",
    "        \"\"\"\n",
    "        print(f\"🔧 GradientMonitoringCallback initialized with clip_min={clip_min}, clip_max={clip_max}, monitor_freq={monitor_frequency}\")\n",
    "        self.clip_min = clip_min\n",
    "        self.clip_max = clip_max\n",
    "        self.monitor_frequency = monitor_frequency\n",
    "        self.batch_count = 0\n",
    "        self.total_calls = 0\n",
    "        self.gradient_checks = 0\n",
    "        self.fallback_calls = 0\n",
    "        \n",
    "    def on_train_begin(self, logs=None):\n",
    "        print(\"🚀 GradientMonitoringCallback: Training started!\")\n",
    "        self.batch_count = 0\n",
    "        self.total_calls = 0\n",
    "        self.gradient_checks = 0\n",
    "        self.fallback_calls = 0\n",
    "        \n",
    "    # def on_epoch_begin(self, epoch, logs=None):\n",
    "    #     print(f\"📍 GradientMonitoringCallback: Starting epoch {epoch + 1}\")\n",
    "        \n",
    "    # def on_train_batch_begin(self, batch, logs=None):\n",
    "    #     # Just to prove we're being called\n",
    "    #     if batch % 50 == 0:  # Print every 50 batches to avoid spam\n",
    "    #         print(f\"⚡ GradientMonitoringCallback: Batch {batch} starting\")\n",
    "        \n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        self.batch_count += 1\n",
    "        self.total_calls += 1\n",
    "        \n",
    "        # Print every time to show we're being called\n",
    "        # if batch % 50 == 0:  # Print every 50 batches\n",
    "            # print(f\"📊 GradientMonitoringCallback: Batch {batch} ended (total calls: {self.total_calls})\")\n",
    "        \n",
    "        # Only monitor every N batches to avoid performance overhead\n",
    "        if self.batch_count % self.monitor_frequency != 0:\n",
    "            return\n",
    "            \n",
    "        # print(f\"🔍 GradientMonitoringCallback: Checking gradients at batch {batch} (check #{self.gradient_checks + 1})\")\n",
    "        \n",
    "        # Get gradients from the optimizer's current state\n",
    "        try:\n",
    "            # Access the model's optimizer to get gradient information\n",
    "            optimizer = self.model.optimizer\n",
    "            print(f\"   📋 Optimizer type: {type(optimizer).__name__}\")\n",
    "            \n",
    "            # Get trainable variables\n",
    "            trainable_vars = self.model.trainable_variables\n",
    "            print(f\"   📈 Number of trainable variables: {len(trainable_vars)}\")\n",
    "            \n",
    "            if hasattr(optimizer, 'get_gradients'):\n",
    "                print(\"   ✅ Optimizer has get_gradients method\")\n",
    "                # For some optimizers, we can access gradients directly\n",
    "                grads = optimizer.get_gradients(self.model.total_loss, trainable_vars)\n",
    "                print(f\"   📊 Retrieved {len([g for g in grads if g is not None])} gradients\")\n",
    "            else:\n",
    "                print(\"   ❌ Optimizer doesn't have get_gradients, using variable norms\")\n",
    "                # Alternative approach: check the current variable states\n",
    "                grad_norms = []\n",
    "                for i, var in enumerate(trainable_vars):\n",
    "                    if var is not None:\n",
    "                        var_norm = tf.norm(var)\n",
    "                        grad_norms.append(var_norm)\n",
    "                        if i < 3:  # Print first 3 for debugging\n",
    "                            print(f\"      Variable {i} norm: {float(var_norm.numpy()):.2e}\")\n",
    "                \n",
    "                self._check_norms(grad_norms, \"Variable\")\n",
    "                self.gradient_checks += 1\n",
    "                return\n",
    "                \n",
    "            # Compute gradient norms\n",
    "            grad_norms = []\n",
    "            for i, grad in enumerate(grads):\n",
    "                if grad is not None:\n",
    "                    grad_norm = tf.norm(grad)\n",
    "                    grad_norms.append(grad_norm)\n",
    "                    if i < 3:  # Print first 3 for debugging\n",
    "                        print(f\"      Gradient {i} norm: {float(grad_norm.numpy()):.2e}\")\n",
    "                    \n",
    "            print(f\"   ✅ Computed {len(grad_norms)} gradient norms\")\n",
    "            self._check_norms(grad_norms, \"Gradient\")\n",
    "            self.gradient_checks += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Exception in gradient monitoring: {str(e)}\")\n",
    "            self.fallback_calls += 1\n",
    "            # Fallback: just monitor the loss for signs of instability\n",
    "            print('   🔄 Fallback: monitoring loss only')\n",
    "            if logs:\n",
    "                loss_value = logs.get('loss', 0)\n",
    "                print(f\"   📉 Current loss: {loss_value:.2e}\")\n",
    "                if np.isnan(loss_value) or np.isinf(loss_value):\n",
    "                    print(f\"   ⚠️  WARNING: Loss became {loss_value} at batch {batch}\")\n",
    "                elif loss_value > 1e6:\n",
    "                    print(f\"   ⚠️  WARNING: Very large loss {loss_value:.2e} at batch {batch}\")\n",
    "    \n",
    "    def _check_norms(self, norms, norm_type=\"Gradient\"):\n",
    "        \"\"\"Check if norms are within acceptable range\"\"\"\n",
    "        print(f\"   🔬 Checking {len(norms)} {norm_type.lower()} norms...\")\n",
    "        warnings = 0\n",
    "        \n",
    "        for idx, norm in enumerate(norms):\n",
    "            try:\n",
    "                norm_value = float(norm.numpy()) if hasattr(norm, 'numpy') else float(norm)\n",
    "                \n",
    "                if norm_value > self.clip_max:\n",
    "                    print(f\"   ⚠️  WARNING: {norm_type} norm {norm_value:.2e} is too large (layer {idx})\")\n",
    "                    warnings += 1\n",
    "                elif norm_value < self.clip_min:\n",
    "                    print(f\"   ⚠️  WARNING: {norm_type} norm {norm_value:.2e} is too small (layer {idx})\")\n",
    "                    warnings += 1\n",
    "                elif np.isnan(norm_value) or np.isinf(norm_value):\n",
    "                    print(f\"   ⚠️  WARNING: {norm_type} norm is {norm_value} (layer {idx})\")\n",
    "                    warnings += 1\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"   ❌ Cannot convert norm to float for layer {idx}: {str(e)}\")\n",
    "                continue\n",
    "                \n",
    "        if warnings == 0:\n",
    "            print(f\"   ✅ All {norm_type.lower()} norms are within acceptable range\")\n",
    "        else:\n",
    "            print(f\"   ⚠️  Found {warnings} norm warnings\")\n",
    "    \n",
    "    # def on_epoch_end(self, epoch, logs=None):\n",
    "    #     \"\"\"Print summary at end of each epoch\"\"\"\n",
    "    #     print(f\"📈 GradientMonitoringCallback: Epoch {epoch + 1} completed\")\n",
    "    #     print(f\"   📊 Total batch calls: {self.total_calls}\")\n",
    "    #     print(f\"   🔍 Gradient checks performed: {self.gradient_checks}\")\n",
    "    #     print(f\"   🔄 Fallback calls: {self.fallback_calls}\")\n",
    "    #     \n",
    "    #     if logs:\n",
    "    #         loss = logs.get('loss', 0)\n",
    "    #         val_loss = logs.get('val_loss', 0)\n",
    "    #         print(f\"   📉 Final epoch loss: {loss:.2e}\")\n",
    "    #         if val_loss:\n",
    "    #             print(f\"   📉 Final epoch val_loss: {val_loss:.2e}\")\n",
    "    #         \n",
    "    #         if np.isnan(loss) or np.isinf(loss):\n",
    "    #             print(f\"   ⚠️  WARNING: Training loss became unstable: {loss}\")\n",
    "    #         if val_loss and (np.isnan(val_loss) or np.isinf(val_loss)):\n",
    "    #             print(f\"   ⚠️  WARNING: Validation loss became unstable: {val_loss}\")\n",
    "        \n",
    "    def on_train_end(self, logs=None):\n",
    "        print(\"🏁 GradientMonitoringCallback: Training completed!\")\n",
    "        print(f\"   📊 Final stats - Total calls: {self.total_calls}, Gradient checks: {self.gradient_checks}, Fallbacks: {self.fallback_calls}\")\n",
    "        \n",
    "        if self.total_calls == 0:\n",
    "            print(\"   ❌ ERROR: Callback was never called! Check if it's properly added to callbacks list.\")\n",
    "        elif self.gradient_checks == 0 and self.fallback_calls == 0:\n",
    "            print(\"   ⚠️  WARNING: No gradient monitoring was performed. Check monitor_frequency setting.\")\n",
    "        else:\n",
    "            print(\"   ✅ Gradient monitoring completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d3c3926a64b326d",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T19:25:03.471048Z",
     "start_time": "2025-06-01T19:25:03.428738Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.src.callbacks import LearningRateScheduler, EarlyStopping, Callback\n",
    "from keras.src.optimizers import Adam\n",
    "from keras import Model\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def exponential_decay_schedule(epoch, lr):\n",
    "    decay_rate = 0.9\n",
    "    decay_steps = 5\n",
    "    if epoch % decay_steps == 0 and epoch:\n",
    "        print('Learning rate update:', lr * decay_rate)\n",
    "        return lr * decay_rate\n",
    "    return lr\n",
    "\n",
    "\n",
    "# Custom callback to monitor LR and stop training\n",
    "class LRThresholdCallback(Callback):\n",
    "    def __init__(self, threshold=9e-5):\n",
    "        super().__init__()\n",
    "        self.threshold = threshold\n",
    "        self.should_stop = False\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        lr = float(self.model.optimizer.learning_rate.numpy())\n",
    "        if lr < self.threshold:\n",
    "            print(f\"\\nLearning rate {lr:.6f} < threshold {self.threshold}, moving to next phase.\")\n",
    "            self.model.stop_training = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5af39153df21a474",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T19:25:03.767738Z",
     "start_time": "2025-06-01T19:25:03.749115Z"
    }
   },
   "outputs": [],
   "source": [
    "class DynamicReduceLROnPlateau(Callback):\n",
    "    def __init__(self, factor=0.1, patience=3, min_lr=1e-7, verbose=1):\n",
    "        super().__init__()\n",
    "        self.factor = factor\n",
    "        self.patience = patience\n",
    "        self.min_lr = min_lr\n",
    "        self.verbose = verbose\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.wait = 0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        current_val_loss = logs.get(\"val_loss\")\n",
    "\n",
    "        if current_val_loss is None:\n",
    "            return  # can't do anything if val_loss isn't available\n",
    "\n",
    "        if current_val_loss < self.best_val_loss - 1e-4:  # a small delta\n",
    "            self.best_val_loss = current_val_loss\n",
    "            self.wait = 0\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                old_lr = float(self.model.optimizer.learning_rate.numpy())\n",
    "                if old_lr > self.min_lr:\n",
    "                    new_lr = max(old_lr * self.factor, self.min_lr)\n",
    "                    self.model.optimizer.learning_rate.assign(new_lr)\n",
    "                    if self.verbose:\n",
    "                        print(f\"\\nEpoch {epoch+1}: val_loss did not improve. Reducing LR from {old_lr:.6f} to {new_lr:.6f}\")\n",
    "                    self.wait = 0  # reset after LR reduction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f08667de7129e4e0",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T19:25:04.156049Z",
     "start_time": "2025-06-01T19:25:04.138547Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.models import save_model\n",
    "\n",
    "class SaveBestModelCallback(Callback):\n",
    "    def __init__(self, save_path='best_model', monitor='val_loss'):\n",
    "        super().__init__()\n",
    "        self.best = float('inf')\n",
    "        self.monitor = monitor\n",
    "        self.save_path = save_path\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current = logs.get(self.monitor)\n",
    "        if current is not None and current < self.best:\n",
    "            self.best = current\n",
    "            print(f\"\\nNew best {self.monitor}: {current:.6f}. Saving model...\")\n",
    "            self.model.save(self.save_path+'.keras', overwrite=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ea4e240971bc7867",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T19:25:04.555788Z",
     "start_time": "2025-06-01T19:25:04.509751Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(train_data, batch_size=32, validation_split=0.2, Tobs=50, Tpred=60,\n",
    "                             epochs1=50, epochs2=50, lr1=0.001, lr2=0.000001):\n",
    "    \"\"\"Version that predicts deltas instead of absolute positions\"\"\"\n",
    "    n_scenarios = train_data.shape[0]\n",
    "    \n",
    "    X_train_scenarios = []\n",
    "    y_train_deltas = []\n",
    "    \n",
    "    pruned_ego_sparse = 0\n",
    "    \n",
    "    for i in range(n_scenarios):\n",
    "        scenario_data = train_data[i]  # (50_agents, 110_timesteps, 6_features)\n",
    "        \n",
    "        # Extract ego data\n",
    "        ego_data = scenario_data[0, :, :]\n",
    "        ego_observed = ego_data[:Tobs]\n",
    "        ego_future = ego_data[Tobs:Tobs + Tpred, :2]\n",
    "        \n",
    "        # Check ego data quality\n",
    "        ego_obs_zero_ratio = np.mean(np.all(ego_observed == 0, axis=1))\n",
    "        ego_future_zero_ratio = np.mean(np.all(ego_future == 0, axis=1))\n",
    "        \n",
    "        if ego_obs_zero_ratio > 0.3 or ego_future_zero_ratio > 0.3:\n",
    "            pruned_ego_sparse += 1\n",
    "            continue\n",
    "        \n",
    "        # Compute deltas\n",
    "        last_obs_pos = ego_observed[-1, :2]\n",
    "        delta = np.diff(np.vstack([last_obs_pos, ego_future]), axis=0)  # (60, 2)\n",
    "        \n",
    "        # Input: all agents' observed data\n",
    "        scenario_input = scenario_data[:, :Tobs, :]\n",
    "        \n",
    "        X_train_scenarios.append(scenario_input)\n",
    "        y_train_deltas.append(delta)\n",
    "    \n",
    "    print(f\"Valid scenarios: {len(X_train_scenarios)}\")\n",
    "    print(f\"Pruned due to sparse ego data: {pruned_ego_sparse}\")\n",
    "    \n",
    "    X_train = np.array(X_train_scenarios)\n",
    "    y_train = np.array(y_train_deltas)\n",
    "    \n",
    "    # --- Normalize Input and Output ---\n",
    "    X_mean = X_train.mean(axis=(0, 1), keepdims=True)  # shape: (1, 1, 6)\n",
    "    X_std = X_train.std(axis=(0, 1), keepdims=True) + 1e-8\n",
    "\n",
    "    y_mean = y_train.mean(axis=(0, 1), keepdims=True)  # shape: (1, 1, 2)\n",
    "    y_std = y_train.std(axis=(0, 1), keepdims=True) + 1e-8\n",
    "\n",
    "    X_train = (X_train - X_mean) / X_std\n",
    "    y_train = (y_train - y_mean) / y_std\n",
    "    \n",
    "    print(f\"Input shape: {X_train.shape}, Delta output shape: {y_train.shape}\")\n",
    "    \n",
    "    model = create_social_lstm_with_attention(\n",
    "    num_agents=num_agents, \n",
    "    lstm_units=128, \n",
    "    timesteps_in=Tobs,\n",
    "    timesteps_out=Tpred,\n",
    "    attention_type='simple',\n",
    "    lr=lr1\n",
    "    )\n",
    "    \n",
    "    # Callbacks\n",
    "    save_best_callback = SaveBestModelCallback(\n",
    "        save_path='attention_social_lstm',\n",
    "        monitor='val_loss'\n",
    "    )\n",
    "    \n",
    "    phase1_callbacks = [\n",
    "        DynamicReduceLROnPlateau(factor=0.7, patience=3, min_lr=1e-9),\n",
    "        EarlyStopping(patience=5, restore_best_weights=True, monitor='val_loss'),\n",
    "        LRThresholdCallback(threshold=9e-8),\n",
    "        save_best_callback\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n--- Phase 1: Training ---\")\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=epochs1,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=validation_split,\n",
    "        callbacks=phase1_callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    print(\"\\n--- Phase 2: Fine-tuning ---\")\n",
    "    model.compile(\n",
    "        optimizer=Adam(\n",
    "            learning_rate=lr2,\n",
    "            clipnorm=0.1,\n",
    "            beta_1=0.9,\n",
    "            beta_2=0.999,\n",
    "            epsilon=1e-7\n",
    "        ),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "    \n",
    "    phase2_callbacks = [\n",
    "        EarlyStopping(patience=4, restore_best_weights=True, monitor='val_loss'),\n",
    "        DynamicReduceLROnPlateau(factor=0.7, patience=3, min_lr=1e-9),\n",
    "        LRThresholdCallback(threshold=9e-8)\n",
    "    ]\n",
    "    \n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=epochs2,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=validation_split,\n",
    "        callbacks=phase2_callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return model, X_mean, X_std, y_mean, y_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1dae50c6d460015",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T19:25:04.942532Z",
     "start_time": "2025-06-01T19:25:04.928899Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_mae_by_timestep(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Visualize MAE across timesteps in the prediction horizon.\n",
    "    \n",
    "    Args:\n",
    "        y_true (np.ndarray): shape (N, Tpred, 2)\n",
    "        y_pred (np.ndarray): shape (N, Tpred, 2)\n",
    "    \"\"\"\n",
    "    mae_per_timestep = np.mean(np.abs(y_true - y_pred), axis=(0, 2))  # shape (Tpred,)\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(mae_per_timestep, label='MAE per Timestep')\n",
    "    plt.xlabel('Timestep')\n",
    "    plt.ylabel('MAE (meters)')\n",
    "    plt.title('Mean Absolute Error Over Prediction Horizon')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f303823d230afdb9",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T19:25:05.255647Z",
     "start_time": "2025-06-01T19:25:05.239807Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def reconstruct_absolute_positions(pred_deltas, last_observed_positions):\n",
    "    \"\"\"\n",
    "    Reconstruct absolute predicted positions in an autoregressive way.\n",
    "    \n",
    "    Args:\n",
    "        pred_deltas: np.ndarray of shape (N, Tpred, 2)\n",
    "        last_observed_positions: np.ndarray of shape (N, 2)\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray of shape (N, Tpred, 2)\n",
    "    \"\"\"\n",
    "    N, Tpred, _ = pred_deltas.shape\n",
    "    positions = np.zeros((N, Tpred, 2), dtype=pred_deltas.dtype)\n",
    "    positions[:, 0, :] = last_observed_positions + pred_deltas[:, 0, :]\n",
    "    \n",
    "    for t in range(1, Tpred):\n",
    "        positions[:, t, :] = positions[:, t-1, :] + pred_deltas[:, t, :]\n",
    "    \n",
    "    return positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "10132f4e6ee1e1df",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T19:25:05.600691Z",
     "start_time": "2025-06-01T19:25:05.588523Z"
    }
   },
   "outputs": [],
   "source": [
    "def forecast_positions(scenario_data, Tobs, Tpred, model, X_mean=None, X_std=None, y_mean=None, y_std=None):\n",
    "    \"\"\"\n",
    "    Use social LSTM model to forecast future deltas for ego agent using all agents' context\n",
    "    \n",
    "    Args:\n",
    "        scenario_data (numpy.ndarray): Shape (50_agents, time_steps, 6_dimensions)\n",
    "        Tobs (int): Number of observed time steps (50)\n",
    "        Tpred (int): Number of future time steps to predict (60)\n",
    "        model (Model): Trained social LSTM model\n",
    "        X_mean, X_std: Optional normalization stats for input\n",
    "        y_mean, y_std: Optional normalization stats for output\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: Predicted absolute positions of shape (Tpred, 2)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Prepare input: all agents' observed trajectories\n",
    "    # Shape: (50_agents, Tobs, 6_features)\n",
    "    all_agents_obs = scenario_data[:, :Tobs, :].copy()\n",
    "    \n",
    "    # Add batch dimension: (1, 50_agents, Tobs, 6_features)\n",
    "    X_pred = np.expand_dims(all_agents_obs, axis=0)\n",
    "    \n",
    "    # Normalize if stats are provided\n",
    "    if X_mean is not None and X_std is not None:\n",
    "        X_pred = (X_pred - X_mean) / X_std\n",
    "    \n",
    "    # Predict deltas for ego agent\n",
    "    pred_deltas = model.predict(X_pred, verbose=0)  # shape (1, Tpred, 2)\n",
    "    \n",
    "    # print(\"pred deltas shape:\", pred_deltas.shape)\n",
    "    # print(\"pred deltas sample:\", pred_deltas[0, :5, :])  # First 5 timesteps\n",
    "    \n",
    "    # Denormalize if stats are provided\n",
    "    if y_mean is not None and y_std is not None:\n",
    "        pred_deltas = pred_deltas * y_std + y_mean\n",
    "    \n",
    "    # Get ego's last observed position\n",
    "    ego_data = scenario_data[0, :Tobs, :]  # Ego is agent 0\n",
    "    last_pos = ego_data[Tobs - 1, :2]  # shape (2,)\n",
    "    \n",
    "    # Reconstruct absolute positions\n",
    "    abs_positions = reconstruct_absolute_positions(\n",
    "        pred_deltas=pred_deltas,\n",
    "        last_observed_positions=np.expand_dims(last_pos, axis=0)\n",
    "    )[0]  # Remove batch dimension\n",
    "    \n",
    "    return abs_positions\n",
    "\n",
    "def reconstruct_absolute_positions(pred_deltas, last_observed_positions):\n",
    "    \"\"\"\n",
    "    Reconstruct absolute predicted positions in an autoregressive way.\n",
    "    \n",
    "    Args:\n",
    "        pred_deltas: np.ndarray of shape (N, Tpred, 2)\n",
    "        last_observed_positions: np.ndarray of shape (N, 2)\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray of shape (N, Tpred, 2)\n",
    "    \"\"\"\n",
    "    N, Tpred, _ = pred_deltas.shape\n",
    "    positions = np.zeros((N, Tpred, 2), dtype=pred_deltas.dtype)\n",
    "    positions[:, 0, :] = last_observed_positions + pred_deltas[:, 0, :]\n",
    "    \n",
    "    for t in range(1, Tpred):\n",
    "        positions[:, t, :] = positions[:, t-1, :] + pred_deltas[:, t, :]\n",
    "    \n",
    "    return positions\n",
    "\n",
    "# Batch processing version for efficiency\n",
    "def forecast_multiple_scenarios(scenarios_data, Tobs, Tpred, model, X_mean=None, X_std=None, y_mean=None, y_std=None):\n",
    "    \"\"\"\n",
    "    Forecast positions for multiple scenarios at once\n",
    "    \n",
    "    Args:\n",
    "        scenarios_data: Shape (n_scenarios, 50_agents, time_steps, 6_features)\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: Shape (n_scenarios, Tpred, 2)\n",
    "    \"\"\"\n",
    "    n_scenarios = scenarios_data.shape[0]\n",
    "    \n",
    "    # Prepare batch input\n",
    "    X_batch = scenarios_data[:, :, :Tobs, :].copy()  # (n_scenarios, 50_agents, Tobs, 6)\n",
    "    \n",
    "    # Normalize if needed\n",
    "    if X_mean is not None and X_std is not None:\n",
    "        X_batch = (X_batch - X_mean) / X_std\n",
    "    \n",
    "    # Batch prediction\n",
    "    pred_deltas_batch = model.predict(X_batch, verbose=0)  # (n_scenarios, Tpred, 2)\n",
    "    \n",
    "    # Denormalize\n",
    "    if y_mean is not None and y_std is not None:\n",
    "        pred_deltas_batch = pred_deltas_batch * y_std + y_mean\n",
    "    \n",
    "    # Get last observed positions for all scenarios (ego agent only)\n",
    "    last_positions = scenarios_data[:, 0, Tobs-1, :2]  # (n_scenarios, 2)\n",
    "    \n",
    "    # Reconstruct positions for all scenarios\n",
    "    abs_positions_batch = reconstruct_absolute_positions(\n",
    "        pred_deltas=pred_deltas_batch,\n",
    "        last_observed_positions=last_positions\n",
    "    )\n",
    "    \n",
    "    return abs_positions_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e475f845b5d73bae",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T19:25:05.903744Z",
     "start_time": "2025-06-01T19:25:05.867705Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "def make_gif(data_matrix1, data_matrix2, name='comparison'):\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.animation as animation\n",
    "\n",
    "    cmap1 = plt.cm.get_cmap('viridis', 50)\n",
    "    cmap2 = plt.cm.get_cmap('plasma', 50)\n",
    "\n",
    "    assert data_matrix1.shape[1] == data_matrix2.shape[1], \"Both matrices must have same number of timesteps\"\n",
    "    timesteps = data_matrix1.shape[1]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(18, 9))\n",
    "    ax1, ax2 = axes\n",
    "\n",
    "    def update(frame):\n",
    "        for ax in axes:\n",
    "            ax.clear()\n",
    "\n",
    "        for i in range(data_matrix1.shape[0]):\n",
    "            for (data_matrix, ax, cmap) in [(data_matrix1, ax1, cmap1), (data_matrix2, ax2, cmap2)]:\n",
    "                x = data_matrix[i, frame, 0]\n",
    "                y = data_matrix[i, frame, 1]\n",
    "                if x != 0 and y != 0:\n",
    "                    xs = data_matrix[i, :frame+1, 0]\n",
    "                    ys = data_matrix[i, :frame+1, 1]\n",
    "                    mask = (xs != 0) & (ys != 0)\n",
    "                    xs = xs[mask]\n",
    "                    ys = ys[mask]\n",
    "                    if len(xs) > 0 and len(ys) > 0:\n",
    "                        color = cmap(i)\n",
    "                        ax.plot(xs, ys, alpha=0.9, color=color)\n",
    "                        ax.scatter(x, y, s=80, color=color)\n",
    "\n",
    "        # Plot ego vehicle (index 0) on both\n",
    "        ax1.plot(data_matrix1[0, :frame, 0], data_matrix1[0, :frame, 1], color='tab:orange', label='Ego Vehicle')\n",
    "        ax1.scatter(data_matrix1[0, frame, 0], data_matrix1[0, frame, 1], s=80, color='tab:orange')\n",
    "        ax1.set_title('Prediction')\n",
    "\n",
    "        ax2.plot(data_matrix2[0, :frame, 0], data_matrix2[0, :frame, 1], color='tab:orange', label='Ego Vehicle')\n",
    "        ax2.scatter(data_matrix2[0, frame, 0], data_matrix2[0, frame, 1], s=80, color='tab:orange')\n",
    "        ax2.set_title('Actual')\n",
    "\n",
    "        for ax, data_matrix in zip(axes, [data_matrix1, data_matrix2]):\n",
    "            ax.set_xlim(data_matrix[:, :, 0][data_matrix[:, :, 0] != 0].min() - 10,\n",
    "                        data_matrix[:, :, 0][data_matrix[:, :, 0] != 0].max() + 10)\n",
    "            ax.set_ylim(data_matrix[:, :, 1][data_matrix[:, :, 1] != 0].min() - 10,\n",
    "                        data_matrix[:, :, 1][data_matrix[:, :, 1] != 0].max() + 10)\n",
    "            ax.legend()\n",
    "            ax.set_xlabel('X')\n",
    "            ax.set_ylabel('Y')\n",
    "\n",
    "        # Compute MSE over non-zero entries up to current frame\n",
    "        mask = (data_matrix2[:, :frame+1, :] != 0) & (data_matrix1[:, :frame+1, :] != 0)\n",
    "        mse = np.mean((data_matrix1[:, :frame+1, :][mask] - data_matrix2[:, :frame+1, :][mask]) ** 2)\n",
    "\n",
    "        fig.suptitle(f\"Timestep {frame} - MSE: {mse:.4f}\", fontsize=16)\n",
    "        return ax1.collections + ax1.lines + ax2.collections + ax2.lines\n",
    "\n",
    "    anim = animation.FuncAnimation(fig, update, frames=list(range(0, timesteps, 3)), interval=100, blit=True)\n",
    "    anim.save(f'trajectory_visualization_{name}.gif', writer='pillow')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ac3b5ab02eaf908a",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T19:25:06.331686Z",
     "start_time": "2025-06-01T19:25:06.316086Z"
    }
   },
   "outputs": [],
   "source": [
    "# # figure out stats\n",
    "# def stats():\n",
    "#     Tobs = 50\n",
    "#     Tpred = 60\n",
    "#     n_scenarios = train_data.shape[0]\n",
    "#     X_train_raw = []\n",
    "#     y_train_deltas = []\n",
    "# \n",
    "#     for i in range(n_scenarios):\n",
    "#         ego_data = train_data[i, 0, :, :]\n",
    "#         if np.all(ego_data == 0):\n",
    "#             continue\n",
    "# \n",
    "#         observed = ego_data[:Tobs]            # shape (50, 6)\n",
    "#         future = ego_data[Tobs:Tobs+Tpred, :2]\n",
    "#         last_obs_pos = observed[-1, :2]\n",
    "# \n",
    "#         if np.any(np.all(observed == 0, axis=1)) or np.any(np.all(future == 0, axis=1)):\n",
    "#             continue\n",
    "# \n",
    "#         # Compute deltas w.r.t. previous future timestep\n",
    "#         delta = np.diff(np.vstack([last_obs_pos, future]), axis=0)  # (60, 2)\n",
    "# \n",
    "#         X_train_raw.append(observed)\n",
    "#         y_train_deltas.append(delta)\n",
    "# \n",
    "# \n",
    "#     X_train = np.array(X_train_raw)\n",
    "#     y_train = np.array(y_train_deltas)\n",
    "# \n",
    "#     print(f\"{X_train.shape[0]} valid sequences.\")\n",
    "#     print(f\"Input shape: {X_train.shape}, Delta Output shape: {y_train.shape}\")\n",
    "# \n",
    "#     # --- Normalize Input and Output ---\n",
    "#     X_mean = X_train.mean(axis=(0, 1), keepdims=True)  # shape: (1, 1, 6)\n",
    "#     X_std = X_train.std(axis=(0, 1), keepdims=True) + 1e-8\n",
    "# \n",
    "#     y_mean = y_train.mean(axis=(0, 1), keepdims=True)  # shape: (1, 1, 2)\n",
    "#     y_std = y_train.std(axis=(0, 1), keepdims=True) + 1e-8\n",
    "# \n",
    "#     X_train = (X_train - X_mean) / X_std\n",
    "#     y_train = (y_train - y_mean) / y_std\n",
    "#     return X_mean, X_std, y_mean, y_std\n",
    "# X_mean, X_std, y_mean, y_std = stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1dfecba2866a2d9d",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-01T19:25:07.386208Z",
     "start_time": "2025-06-01T19:25:07.382399Z"
    }
   },
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d257d5cc4ad709",
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2025-06-01T19:25:22.653295Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid scenarios: 30000\n",
      "Pruned due to sparse ego data: 0\n",
      "Input shape: (30000, 15, 50, 6), Delta output shape: (30000, 60, 2)\n",
      "\n",
      "--- Phase 1: Training ---\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 12:25:36.991171: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m750/750\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 380ms/step - loss: 0.2572 - mae: 0.3328\n",
      "New best val_loss: 0.136776. Saving model...\n",
      "\u001B[1m750/750\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m316s\u001B[0m 401ms/step - loss: 0.2571 - mae: 0.3328 - val_loss: 0.1368 - val_mae: 0.2420\n",
      "Epoch 2/200\n",
      "\u001B[1m750/750\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m234s\u001B[0m 311ms/step - loss: 0.1382 - mae: 0.2461 - val_loss: 0.2449 - val_mae: 0.3748\n",
      "Epoch 3/200\n",
      "\u001B[1m750/750\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m228s\u001B[0m 304ms/step - loss: 0.2101 - mae: 0.3278 - val_loss: 0.1610 - val_mae: 0.2726\n",
      "Epoch 4/200\n",
      "\u001B[1m750/750\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 302ms/step - loss: 0.1401 - mae: 0.2532\n",
      "Epoch 4: val_loss did not improve. Reducing LR from 0.000300 to 0.000210\n",
      "\u001B[1m750/750\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m238s\u001B[0m 317ms/step - loss: 0.1401 - mae: 0.2533 - val_loss: 0.1555 - val_mae: 0.2715\n",
      "Epoch 5/200\n",
      "\u001B[1m750/750\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m257s\u001B[0m 342ms/step - loss: 0.2250 - mae: 0.3316 - val_loss: 0.2892 - val_mae: 0.3637\n",
      "Epoch 6/200\n",
      "\u001B[1m750/750\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m347s\u001B[0m 462ms/step - loss: 0.4287 - mae: 0.4734 - val_loss: 0.1976 - val_mae: 0.3178\n",
      "\n",
      "--- Phase 2: Fine-tuning ---\n",
      "Epoch 1/100\n",
      "\u001B[1m 16/750\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m6:49\u001B[0m 558ms/step - loss: 0.1375 - mae: 0.2391"
     ]
    }
   ],
   "source": [
    "# fit\n",
    "model, X_mean, X_std, y_mean, y_std = train_model(standardized_train_data[:],epochs1=200, epochs2=100, validation_split=0.2, lr1=0.0003, lr2=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7209bcaafda7611",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-06-01T19:24:39.749538Z"
    }
   },
   "outputs": [],
   "source": [
    "model = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ac55f8c53f2860",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-06-01T19:24:39.750082Z"
    }
   },
   "outputs": [],
   "source": [
    "# visualize regular prediction\n",
    "\n",
    "# X_mean, X_std, y_mean, y_std = None, None, None, None\n",
    "\n",
    "# model = load_model()\n",
    "\n",
    "# Parameters\n",
    "Tobs = 50\n",
    "Tpred = 60\n",
    "\n",
    "data = standardized_train_data[6325]\n",
    "\n",
    "# Select a test scenario (can use any valid index)\n",
    "test_scenario = data.copy()  # shape (agents, time_steps, features)\n",
    "\n",
    "\n",
    "# Forecast future positions\n",
    "predicted_positions = forecast_positions(test_scenario, Tobs, Tpred, model, X_mean, X_std, y_mean, y_std)\n",
    "\n",
    "# Create combined matrix of past observed + predicted for ego agent (agent 0)\n",
    "ego_past = test_scenario[0, :Tobs, :2]               # shape (Tobs, 2)\n",
    "ego_future = predicted_positions                     # shape (Tpred, 2)\n",
    "\n",
    "print(ego_future[:5])\n",
    "print(test_scenario[0, Tobs:Tobs+5, :2])\n",
    "ego_full = np.concatenate([ego_past, ego_future], axis=0)  # shape (Tobs + Tpred, 2)\n",
    "\n",
    "# Create updated scenario with predicted ego and original others\n",
    "updated_scenario = test_scenario.copy()\n",
    "updated_scenario[0, :Tobs+Tpred, :2] = ego_full  # Replace ego trajectory\n",
    "\n",
    "# Visualize\n",
    "make_gif(updated_scenario, data, name='lstmsocial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd67ae1834bd217",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-06-01T19:24:39.750843Z"
    }
   },
   "outputs": [],
   "source": [
    "# # visualize prediction\n",
    "# \n",
    "# # model = load_model()\n",
    "# \n",
    "# # Parameters\n",
    "# Tobs = 50\n",
    "# Tpred = 60\n",
    "# \n",
    "# data = train_data[0]\n",
    "# \n",
    "# # Select a test scenario (can use any valid index)\n",
    "# test_scenario = data.copy()  # shape (agents, time_steps, features)\n",
    "# \n",
    "# \n",
    "# # Forecast future positions\n",
    "# predicted_positions = finetune_forecast_positions(test_scenario, Tobs, Tpred, model, X_mean, X_std, y_mean, y_std)\n",
    "# \n",
    "# # Create combined matrix of past observed + predicted for ego agent (agent 0)\n",
    "# ego_past = test_scenario[0, :Tobs, :2]               # shape (Tobs, 2)\n",
    "# ego_future = predicted_positions[0]                  # shape (Tpred, 2)\n",
    "# ego_full = np.concatenate([ego_past, ego_future], axis=0)  # shape (Tobs + Tpred, 2)\n",
    "# \n",
    "# # Create updated scenario with predicted ego and original others\n",
    "# updated_scenario = test_scenario.copy()\n",
    "# updated_scenario[0, :Tobs+Tpred, :2] = ego_full  # Replace ego trajectory\n",
    "# \n",
    "# # Visualize\n",
    "# make_gif(updated_scenario, data, name='lstm2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f6401b403c51b",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-06-01T19:24:39.751069Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def evaluate_mse(train_data, model, Tobs=50, Tpred=60, X_mean=None, X_std=None, y_mean=None, y_std=None):\n",
    "    \"\"\"\n",
    "    Computes LSTM prediction for ego agent and evaluates MSE with progress reporting.\n",
    "    \"\"\"\n",
    "    N = train_data.shape[0]\n",
    "    mse_list = []\n",
    "    valid_scenarios = 0\n",
    "    \n",
    "    print(f\"Evaluating {N} scenarios...\")\n",
    "    \n",
    "    # Progress reporting variables\n",
    "    report_interval = max(1, N // 10)  # Report at 10% intervals\n",
    "    \n",
    "    for i in range(N):\n",
    "        # Progress reporting\n",
    "        if i % report_interval == 0 or i == N-1:\n",
    "            print(f\"Processing scenario {i+1}/{N} ({(i+1)/N*100:.1f}%)\")\n",
    "        \n",
    "        scenario_data = train_data[i]  # Shape: (50_agents, time_steps, 6)\n",
    "        ego_agent_data = scenario_data[0]  # Shape: (time_steps, 6)\n",
    "        ground_truth = ego_agent_data[Tobs:Tobs+Tpred, :2]\n",
    "        \n",
    "        # Skip if ground truth contains all zeros (padded)\n",
    "        if np.all(ground_truth == 0):\n",
    "            continue\n",
    "            \n",
    "        valid_scenarios += 1\n",
    "        \n",
    "        # Forecast future positions - pass full scenario_data, not just ego\n",
    "        predicted_positions = forecast_positions(\n",
    "            scenario_data,  # Pass full scenario (50_agents, time_steps, 6)\n",
    "            Tobs, Tpred, model, X_mean, X_std, y_mean, y_std\n",
    "        )\n",
    "        \n",
    "        # Compute MSE\n",
    "        mse = mean_squared_error(ground_truth, predicted_positions)\n",
    "        mse_list.append(mse)\n",
    "        \n",
    "        # Occasional MSE reporting\n",
    "        if i % report_interval == 0:\n",
    "            print(f\"  Current scenario MSE: {mse:.4f}\")\n",
    "    \n",
    "    # Final results\n",
    "    if mse_list:\n",
    "        overall_mse = np.mean(mse_list)\n",
    "        print(f\"Evaluation complete: {valid_scenarios} valid scenarios\")\n",
    "        print(f\"Mean Squared Error (MSE): {overall_mse:.4f}\")\n",
    "        print(f\"Min MSE: {np.min(mse_list):.4f}, Max MSE: {np.max(mse_list):.4f}\")\n",
    "        return overall_mse\n",
    "    else:\n",
    "        print(\"No valid scenarios for evaluation.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75caee9a06e8acb",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-06-01T19:24:39.751576Z"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate on training data\n",
    "evaluate_mse(standardized_train_data[:300], model, Tobs, Tpred, X_mean, X_std, y_mean, y_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907c41122d6ed7b9",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-06-01T19:24:39.752474Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def generate_social_lstm_submission(data, model, output_csv, Tobs=50, Tpred=60, \n",
    "                                  X_mean=None, X_std=None, y_mean=None, y_std=None):\n",
    "    \"\"\"\n",
    "    Applies social LSTM forecasting and generates a submission CSV.\n",
    "    \n",
    "    Args:\n",
    "        data (np.ndarray): Test data of shape (num_scenarios, 50, time_steps, 6).\n",
    "        model: Trained social LSTM model.\n",
    "        output_csv (str): Output CSV file path.\n",
    "        Tobs (int): Observed time steps (default 50).\n",
    "        Tpred (int): Prediction time steps (default 60).\n",
    "        X_mean, X_std, y_mean, y_std: Normalization parameters from training\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    print(f\"Generating predictions for {data.shape[0]} scenarios...\")\n",
    "    \n",
    "    for i in range(data.shape[0]):\n",
    "        if i % max(1, data.shape[0] // 10) == 0:\n",
    "            print(f\"Processing scenario {i+1}/{data.shape[0]}\")\n",
    "            \n",
    "        scenario_data = data[i]  # Shape: (50, time_steps, 6)\n",
    "        \n",
    "        # 1. Standardize the scenario data (min-centering)\n",
    "        standardized_data, min_values = standardize_data_dimensions(scenario_data)\n",
    "        \n",
    "        # 2. Use forecast_positions with standardized data\n",
    "        predicted_positions = forecast_positions(\n",
    "            standardized_data, Tobs, Tpred, model, \n",
    "            X_mean, X_std, y_mean, y_std\n",
    "        )  # Shape: (Tpred, 2)\n",
    "        \n",
    "        # 3. Denormalize predictions (add back min values)\n",
    "        denormalized_predictions = denormalize_predictions_2d(predicted_positions, min_values)\n",
    "        \n",
    "        # 4. Append predictions for this scenario\n",
    "        predictions.extend(denormalized_predictions)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    submission_df = pd.DataFrame(predictions, columns=[\"x\", \"y\"])\n",
    "    submission_df.index.name = 'index'\n",
    "    \n",
    "    # Save CSV with index\n",
    "    submission_df.to_csv(output_csv)\n",
    "    print(f\"Submission file '{output_csv}' saved with shape {submission_df.shape}\")\n",
    "\n",
    "def denormalize_predictions_2d(predictions, min_values):\n",
    "    \"\"\"\n",
    "    Helper function to add back the min values to predicted positions (2D version).\n",
    "    \n",
    "    :param predictions: predicted data with standardized positions, shape (Tpred, 2)\n",
    "    :param min_values: array of shape (2,) containing [min_x, min_y] for this scenario\n",
    "    :returns: predictions with original coordinate system restored\n",
    "    \"\"\"\n",
    "    denormalized = predictions.copy()\n",
    "    \n",
    "    # Add back the min values to restore original coordinate system\n",
    "    denormalized[:, 0] += min_values[0]  # position_x\n",
    "    denormalized[:, 1] += min_values[1]  # position_y\n",
    "    \n",
    "    return denormalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8955d24a-3bed-429c-9456-010d79f5bdbf",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-06-01T19:24:39.752846Z"
    }
   },
   "outputs": [],
   "source": [
    "generate_social_lstm_submission(test_data, model, 'attention_social_lstm.csv',\n",
    "                               X_mean=X_mean, X_std=X_std, y_mean=y_mean, y_std=y_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16b2b0b-0e89-485c-bcce-086e85be4e04",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-06-01T19:24:39.753341Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
