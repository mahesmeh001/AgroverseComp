{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dmTA3b9bzjw",
   "metadata": {
    "id": "9dmTA3b9bzjw"
   },
   "outputs": [],
   "source": [
    "!mkdir -p ~/.kaggle\n",
    "!mv kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rnCuK3inb1cY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rnCuK3inb1cY",
    "outputId": "de93a29d-1b72-4f10-996e-c80b8bd85288"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading cse-251-b-2025.zip to /content\n",
      " 97% 970M/0.98G [00:09<00:00, 195MB/s]\n",
      "100% 0.98G/0.98G [00:09<00:00, 111MB/s]\n"
     ]
    }
   ],
   "source": [
    "!kaggle competitions download -c cse-251-b-2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hSWeXkb2b3fn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hSWeXkb2b3fn",
    "outputId": "cc1acf4f-32a2-480d-d44b-782c71f37bdc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  cse-251-b-2025.zip\n",
      "  inflating: data/test_input.npz     \n",
      "  inflating: data/train.npz          \n"
     ]
    }
   ],
   "source": [
    "!unzip cse-251-b-2025.zip -d data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T17:40:36.649125Z",
     "start_time": "2025-05-02T17:40:35.182094Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "initial_id",
    "outputId": "33e853f4-3b7e-4943-adaa-f1c1933befcc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data's shape (10000, 50, 110, 6)\n",
      "test_data's shape (2100, 50, 50, 6)\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "import numpy as np\n",
    "train_file = np.load('argoverse_data/train.npz')\n",
    "train_data = train_file['data']\n",
    "print(\"train_data's shape\", train_data.shape)\n",
    "test_file = np.load('argoverse_data/test_input.npz')\n",
    "test_data = test_file['data']\n",
    "print(\"test_data's shape\", test_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "856f0da9-135e-4e71-8e71-89aaf29f01b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List, Optional\n",
    "import random\n",
    "\n",
    "class ArgoverseAugmentor:\n",
    "    \"\"\"\n",
    "    Data augmentation class for Argoverse trajectory data.\n",
    "    Supports rotations, translations, and horizontal/vertical flips.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, seed: Optional[int] = None):\n",
    "        \"\"\"\n",
    "        Initialize the augmentor.\n",
    "        \n",
    "        Args:\n",
    "            seed: Random seed for reproducible augmentations\n",
    "        \"\"\"\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "            random.seed(seed)\n",
    "    \n",
    "    def rotate_trajectory(self, trajectory: np.ndarray, angle: float) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Rotate trajectory by given angle around the origin.\n",
    "        Assumes first 2 dimensions are (x, y) coordinates.\n",
    "        \n",
    "        Args:\n",
    "            trajectory: Shape (N, F) array where F >= 2, first 2 cols are (x, y)\n",
    "            angle: Rotation angle in radians\n",
    "            \n",
    "        Returns:\n",
    "            Rotated trajectory of same shape\n",
    "        \"\"\"\n",
    "        if trajectory.shape[-1] < 2:\n",
    "            raise ValueError(\"Trajectory must have at least 2 features (x, y)\")\n",
    "            \n",
    "        cos_a, sin_a = np.cos(angle), np.sin(angle)\n",
    "        rotation_matrix = np.array([[cos_a, -sin_a],\n",
    "                                   [sin_a, cos_a]])\n",
    "        \n",
    "        # Only rotate x, y coordinates (first 2 columns)\n",
    "        rotated_trajectory = trajectory.copy()\n",
    "        xy_coords = trajectory[:, :2]  # Extract x, y coordinates\n",
    "        rotated_xy = xy_coords @ rotation_matrix.T\n",
    "        rotated_trajectory[:, :2] = rotated_xy  # Update only x, y\n",
    "        \n",
    "        return rotated_trajectory\n",
    "    \n",
    "    def translate_trajectory(self, trajectory: np.ndarray, \n",
    "                           dx: float, dy: float) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Translate trajectory by given offsets.\n",
    "        Only translates x, y coordinates (first 2 features).\n",
    "        \n",
    "        Args:\n",
    "            trajectory: Shape (N, F) array where F >= 2, first 2 cols are (x, y)\n",
    "            dx: Translation in x direction\n",
    "            dy: Translation in y direction\n",
    "            \n",
    "        Returns:\n",
    "            Translated trajectory of same shape\n",
    "        \"\"\"\n",
    "        if trajectory.shape[-1] < 2:\n",
    "            raise ValueError(\"Trajectory must have at least 2 features (x, y)\")\n",
    "            \n",
    "        translated_trajectory = trajectory.copy()\n",
    "        translated_trajectory[:, 0] += dx  # Translate x\n",
    "        translated_trajectory[:, 1] += dy  # Translate y\n",
    "        \n",
    "        return translated_trajectory\n",
    "    \n",
    "    def flip_horizontal(self, trajectory: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Flip trajectory horizontally (mirror across y-axis).\n",
    "        Only affects x coordinate (first feature).\n",
    "        \n",
    "        Args:\n",
    "            trajectory: Shape (N, F) array where F >= 2, first 2 cols are (x, y)\n",
    "            \n",
    "        Returns:\n",
    "            Horizontally flipped trajectory\n",
    "        \"\"\"\n",
    "        if trajectory.shape[-1] < 2:\n",
    "            raise ValueError(\"Trajectory must have at least 2 features (x, y)\")\n",
    "            \n",
    "        flipped = trajectory.copy()\n",
    "        flipped[:, 0] = -flipped[:, 0]  # Negate x coordinates\n",
    "        return flipped\n",
    "    \n",
    "    def flip_vertical(self, trajectory: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Flip trajectory vertically (mirror across x-axis).\n",
    "        Only affects y coordinate (second feature).\n",
    "        \n",
    "        Args:\n",
    "            trajectory: Shape (N, F) array where F >= 2, first 2 cols are (x, y)\n",
    "            \n",
    "        Returns:\n",
    "            Vertically flipped trajectory\n",
    "        \"\"\"\n",
    "        if trajectory.shape[-1] < 2:\n",
    "            raise ValueError(\"Trajectory must have at least 2 features (x, y)\")\n",
    "            \n",
    "        flipped = trajectory.copy()\n",
    "        flipped[:, 1] = -flipped[:, 1]  # Negate y coordinates\n",
    "        return flipped\n",
    "    \n",
    "    def augment_single_sample(self, sample: np.ndarray, \n",
    "                            rotation_range: float = np.pi/4,\n",
    "                            translation_range: float = 5.0,\n",
    "                            flip_prob: float = 0.5) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Apply random augmentations to a single sample.\n",
    "        \n",
    "        Args:\n",
    "            sample: Trajectory data - shape depends on format:\n",
    "                   - (seq_len, features) for single agent\n",
    "                   - (seq_len, num_agents, features) for multi-agent\n",
    "            rotation_range: Maximum rotation angle in radians\n",
    "            translation_range: Maximum translation distance\n",
    "            flip_prob: Probability of applying flips\n",
    "            \n",
    "        Returns:\n",
    "            Augmented sample of same shape\n",
    "        \"\"\"\n",
    "        augmented = sample.copy()\n",
    "        \n",
    "        # Random rotation\n",
    "        angle = np.random.uniform(-rotation_range, rotation_range)\n",
    "        \n",
    "        # Random translation\n",
    "        dx = np.random.uniform(-translation_range, translation_range)\n",
    "        dy = np.random.uniform(-translation_range, translation_range)\n",
    "        \n",
    "        # Handle different data formats\n",
    "        if len(sample.shape) == 2:\n",
    "            # Single agent: (seq_len, features)\n",
    "            trajectory = sample\n",
    "            if not np.all(trajectory == 0):  # Skip if all zeros\n",
    "                trajectory = self.rotate_trajectory(trajectory, angle)\n",
    "                trajectory = self.translate_trajectory(trajectory, dx, dy)\n",
    "                \n",
    "                if np.random.random() < flip_prob:\n",
    "                    trajectory = self.flip_horizontal(trajectory)\n",
    "                \n",
    "                if np.random.random() < flip_prob:\n",
    "                    trajectory = self.flip_vertical(trajectory)\n",
    "                    \n",
    "                augmented = trajectory\n",
    "                \n",
    "        elif len(sample.shape) == 3:\n",
    "            # Multi-agent: (seq_len, num_agents, features)\n",
    "            for agent_idx in range(sample.shape[1]):\n",
    "                trajectory = sample[:, agent_idx, :]\n",
    "                \n",
    "                # Skip if trajectory is all zeros (padding)\n",
    "                if np.all(trajectory == 0):\n",
    "                    continue\n",
    "                    \n",
    "                # Apply rotation and translation\n",
    "                trajectory = self.rotate_trajectory(trajectory, angle)\n",
    "                trajectory = self.translate_trajectory(trajectory, dx, dy)\n",
    "                \n",
    "                # Apply flips with probability\n",
    "                if np.random.random() < flip_prob:\n",
    "                    trajectory = self.flip_horizontal(trajectory)\n",
    "                \n",
    "                if np.random.random() < flip_prob:\n",
    "                    trajectory = self.flip_vertical(trajectory)\n",
    "                    \n",
    "                augmented[:, agent_idx, :] = trajectory\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported sample shape: {sample.shape}\")\n",
    "        \n",
    "        return augmented\n",
    "    \n",
    "    def augment_dataset(self, data: np.ndarray, \n",
    "                       num_augmentations: int = 1,\n",
    "                       rotation_range: float = np.pi/4,\n",
    "                       translation_range: float = 5.0,\n",
    "                       flip_prob: float = 0.5,\n",
    "                       return_original: bool = True) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Augment entire dataset.\n",
    "        \n",
    "        Args:\n",
    "            data: Shape (N, seq_len, num_agents, 2) dataset\n",
    "            num_augmentations: Number of augmented versions per sample\n",
    "            rotation_range: Maximum rotation angle in radians\n",
    "            translation_range: Maximum translation distance\n",
    "            flip_prob: Probability of applying flips\n",
    "            return_original: Whether to include original data in output\n",
    "            \n",
    "        Returns:\n",
    "            Augmented dataset with shape (N*(1+num_augmentations), seq_len, num_agents, 2)\n",
    "        \"\"\"\n",
    "        original_size = data.shape[0]\n",
    "        augmented_samples = []\n",
    "        \n",
    "        # Add original data if requested\n",
    "        if return_original:\n",
    "            augmented_samples.append(data)\n",
    "        \n",
    "        # Generate augmentations\n",
    "        for aug_idx in range(num_augmentations):\n",
    "            print(f\"Generating augmentation {aug_idx + 1}/{num_augmentations}...\")\n",
    "            \n",
    "            augmented_batch = np.zeros_like(data)\n",
    "            for i in range(original_size):\n",
    "                augmented_batch[i] = self.augment_single_sample(\n",
    "                    data[i], rotation_range, translation_range, flip_prob\n",
    "                )\n",
    "            \n",
    "            augmented_samples.append(augmented_batch)\n",
    "        \n",
    "        return np.concatenate(augmented_samples, axis=0)\n",
    "    \n",
    "    def visualize_augmentations(self, original_sample: np.ndarray, \n",
    "                              num_examples: int = 4,\n",
    "                              agent_idx: int = 0) -> None:\n",
    "        \"\"\"\n",
    "        Visualize original and augmented trajectories for comparison.\n",
    "        \n",
    "        Args:\n",
    "            original_sample: Single sample to augment and visualize\n",
    "            num_examples: Number of augmented examples to show\n",
    "            agent_idx: Which agent's trajectory to visualize (for multi-agent data)\n",
    "        \"\"\"\n",
    "        fig, axes = plt.subplots(1, num_examples + 1, figsize=(15, 3))\n",
    "        \n",
    "        # Extract trajectory based on data format\n",
    "        if len(original_sample.shape) == 2:\n",
    "            # Single agent data\n",
    "            orig_traj = original_sample[:, :2]  # Use only x, y coordinates\n",
    "        elif len(original_sample.shape) == 3:\n",
    "            # Multi-agent data\n",
    "            orig_traj = original_sample[:, agent_idx, :2]  # Use only x, y coordinates\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported sample shape: {original_sample.shape}\")\n",
    "        \n",
    "        # Plot original\n",
    "        axes[0].plot(orig_traj[:, 0], orig_traj[:, 1], 'b-o', markersize=3)\n",
    "        axes[0].set_title('Original')\n",
    "        axes[0].grid(True)\n",
    "        axes[0].axis('equal')\n",
    "        \n",
    "        # Plot augmented versions\n",
    "        for i in range(num_examples):\n",
    "            aug_sample = self.augment_single_sample(original_sample)\n",
    "            \n",
    "            # Extract augmented trajectory\n",
    "            if len(aug_sample.shape) == 2:\n",
    "                aug_traj = aug_sample[:, :2]\n",
    "            else:\n",
    "                aug_traj = aug_sample[:, agent_idx, :2]\n",
    "            \n",
    "            axes[i + 1].plot(aug_traj[:, 0], aug_traj[:, 1], 'r-o', markersize=3)\n",
    "            axes[i + 1].set_title(f'Augmented {i + 1}')\n",
    "            axes[i + 1].grid(True)\n",
    "            axes[i + 1].axis('equal')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6db64d59-a961-453a-9cec-10c92428327f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Augmenting training data...\n",
      "Augmented train_data shape: (10000, 50, 110, 6)\n",
      "Data increased by factor of: 1.0\n"
     ]
    }
   ],
   "source": [
    "augmentor = ArgoverseAugmentor(seed=42)\n",
    "\n",
    "# Example 1: Augment training data with 2 additional versions per sample\n",
    "print(\"\\nAugmenting training data...\")\n",
    "augmented_train = augmentor.augment_dataset(\n",
    "    train_data,\n",
    "    num_augmentations=0,  # Create 2 augmented versions per sample\n",
    "    rotation_range=np.pi/6,  # ±30 degrees\n",
    "    translation_range=3.0,   # ±3 units\n",
    "    flip_prob=0.3,           # 30% chance of flips\n",
    "    return_original=True     # Include original data\n",
    ")\n",
    "\n",
    "print(f\"Augmented train_data shape: {augmented_train.shape}\")\n",
    "print(f\"Data increased by factor of: {augmented_train.shape[0] / train_data.shape[0]:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5829b18a-2aa8-47a7-9f2a-537f0ccc5c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = augmented_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81a062b78c6346e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T17:40:37.767406Z",
     "start_time": "2025-05-02T17:40:36.640269Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "81a062b78c6346e7",
    "outputId": "0b226b5d-6080-4946-b7fe-548df8f449d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Train Data Shape: (10000, 50, 110, 6)\n",
      "Processed Train Data Length: 10000 (variable agents per scenario)\n",
      "Original Test Data Shape: (2100, 50, 50, 6)\n",
      "Processed Test Data Length: 2100 (variable agents per scenario)\n"
     ]
    }
   ],
   "source": [
    "# preprocess\n",
    "def preprocess_data(data):\n",
    "    \"\"\"\n",
    "    Removes padded agents (agents with all zero values across time steps).\n",
    "\n",
    "    Args:\n",
    "        data (numpy.ndarray): Shape (scenarios, agents, time_steps, dimensions)\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Filtered dataset without padded agents.\n",
    "    \"\"\"\n",
    "    scenarios, agents, time_steps, dimensions = data.shape\n",
    "    processed_data = []\n",
    "\n",
    "    for i in range(scenarios):\n",
    "        scenario_data = data[i]  # Shape (agents, time_steps, dimensions)\n",
    "\n",
    "        # Identify non-padded agents (at least one nonzero value across all time steps)\n",
    "        valid_agents = np.any(scenario_data != 0, axis=(1, 2))  # Shape (agents,)\n",
    "\n",
    "        # Filter out only the valid agents\n",
    "        filtered_agents = scenario_data[valid_agents]  # Shape (valid_agents, time_steps, dimensions)\n",
    "\n",
    "        processed_data.append(filtered_agents)\n",
    "\n",
    "    return processed_data  # List of variable-length arrays per scenario\n",
    "\n",
    "train_data_processed = preprocess_data(train_data)\n",
    "test_data_processed = preprocess_data(test_data)\n",
    "\n",
    "# Print results\n",
    "print(f\"Original Train Data Shape: {train_data.shape}\")\n",
    "print(f\"Processed Train Data Length: {len(train_data_processed)} (variable agents per scenario)\")\n",
    "\n",
    "print(f\"Original Test Data Shape: {test_data.shape}\")\n",
    "print(f\"Processed Test Data Length: {len(test_data_processed)} (variable agents per scenario)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5df04a84c63e60f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T17:40:37.770858Z",
     "start_time": "2025-05-02T17:40:37.768140Z"
    },
    "id": "c5df04a84c63e60f"
   },
   "outputs": [],
   "source": [
    "# some kind of missing trajectory handling?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db97d754762b390b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T17:40:42.779341Z",
     "start_time": "2025-05-02T17:40:37.771284Z"
    },
    "id": "db97d754762b390b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-30 00:53:46.015764: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-05-30 00:53:46.015829: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-05-30 00:53:46.017484: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-30 00:53:46.027180: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input, RepeatVector, TimeDistributed, Dropout, Bidirectional, LayerNormalization, Flatten, Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a43f0094b638b0ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T17:40:42.786049Z",
     "start_time": "2025-05-02T17:40:42.783787Z"
    },
    "id": "a43f0094b638b0ed"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def angle_change_loss(y_pred):\n",
    "    \"\"\"\n",
    "    Penalize large changes in direction between consecutive deltas.\n",
    "\n",
    "    Args:\n",
    "        y_pred: Tensor of shape (batch_size, Tpred, 2)\n",
    "\n",
    "    Returns:\n",
    "        Scalar loss penalizing angle differences\n",
    "    \"\"\"\n",
    "    # Normalize deltas to unit vectors\n",
    "    delta_unit = tf.math.l2_normalize(y_pred, axis=-1)  # shape: (B, T, 2)\n",
    "\n",
    "    # Compute cosine similarity between consecutive deltas\n",
    "    dot_products = tf.reduce_sum(delta_unit[:, 1:, :] * delta_unit[:, :-1, :], axis=-1)  # shape: (B, T-1)\n",
    "\n",
    "    # Clamp for numerical stability (to avoid NaNs in arccos)\n",
    "    dot_products = tf.clip_by_value(dot_products, -1.0, 1.0)\n",
    "\n",
    "    # Compute angle in radians between -1 and 1 (cos⁻¹)\n",
    "    angle_diff = tf.acos(dot_products)  # shape: (B, T-1)\n",
    "\n",
    "    # Mean angle difference per sequence\n",
    "    return tf.reduce_mean(angle_diff)\n",
    "\n",
    "def combined_loss(y_true, y_pred):\n",
    "    # Check if there are NaN values in y_true or y_pred\n",
    "    nan_check = tf.reduce_any(tf.math.is_nan(y_true)) | tf.reduce_any(tf.math.is_nan(y_pred))\n",
    "\n",
    "    # Use tf.cond to perform the check\n",
    "    def return_nan_loss():\n",
    "        return tf.constant(float('nan'))\n",
    "\n",
    "    def calculate_loss():\n",
    "        mse_loss = tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "        angle_loss = angle_change_loss(y_pred)\n",
    "        return mse_loss + 0.5 * angle_loss\n",
    "\n",
    "    return tf.cond(nan_check, return_nan_loss, calculate_loss)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7f20fc814e583a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T18:15:51.012605Z",
     "start_time": "2025-05-02T18:15:51.003936Z"
    },
    "id": "d7f20fc814e583a3"
   },
   "outputs": [],
   "source": [
    "def create_lstm_encoder_decoder(input_dim, output_dim, timesteps_in, timesteps_out,\n",
    "                                max_stationary=30, lstm_units=512, num_layers=3, loss_fn='mse', lr=0.001):\n",
    "    # Two inputs\n",
    "    trajectory_input = Input(shape=(timesteps_in, input_dim))\n",
    "    context_input = Input(shape=(max_stationary, 2))\n",
    "    \n",
    "    # Encoder for trajectory\n",
    "    x = trajectory_input\n",
    "    for _ in range(num_layers):\n",
    "        x = LSTM(lstm_units, return_sequences=True)(x)\n",
    "        #x = LayerNormalization()(x)\n",
    "        # x = Dropout(0.2)(x)\n",
    "    encoded = LSTM(lstm_units)(x)  # Final encoder output\n",
    "    \n",
    "    # Process stationary context\n",
    "    context_processed = Flatten()(context_input)\n",
    "    context_processed = Dense(64, activation='relu')(context_processed)\n",
    "    context_processed = Dense(32, activation='relu')(context_processed)\n",
    "    \n",
    "    # Combine encoded trajectory with context\n",
    "    combined = Concatenate()([encoded, context_processed])\n",
    "    combined = Dense(lstm_units, activation='relu')(combined)  # Project back to lstm_units size\n",
    "    \n",
    "    # Decoder\n",
    "    x = RepeatVector(timesteps_out)(combined)\n",
    "    for _ in range(num_layers):\n",
    "        x = LSTM(lstm_units, return_sequences=True)(x)\n",
    "        #x = LayerNormalization()(x)\n",
    "        # x = Dropout(0.2)(x)\n",
    "    \n",
    "    x = TimeDistributed(Dense(128, activation='relu'))(x)\n",
    "    x = TimeDistributed(Dense(64, activation='relu'))(x)\n",
    "    outputs = TimeDistributed(Dense(output_dim))(x)\n",
    "    \n",
    "    model = Model([trajectory_input, context_input], outputs)  # Multiple inputs\n",
    "    model.compile(optimizer=Adam(learning_rate=lr), loss='mse', metrics=['mae'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d68034c165292c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T19:20:44.861403Z",
     "start_time": "2025-05-02T19:20:44.758718Z"
    },
    "id": "4d68034c165292c5"
   },
   "outputs": [],
   "source": [
    "from keras.src.callbacks import LearningRateScheduler, EarlyStopping, Callback\n",
    "from keras.src.optimizers import Adam\n",
    "from keras import Model\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def exponential_decay_schedule(epoch, lr):\n",
    "    decay_rate = 0.9\n",
    "    decay_steps = 5\n",
    "    if epoch % decay_steps == 0 and epoch:\n",
    "        print('Learning rate update:', lr * decay_rate)\n",
    "        return lr * decay_rate\n",
    "    return lr\n",
    "\n",
    "def is_stationary(trajectory, threshold=0.1):\n",
    "    \"\"\"Check if agent is stationary based on position variance\"\"\"\n",
    "    positions = trajectory[:, :2]  # x, y coordinates\n",
    "    return np.all(np.var(positions, axis=0) < threshold)\n",
    "\n",
    "# Custom callback to monitor LR and stop training\n",
    "class LRThresholdCallback(Callback):\n",
    "    def __init__(self, threshold=9e-5):\n",
    "        super().__init__()\n",
    "        self.threshold = threshold\n",
    "        self.should_stop = False\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        lr = float(self.model.optimizer.learning_rate.numpy())\n",
    "        if lr < self.threshold:\n",
    "            print(f\"\\nLearning rate {lr:.6f} < threshold {self.threshold}, moving to Phase 2.\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "def train_model(train_data, batch_size=32, validation_split=0.2, Tobs=50, Tpred=60):\n",
    "    n_scenarios = train_data.shape[0]\n",
    "    X_train_raw = []\n",
    "    y_train_deltas = []\n",
    "    stationary_context = []\n",
    "\n",
    "    for i in range(n_scenarios):\n",
    "        # Process ego vehicle as before\n",
    "        ego_data = train_data[i, 0, :, :]\n",
    "        if np.all(ego_data == 0):\n",
    "            continue\n",
    "        \n",
    "        observed = ego_data[:Tobs]\n",
    "        future = ego_data[Tobs:Tobs+Tpred, :2]\n",
    "        last_obs_pos = observed[-1, :2]\n",
    "        \n",
    "        if np.any(np.all(observed == 0, axis=1)) or np.any(np.all(future == 0, axis=1)):\n",
    "            continue\n",
    "        \n",
    "        # Extract stationary agents for this scenario\n",
    "        scenario_stationary = []\n",
    "        for vehicle_idx in range(1, train_data.shape[1]):  # Skip ego\n",
    "            vehicle_data = train_data[i, vehicle_idx, :, :]\n",
    "            \n",
    "            if np.all(vehicle_data == 0):\n",
    "                continue\n",
    "                \n",
    "            # Check if agent is present and stationary during observation window\n",
    "            obs_segment = vehicle_data[:Tobs]\n",
    "            if not np.any(np.all(obs_segment == 0, axis=1)):\n",
    "                # Store relative position to ego\n",
    "                relative_pos = obs_segment[-1, :2] - last_obs_pos\n",
    "                scenario_stationary.append(relative_pos)\n",
    "        \n",
    "        # Pad or limit number of stationary agents\n",
    "        max_stationary = 30  # Adjust as needed\n",
    "        if len(scenario_stationary) > max_stationary:\n",
    "            scenario_stationary = scenario_stationary[:max_stationary]\n",
    "        else:\n",
    "            # Pad with zeros\n",
    "            while len(scenario_stationary) < max_stationary:\n",
    "                scenario_stationary.append([0.0, 0.0])\n",
    "        \n",
    "        delta = np.diff(np.vstack([last_obs_pos, future]), axis=0)\n",
    "        \n",
    "        X_train_raw.append(observed)\n",
    "        y_train_deltas.append(delta)\n",
    "        stationary_context.append(scenario_stationary)\n",
    "    \n",
    "    stationary_context = np.array(stationary_context)\n",
    "\n",
    "\n",
    "    X_train = np.array(X_train_raw)\n",
    "    y_train = np.array(y_train_deltas)\n",
    "\n",
    "    print(f\"Training on {X_train.shape[0]} valid sequences.\")\n",
    "    print(f\"Input shape: {X_train.shape}, Delta Output shape: {y_train.shape}\")\n",
    "\n",
    "    # --- Normalize Input and Output ---\n",
    "    X_mean = X_train.mean(axis=(0, 1), keepdims=True)  # shape: (1, 1, 6)\n",
    "    X_std = X_train.std(axis=(0, 1), keepdims=True) + 1e-8\n",
    "\n",
    "    y_mean = y_train.mean(axis=(0, 1), keepdims=True)  # shape: (1, 1, 2)\n",
    "    y_std = y_train.std(axis=(0, 1), keepdims=True) + 1e-8\n",
    "\n",
    "    X_train = (X_train - X_mean) / X_std\n",
    "    y_train = (y_train - y_mean) / y_std\n",
    "\n",
    "    stationary_context = np.array(stationary_context)  # Shape: (n_samples, max_stationary, 2)\n",
    "\n",
    "    # Normalize context - same approach as your other data\n",
    "    context_mean = stationary_context.mean(axis=(0, 1), keepdims=True)  # shape: (1, 1, 2)\n",
    "    context_std = stationary_context.std(axis=(0, 1), keepdims=True) + 1e-8\n",
    "    \n",
    "    stationary_context = (stationary_context - context_mean) / context_std\n",
    "\n",
    "    model = create_lstm_encoder_decoder(\n",
    "        input_dim=X_train.shape[-1],\n",
    "        output_dim=2,\n",
    "        timesteps_in=Tobs,\n",
    "        timesteps_out=Tpred,\n",
    "        loss_fn='mse',\n",
    "        lr=0.001\n",
    "    )\n",
    "\n",
    "    phase1_callbacks = [\n",
    "        LearningRateScheduler(exponential_decay_schedule),\n",
    "        EarlyStopping(patience=4, restore_best_weights=True, monitor='val_loss'),\n",
    "        LRThresholdCallback(threshold=9e-5)\n",
    "    ]\n",
    "\n",
    "    print(\"\\n--- Phase 1: Training ---\")\n",
    "    model.fit(\n",
    "        [X_train, stationary_context], y_train,  # List of inputs\n",
    "        epochs=50,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=validation_split,\n",
    "        callbacks=phase1_callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- Phase 2: Fine-tuning ---\")\n",
    "    model.compile(optimizer=Adam(1e-4), loss='mse', metrics=['mae'])\n",
    "    phase2_callbacks = [\n",
    "        LearningRateScheduler(exponential_decay_schedule),\n",
    "        EarlyStopping(patience=3, restore_best_weights=True, monitor='val_loss')\n",
    "    ]\n",
    "    model.fit(\n",
    "        [X_train, stationary_context], y_train,\n",
    "        epochs=20,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=validation_split,\n",
    "        callbacks=phase2_callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Return model and normalization parameters\n",
    "    return model, X_mean, X_std, y_mean, y_std, context_mean, context_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3d667b335d4a0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T19:20:47.293888Z",
     "start_time": "2025-05-02T19:20:47.285992Z"
    },
    "id": "e3d667b335d4a0a"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_model(model, filepath='lstm_1.pkl'):\n",
    "    \"\"\"Save model and scaler together in a pickle file\"\"\"\n",
    "    model_json = model.to_json()\n",
    "    model_weights = model.get_weights()\n",
    "    data = {\n",
    "        'model_json': model_json,\n",
    "        'model_weights': model_weights,\n",
    "    }\n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    print(f\"Model saved to {filepath}\")\n",
    "\n",
    "def load_model(filepath='lstm_1.pkl'):\n",
    "    \"\"\"Load model and scaler from pickle file\"\"\"\n",
    "    with open(filepath, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    # Reconstruct model\n",
    "    model = tf.keras.models.model_from_json(data['model_json'])\n",
    "    model.set_weights(data['model_weights'])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "deeaeac04573c115",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T18:42:28.285866Z",
     "start_time": "2025-05-02T18:41:59.875150Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "deeaeac04573c115",
    "outputId": "35cacfb6-17f4-49a4-93d9-81857d544c2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 20 valid sequences.\n",
      "Input shape: (20, 50, 6), Delta Output shape: (20, 60, 2)\n",
      "\n",
      "--- Phase 1: Training ---\n",
      "Epoch 1/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0039 - mae: 0.6571WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae,lr\n",
      "1/1 [==============================] - 12s 12s/step - loss: 1.0039 - mae: 0.6571 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.9688 - mae: 0.6372WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae,lr\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.9688 - mae: 0.6372 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7024 - mae: 0.6291WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae,lr\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.7024 - mae: 0.6291 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.2292 - mae: 0.8683WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae,lr\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 1.2292 - mae: 0.8683 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.9423 - mae: 0.6617WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae,lr\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 0.9423 - mae: 0.6617 - lr: 0.0010\n",
      "Learning rate update: 0.0009000000427477062\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.8056 - mae: 0.5824WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae,lr\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.8056 - mae: 0.5824 - lr: 9.0000e-04\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.8032 - mae: 0.6789WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae,lr\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.8032 - mae: 0.6789 - lr: 9.0000e-04\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.7652 - mae: 0.7072WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae,lr\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.7652 - mae: 0.7072 - lr: 9.0000e-04\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.6059 - mae: 0.5474WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae,lr\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.6059 - mae: 0.5474 - lr: 9.0000e-04\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5463 - mae: 0.5152WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae,lr\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.5463 - mae: 0.5152 - lr: 9.0000e-04\n",
      "Learning rate update: 0.0008100000384729356\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.5661 - mae: 0.5617WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae,lr\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.5661 - mae: 0.5617 - lr: 8.1000e-04\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.4178 - mae: 0.4442WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae,lr\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.4178 - mae: 0.4442 - lr: 8.1000e-04\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3842 - mae: 0.4304WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae,lr\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.3842 - mae: 0.4304 - lr: 8.1000e-04\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3721 - mae: 0.4327WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae,lr\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.3721 - mae: 0.4327 - lr: 8.1000e-04\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3919 - mae: 0.4436WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae,lr\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.3919 - mae: 0.4436 - lr: 8.1000e-04\n",
      "Learning rate update: 0.0007290000503417104\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3655 - mae: 0.4223WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae,lr\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.3655 - mae: 0.4223 - lr: 7.2900e-04\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3053 - mae: 0.3779WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae,lr\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.3053 - mae: 0.3779 - lr: 7.2900e-04\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.3190 - mae: 0.3661WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae,lr\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.3190 - mae: 0.3661 - lr: 7.2900e-04\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2746 - mae: 0.3440WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae,lr\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.2746 - mae: 0.3440 - lr: 7.2900e-04\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2409 - mae: 0.3283WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae,lr\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.2409 - mae: 0.3283 - lr: 7.2900e-04\n",
      "Learning rate update: 0.0006561000715009868\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2236 - mae: 0.3057WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae,lr\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.2236 - mae: 0.3057 - lr: 6.5610e-04\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.2041 - mae: 0.2946WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae,lr\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.2041 - mae: 0.2946 - lr: 6.5610e-04\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1771 - mae: 0.2716WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae,lr\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.1771 - mae: 0.2716 - lr: 6.5610e-04\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1687 - mae: 0.2673WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae,lr\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.1687 - mae: 0.2673 - lr: 6.5610e-04\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1501 - mae: 0.2459WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae,lr\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.1501 - mae: 0.2459 - lr: 6.5610e-04\n",
      "Learning rate update: 0.0005904900433961303\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1417 - mae: 0.2404WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae,lr\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.1417 - mae: 0.2404 - lr: 5.9049e-04\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1433 - mae: 0.2396WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae,lr\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.1433 - mae: 0.2396 - lr: 5.9049e-04\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1289 - mae: 0.2212WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae,lr\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.1289 - mae: 0.2212 - lr: 5.9049e-04\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1176 - mae: 0.2107WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae,lr\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.1176 - mae: 0.2107 - lr: 5.9049e-04\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1144 - mae: 0.2157WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae,lr\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.1144 - mae: 0.2157 - lr: 5.9049e-04\n",
      "Learning rate update: 0.0005314410547725857\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1051 - mae: 0.2067WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae,lr\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.1051 - mae: 0.2067 - lr: 5.3144e-04\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1009 - mae: 0.1963WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae,lr\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.1009 - mae: 0.1963 - lr: 5.3144e-04\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0957 - mae: 0.1910WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae,lr\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0957 - mae: 0.1910 - lr: 5.3144e-04\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0937 - mae: 0.1845WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae,lr\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.0937 - mae: 0.1845 - lr: 5.3144e-04\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0834 - mae: 0.1672WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae,lr\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0834 - mae: 0.1672 - lr: 5.3144e-04\n",
      "Learning rate update: 0.00047829695977270604\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0804 - mae: 0.1649WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae,lr\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0804 - mae: 0.1649 - lr: 4.7830e-04\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0772 - mae: 0.1586WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae,lr\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0772 - mae: 0.1586 - lr: 4.7830e-04\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0748 - mae: 0.1535WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae,lr\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0748 - mae: 0.1535 - lr: 4.7830e-04\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0718 - mae: 0.1484WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae,lr\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0718 - mae: 0.1484 - lr: 4.7830e-04\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0684 - mae: 0.1447WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae,lr\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0684 - mae: 0.1447 - lr: 4.7830e-04\n",
      "Learning rate update: 0.0004304672533180565\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0641 - mae: 0.1425WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae,lr\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0641 - mae: 0.1425 - lr: 4.3047e-04\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0619 - mae: 0.1396WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae,lr\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0619 - mae: 0.1396 - lr: 4.3047e-04\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0579 - mae: 0.1290WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae,lr\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0579 - mae: 0.1290 - lr: 4.3047e-04\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0576 - mae: 0.1258WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae,lr\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0576 - mae: 0.1258 - lr: 4.3047e-04\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0536 - mae: 0.1195WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae,lr\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0536 - mae: 0.1195 - lr: 4.3047e-04\n",
      "Learning rate update: 0.00038742052274756136\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0520 - mae: 0.1215WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae,lr\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0520 - mae: 0.1215 - lr: 3.8742e-04\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0568 - mae: 0.1261WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae,lr\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0568 - mae: 0.1261 - lr: 3.8742e-04\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0732 - mae: 0.1373WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae,lr\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0732 - mae: 0.1373 - lr: 3.8742e-04\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0714 - mae: 0.1381WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae,lr\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0714 - mae: 0.1381 - lr: 3.8742e-04\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0790 - mae: 0.1388WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae,lr\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0790 - mae: 0.1388 - lr: 3.8742e-04\n",
      "\n",
      "--- Phase 2: Fine-tuning ---\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0914 - mae: 0.1739WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae,lr\n",
      "1/1 [==============================] - 9s 9s/step - loss: 0.0914 - mae: 0.1739 - lr: 1.0000e-04\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.1130 - mae: 0.1981WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae,lr\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.1130 - mae: 0.1981 - lr: 1.0000e-04\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0959 - mae: 0.1834WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae,lr\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.0959 - mae: 0.1834 - lr: 1.0000e-04\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0770 - mae: 0.1539WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae,lr\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.0770 - mae: 0.1539 - lr: 1.0000e-04\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0692 - mae: 0.1415WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae,lr\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 0.0692 - mae: 0.1415 - lr: 1.0000e-04\n",
      "Learning rate update: 8.999999772640876e-05\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0686 - mae: 0.1449WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae,lr\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0686 - mae: 0.1449 - lr: 9.0000e-05\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0659 - mae: 0.1448WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae,lr\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.0659 - mae: 0.1448 - lr: 9.0000e-05\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0603 - mae: 0.1392WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae,lr\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0603 - mae: 0.1392 - lr: 9.0000e-05\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0555 - mae: 0.1306WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae,lr\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.0555 - mae: 0.1306 - lr: 9.0000e-05\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0534 - mae: 0.1247WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae,lr\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.0534 - mae: 0.1247 - lr: 9.0000e-05\n"
     ]
    }
   ],
   "source": [
    "# first check that the model can overfit on small data\n",
    "model, X_mean, X_std, y_mean, y_std, context_mean, context_std  = train_model(train_data[:20], batch_size=20, validation_split=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ce3811f963721e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T18:42:54.682368Z",
     "start_time": "2025-05-02T18:42:54.675886Z"
    },
    "id": "8ce3811f963721e2"
   },
   "outputs": [],
   "source": [
    "def plot_mae_by_timestep(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Visualize MAE across timesteps in the prediction horizon.\n",
    "\n",
    "    Args:\n",
    "        y_true (np.ndarray): shape (N, Tpred, 2)\n",
    "        y_pred (np.ndarray): shape (N, Tpred, 2)\n",
    "    \"\"\"\n",
    "    mae_per_timestep = np.mean(np.abs(y_true - y_pred), axis=(0, 2))  # shape (Tpred,)\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(mae_per_timestep, label='MAE per Timestep')\n",
    "    plt.xlabel('Timestep')\n",
    "    plt.ylabel('MAE (meters)')\n",
    "    plt.title('Mean Absolute Error Over Prediction Horizon')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49143614a59770d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T18:42:55.168565Z",
     "start_time": "2025-05-02T18:42:55.165875Z"
    },
    "id": "49143614a59770d"
   },
   "outputs": [],
   "source": [
    "def reconstruct_absolute_positions(pred_deltas, last_observed_positions):\n",
    "    \"\"\"\n",
    "    Reconstruct absolute predicted positions by adding deltas to the last observed position.\n",
    "\n",
    "    Args:\n",
    "        pred_deltas: np.ndarray of shape (N, Tpred, 2)\n",
    "        last_observed_positions: np.ndarray of shape (N, 2)\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray of shape (N, Tpred, 2)\n",
    "    \"\"\"\n",
    "    return last_observed_positions[:, None, :] + np.cumsum(pred_deltas, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "def forecast_positions(scenario_data, Tobs, Tpred, model, X_mean, X_std, y_mean, y_std, \n",
    "                      context_mean, context_std, max_stationary=30):\n",
    "    \"\"\"\n",
    "    Use normalized LSTM model to forecast future deltas and reconstruct absolute positions.\n",
    "    Args:\n",
    "        scenario_data (numpy.ndarray): Shape (agents, time_steps, dimensions)\n",
    "        Tobs (int): Number of observed time steps\n",
    "        Tpred (int): Number of future time steps to predict\n",
    "        model (Model): Trained LSTM model that predicts normalized deltas\n",
    "        X_mean, X_std: Normalization stats for input\n",
    "        y_mean, y_std: Normalization stats for output\n",
    "        context_mean, context_std: Normalization stats for context\n",
    "        max_stationary (int): Maximum number of stationary agents in context\n",
    "    Returns:\n",
    "        numpy.ndarray: Predicted absolute positions of shape (agents, Tpred, 2)\n",
    "    \"\"\"\n",
    "    agents, _, _ = scenario_data.shape\n",
    "    predicted_positions = np.zeros((agents, Tpred, 2))\n",
    "    pred_deltas_all = []\n",
    "    \n",
    "    for agent_idx in range(agents):\n",
    "        agent_data = scenario_data[agent_idx, :Tobs, :]  # shape (Tobs, 6)\n",
    "        \n",
    "        # Skip if fully padded\n",
    "        if np.all(agent_data == 0):\n",
    "            continue\n",
    "            \n",
    "        # Extract stationary context for this agent\n",
    "        agent_last_pos = agent_data[-1, :2]\n",
    "        scenario_stationary = []\n",
    "        \n",
    "        for other_idx in range(agents):\n",
    "            if other_idx == agent_idx:  # Skip self\n",
    "                continue\n",
    "                \n",
    "            other_data = scenario_data[other_idx, :Tobs, :]\n",
    "            if np.all(other_data == 0):\n",
    "                continue\n",
    "                \n",
    "            # Check if other agent is stationary\n",
    "            relative_pos = other_data[-1, :2] - agent_last_pos\n",
    "            scenario_stationary.append(relative_pos)\n",
    "        \n",
    "        # Pad or limit stationary agents\n",
    "        if len(scenario_stationary) > max_stationary:\n",
    "            scenario_stationary = scenario_stationary[:max_stationary]\n",
    "        else:\n",
    "            while len(scenario_stationary) < max_stationary:\n",
    "                scenario_stationary.append([0.0, 0.0])\n",
    "        \n",
    "        # Prepare inputs\n",
    "        X_pred = np.expand_dims(agent_data, axis=0)  # shape (1, Tobs, 6)\n",
    "        X_pred_norm = (X_pred - X_mean) / X_std\n",
    "        \n",
    "        context_pred = np.expand_dims(scenario_stationary, axis=0)  # shape (1, max_stationary, 2)\n",
    "        context_pred_norm = (context_pred - context_mean) / context_std\n",
    "        \n",
    "        # Predict normalized deltas with both inputs\n",
    "        pred_deltas_norm = model.predict([X_pred_norm, context_pred_norm], verbose=0)  # shape (1, Tpred, 2)\n",
    "        \n",
    "        # Denormalize deltas\n",
    "        pred_deltas = pred_deltas_norm * y_std + y_mean\n",
    "        pred_deltas_all.append(pred_deltas[0])\n",
    "        \n",
    "        # Reconstruct absolute positions\n",
    "        last_pos = agent_data[Tobs - 1, :2]  # shape (2,)\n",
    "        abs_positions = reconstruct_absolute_positions(\n",
    "            pred_deltas=pred_deltas,\n",
    "            last_observed_positions=np.expand_dims(last_pos, axis=0)\n",
    "        )[0]\n",
    "        \n",
    "        predicted_positions[agent_idx] = abs_positions\n",
    "    \n",
    "    return predicted_positions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "658c258cac06616e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T18:42:56.727430Z",
     "start_time": "2025-05-02T18:42:56.726626Z"
    },
    "id": "658c258cac06616e"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "def make_gif(data_matrix1, data_matrix2, name='comparison'):\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.animation as animation\n",
    "\n",
    "    cmap1 = plt.cm.get_cmap('viridis', 50)\n",
    "    cmap2 = plt.cm.get_cmap('plasma', 50)\n",
    "\n",
    "    assert data_matrix1.shape[1] == data_matrix2.shape[1], \"Both matrices must have same number of timesteps\"\n",
    "    timesteps = data_matrix1.shape[1]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(18, 9))\n",
    "    ax1, ax2 = axes\n",
    "\n",
    "    def update(frame):\n",
    "        for ax in axes:\n",
    "            ax.clear()\n",
    "\n",
    "        for i in range(data_matrix1.shape[0]):\n",
    "            for (data_matrix, ax, cmap) in [(data_matrix1, ax1, cmap1), (data_matrix2, ax2, cmap2)]:\n",
    "                x = data_matrix[i, frame, 0]\n",
    "                y = data_matrix[i, frame, 1]\n",
    "                if x != 0 and y != 0:\n",
    "                    xs = data_matrix[i, :frame+1, 0]\n",
    "                    ys = data_matrix[i, :frame+1, 1]\n",
    "                    mask = (xs != 0) & (ys != 0)\n",
    "                    xs = xs[mask]\n",
    "                    ys = ys[mask]\n",
    "                    if len(xs) > 0 and len(ys) > 0:\n",
    "                        color = cmap(i)\n",
    "                        ax.plot(xs, ys, alpha=0.9, color=color)\n",
    "                        ax.scatter(x, y, s=80, color=color)\n",
    "\n",
    "        # Plot ego vehicle (index 0) on both\n",
    "        ax1.plot(data_matrix1[0, :frame, 0], data_matrix1[0, :frame, 1], color='tab:orange', label='Ego Vehicle')\n",
    "        ax1.scatter(data_matrix1[0, frame, 0], data_matrix1[0, frame, 1], s=80, color='tab:orange')\n",
    "        ax1.set_title('Prediction')\n",
    "\n",
    "        ax2.plot(data_matrix2[0, :frame, 0], data_matrix2[0, :frame, 1], color='tab:orange', label='Ego Vehicle')\n",
    "        ax2.scatter(data_matrix2[0, frame, 0], data_matrix2[0, frame, 1], s=80, color='tab:orange')\n",
    "        ax2.set_title('Actual')\n",
    "\n",
    "        for ax, data_matrix in zip(axes, [data_matrix1, data_matrix2]):\n",
    "            ax.set_xlim(data_matrix[:, :, 0][data_matrix[:, :, 0] != 0].min() - 10,\n",
    "                        data_matrix[:, :, 0][data_matrix[:, :, 0] != 0].max() + 10)\n",
    "            ax.set_ylim(data_matrix[:, :, 1][data_matrix[:, :, 1] != 0].min() - 10,\n",
    "                        data_matrix[:, :, 1][data_matrix[:, :, 1] != 0].max() + 10)\n",
    "            ax.legend()\n",
    "            ax.set_xlabel('X')\n",
    "            ax.set_ylabel('Y')\n",
    "\n",
    "        # Compute MSE over non-zero entries up to current frame\n",
    "        mask = (data_matrix2[:, :frame+1, :] != 0) & (data_matrix1[:, :frame+1, :] != 0)\n",
    "        mse = np.mean((data_matrix1[:, :frame+1, :][mask] - data_matrix2[:, :frame+1, :][mask]) ** 2)\n",
    "\n",
    "        fig.suptitle(f\"Timestep {frame} - MSE: {mse:.4f}\", fontsize=16)\n",
    "        return ax1.collections + ax1.lines + ax2.collections + ax2.lines\n",
    "\n",
    "    anim = animation.FuncAnimation(fig, update, frames=list(range(0, timesteps, 3)), interval=100, blit=True)\n",
    "    anim.save(f'trajectory_visualization_{name}.gif', writer='pillow')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a5f4cc76c3c72d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T01:00:46.798817Z",
     "start_time": "2025-05-03T01:00:35.906297Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5a5f4cc76c3c72d3",
    "outputId": "751261af-dc4b-4b92-f147-060177853e66"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m test_scenario \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcopy()  \u001b[38;5;66;03m# shape (agents, time_steps, features)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Forecast future positions\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m predicted_positions \u001b[38;5;241m=\u001b[39m forecast_positions(test_scenario, Tobs, Tpred, \u001b[43mmodel\u001b[49m, X_mean, X_std, y_mean, y_std, context_mean, context_std)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Create combined matrix of past observed + predicted for ego agent (agent 0)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m ego_past \u001b[38;5;241m=\u001b[39m test_scenario[\u001b[38;5;241m0\u001b[39m, :Tobs, :\u001b[38;5;241m2\u001b[39m]               \u001b[38;5;66;03m# shape (Tobs, 2)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# visualize prediction\n",
    "\n",
    "# model = load_model()\n",
    "\n",
    "# Parameters\n",
    "Tobs = 50\n",
    "Tpred = 60\n",
    "\n",
    "data = train_data[6000]\n",
    "\n",
    "# Select a test scenario (can use any valid index)\n",
    "test_scenario = data.copy()  # shape (agents, time_steps, features)\n",
    "\n",
    "# Forecast future positions\n",
    "predicted_positions = forecast_positions(test_scenario, Tobs, Tpred, model, X_mean, X_std, y_mean, y_std, context_mean, context_std)\n",
    "\n",
    "# Create combined matrix of past observed + predicted for ego agent (agent 0)\n",
    "ego_past = test_scenario[0, :Tobs, :2]               # shape (Tobs, 2)\n",
    "ego_future = predicted_positions[0]                  # shape (Tpred, 2)\n",
    "ego_full = np.concatenate([ego_past, ego_future], axis=0)  # shape (Tobs + Tpred, 2)\n",
    "\n",
    "# Create updated scenario with predicted ego and original others\n",
    "updated_scenario = test_scenario.copy()\n",
    "updated_scenario[0, :Tobs+Tpred, :2] = ego_full  # Replace ego trajectory\n",
    "\n",
    "# Visualize\n",
    "make_gif(updated_scenario, data, name='lstm1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3338be4ed075b368",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-02T22:42:07.483287Z",
     "start_time": "2025-05-02T19:20:54.021001Z"
    },
    "id": "3338be4ed075b368"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 10000 valid sequences.\n",
      "Input shape: (10000, 50, 6), Delta Output shape: (10000, 60, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-30 00:54:08.272017: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10534 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:8a:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Phase 1: Training ---\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-30 00:54:20.062082: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 29s 77ms/step - loss: 0.3199 - mae: 0.3827 - val_loss: 0.1791 - val_mae: 0.2816 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "250/250 [==============================] - 16s 63ms/step - loss: 0.1602 - mae: 0.2698 - val_loss: 0.1348 - val_mae: 0.2484 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "250/250 [==============================] - 16s 63ms/step - loss: 0.1263 - mae: 0.2345 - val_loss: 0.1167 - val_mae: 0.2154 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "250/250 [==============================] - 16s 63ms/step - loss: 0.1209 - mae: 0.2280 - val_loss: 0.1166 - val_mae: 0.2143 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "250/250 [==============================] - 16s 65ms/step - loss: 0.1100 - mae: 0.2152 - val_loss: 0.1149 - val_mae: 0.2142 - lr: 0.0010\n",
      "Learning rate update: 0.0009000000427477062\n",
      "Epoch 6/50\n",
      "250/250 [==============================] - 16s 65ms/step - loss: 0.1042 - mae: 0.2085 - val_loss: 0.1101 - val_mae: 0.2205 - lr: 9.0000e-04\n",
      "Epoch 7/50\n",
      "250/250 [==============================] - 16s 64ms/step - loss: 0.0984 - mae: 0.2013 - val_loss: 0.1101 - val_mae: 0.2135 - lr: 9.0000e-04\n",
      "Epoch 8/50\n",
      "250/250 [==============================] - 16s 65ms/step - loss: 0.0983 - mae: 0.2025 - val_loss: 0.1044 - val_mae: 0.2066 - lr: 9.0000e-04\n",
      "Epoch 9/50\n",
      "250/250 [==============================] - 16s 63ms/step - loss: 0.0926 - mae: 0.1946 - val_loss: 0.1027 - val_mae: 0.1934 - lr: 9.0000e-04\n",
      "Epoch 10/50\n",
      "250/250 [==============================] - 16s 63ms/step - loss: 0.0898 - mae: 0.1916 - val_loss: 0.1111 - val_mae: 0.2158 - lr: 9.0000e-04\n",
      "Learning rate update: 0.0008100000384729356\n",
      "Epoch 11/50\n",
      "250/250 [==============================] - 16s 63ms/step - loss: 0.0819 - mae: 0.1807 - val_loss: 0.1167 - val_mae: 0.2252 - lr: 8.1000e-04\n",
      "Epoch 12/50\n",
      "250/250 [==============================] - 16s 63ms/step - loss: 0.0805 - mae: 0.1801 - val_loss: 0.0968 - val_mae: 0.1859 - lr: 8.1000e-04\n",
      "Epoch 13/50\n",
      "250/250 [==============================] - 16s 65ms/step - loss: 0.0749 - mae: 0.1719 - val_loss: 0.0936 - val_mae: 0.1836 - lr: 8.1000e-04\n",
      "Epoch 14/50\n",
      "106/250 [===========>..................] - ETA: 8s - loss: 0.0697 - mae: 0.1669"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model, X_mean, X_std, y_mean, y_std, context_mean, context_std = train_model(train_data)\n",
    "\n",
    "# Save the model\n",
    "save_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e8bf6a02ebcd4b24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T01:01:38.098690Z",
     "start_time": "2025-05-03T01:01:38.097196Z"
    },
    "id": "e8bf6a02ebcd4b24"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "def evaluate_mse(train_data, model, Tobs=50, Tpred=60):\n",
    "    \"\"\"\n",
    "    Computes LSTM prediction for ego agent and evaluates MSE with progress reporting.\n",
    "    \"\"\"\n",
    "    N = train_data.shape[0]\n",
    "    mse_list = []\n",
    "    valid_scenarios = 0\n",
    "\n",
    "    print(f\"Evaluating {N} scenarios...\")\n",
    "\n",
    "    # Progress reporting variables\n",
    "    report_interval = max(1, N // 10)  # Report at 10% intervals\n",
    "\n",
    "    for i in range(N):\n",
    "        # Progress reporting\n",
    "        if i % report_interval == 0 or i == N-1:\n",
    "            print(f\"Processing scenario {i+1}/{N} ({(i+1)/N*100:.1f}%)\")\n",
    "\n",
    "        scenario_data = train_data[i]\n",
    "        ego_agent_data = scenario_data[0]\n",
    "        ground_truth = ego_agent_data[Tobs:Tobs+Tpred, :2]\n",
    "\n",
    "        # Skip if ground truth contains all zeros (padded)\n",
    "        if np.all(ground_truth == 0):\n",
    "            continue\n",
    "\n",
    "        valid_scenarios += 1\n",
    "\n",
    "        # Forecast future positions\n",
    "        predicted_positions = forecast_positions(\n",
    "            ego_agent_data[np.newaxis, :, :],\n",
    "            Tobs, Tpred, model, X_mean, X_std, y_mean, y_std, context_mean, context_std\n",
    "        )\n",
    "\n",
    "        # Compute MSE\n",
    "        mse = mean_squared_error(ground_truth, predicted_positions[0])\n",
    "        mse_list.append(mse)\n",
    "\n",
    "        # Occasional MSE reporting\n",
    "        if i % report_interval == 0:\n",
    "            print(f\"  Current scenario MSE: {mse:.4f}\")\n",
    "\n",
    "    # Final results\n",
    "    if mse_list:\n",
    "        overall_mse = np.mean(mse_list)\n",
    "        print(f\"Evaluation complete: {valid_scenarios} valid scenarios\")\n",
    "        print(f\"Mean Squared Error (MSE): {overall_mse:.4f}\")\n",
    "        print(f\"Min MSE: {np.min(mse_list):.4f}, Max MSE: {np.max(mse_list):.4f}\")\n",
    "        return overall_mse\n",
    "    else:\n",
    "        print(\"No valid scenarios for evaluation.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6ae1b88910d7e8bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T01:12:28.986816Z",
     "start_time": "2025-05-03T01:04:00.357291Z"
    },
    "id": "6ae1b88910d7e8bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating 10000 scenarios...\n",
      "Processing scenario 1/10000 (0.0%)\n",
      "  Current scenario MSE: 0.6715\n",
      "Processing scenario 1001/10000 (10.0%)\n",
      "  Current scenario MSE: 6.1291\n",
      "Processing scenario 2001/10000 (20.0%)\n",
      "  Current scenario MSE: 0.5003\n",
      "Processing scenario 3001/10000 (30.0%)\n",
      "  Current scenario MSE: 0.0499\n",
      "Processing scenario 4001/10000 (40.0%)\n",
      "  Current scenario MSE: 0.4941\n",
      "Processing scenario 5001/10000 (50.0%)\n",
      "  Current scenario MSE: 14.7437\n",
      "Processing scenario 6001/10000 (60.0%)\n",
      "  Current scenario MSE: 88.2834\n",
      "Processing scenario 7001/10000 (70.0%)\n",
      "  Current scenario MSE: 11.7321\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/ops/array_ops.py:5238\u001b[0m, in \u001b[0;36mgather\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   5235\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   5236\u001b[0m   \u001b[38;5;66;03m# TODO(apassos) find a less bad way of detecting resource variables\u001b[39;00m\n\u001b[1;32m   5237\u001b[0m   \u001b[38;5;66;03m# without introducing a circular dependency.\u001b[39;00m\n\u001b[0;32m-> 5238\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse_read\u001b[49m(indices, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[1;32m   5239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/framework/tensor.py:261\u001b[0m, in \u001b[0;36mTensor.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m    257\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;124m    If you are looking for numpy-related methods, please run the following:\u001b[39m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;124m    tf.experimental.numpy.experimental_enable_numpy_behavior()\u001b[39m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;124m  \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[0;32m--> 261\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SymbolicTensor' object has no attribute 'sparse_read'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Evaluate on training data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mevaluate_mse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[83], line 33\u001b[0m, in \u001b[0;36mevaluate_mse\u001b[0;34m(train_data, model, Tobs, Tpred)\u001b[0m\n\u001b[1;32m     30\u001b[0m valid_scenarios \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Forecast future positions\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m predicted_positions \u001b[38;5;241m=\u001b[39m \u001b[43mforecast_positions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mego_agent_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnewaxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mTobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_std\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_std\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_std\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Compute MSE\u001b[39;00m\n\u001b[1;32m     39\u001b[0m mse \u001b[38;5;241m=\u001b[39m mean_squared_error(ground_truth, predicted_positions[\u001b[38;5;241m0\u001b[39m])\n",
      "Cell \u001b[0;32mIn[76], line 74\u001b[0m, in \u001b[0;36mforecast_positions\u001b[0;34m(scenario_data, Tobs, Tpred, model, X_mean, X_std, y_mean, y_std, context_mean, context_std, max_stationary)\u001b[0m\n\u001b[1;32m     71\u001b[0m context_pred_norm \u001b[38;5;241m=\u001b[39m (context_pred \u001b[38;5;241m-\u001b[39m context_mean) \u001b[38;5;241m/\u001b[39m context_std\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# Predict normalized deltas with both inputs\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m pred_deltas_norm \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_pred_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_pred_norm\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# shape (1, Tpred, 2)\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# Denormalize deltas\u001b[39;00m\n\u001b[1;32m     77\u001b[0m pred_deltas \u001b[38;5;241m=\u001b[39m pred_deltas_norm \u001b[38;5;241m*\u001b[39m y_std \u001b[38;5;241m+\u001b[39m y_mean\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/keras/src/engine/training.py:2620\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2611\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m   2612\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   2613\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing Model.predict with MultiWorkerMirroredStrategy \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2614\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor TPUStrategy and AutoShardPolicy.FILE might lead to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2617\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m   2618\u001b[0m         )\n\u001b[0;32m-> 2620\u001b[0m data_handler \u001b[38;5;241m=\u001b[39m \u001b[43mdata_adapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_data_handler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2621\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2622\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2623\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2624\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2625\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2626\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2627\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2628\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2629\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2630\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_execution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_steps_per_execution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2631\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2633\u001b[0m \u001b[38;5;66;03m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[1;32m   2634\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callbacks, callbacks_module\u001b[38;5;241m.\u001b[39mCallbackList):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/keras/src/engine/data_adapter.py:1688\u001b[0m, in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _ClusterCoordinatorExactEvalDataHandler(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1687\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _ClusterCoordinatorDataHandler(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 1688\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataHandler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/keras/src/engine/data_adapter.py:1292\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute, pss_evaluation_shards)\u001b[0m\n\u001b[1;32m   1289\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution \u001b[38;5;241m=\u001b[39m steps_per_execution\n\u001b[1;32m   1291\u001b[0m adapter_cls \u001b[38;5;241m=\u001b[39m select_data_adapter(x, y)\n\u001b[0;32m-> 1292\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_adapter \u001b[38;5;241m=\u001b[39m \u001b[43madapter_cls\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1294\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1298\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1302\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1303\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistribution_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_strategy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1304\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpss_evaluation_shards\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpss_evaluation_shards\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1306\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1308\u001b[0m strategy \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mget_strategy()\n\u001b[1;32m   1310\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/keras/src/engine/data_adapter.py:355\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m flat_dataset\n\u001b[1;32m    353\u001b[0m indices_dataset \u001b[38;5;241m=\u001b[39m indices_dataset\u001b[38;5;241m.\u001b[39mflat_map(slice_batch_indices)\n\u001b[0;32m--> 355\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mslice_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshuffle_batch\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/keras/src/engine/data_adapter.py:396\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.slice_inputs\u001b[0;34m(self, indices_dataset, inputs)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgrab_batch\u001b[39m(i, data):\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[1;32m    393\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m d: tf\u001b[38;5;241m.\u001b[39mgather(d, i, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), data\n\u001b[1;32m    394\u001b[0m     )\n\u001b[0;32m--> 396\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrab_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_parallel_calls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAUTOTUNE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;66;03m# Default optimizations are disabled to avoid the overhead of\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;66;03m# (unnecessary) input pipeline graph serialization and deserialization\u001b[39;00m\n\u001b[1;32m    400\u001b[0m options \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mOptions()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/data/ops/dataset_ops.py:2280\u001b[0m, in \u001b[0;36mDatasetV2.map\u001b[0;34m(self, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[1;32m   2276\u001b[0m \u001b[38;5;66;03m# Loaded lazily due to a circular dependency (dataset_ops -> map_op ->\u001b[39;00m\n\u001b[1;32m   2277\u001b[0m \u001b[38;5;66;03m# dataset_ops).\u001b[39;00m\n\u001b[1;32m   2278\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top,protected-access\u001b[39;00m\n\u001b[1;32m   2279\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m map_op\n\u001b[0;32m-> 2280\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmap_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_v2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2281\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2282\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2283\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_parallel_calls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_parallel_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2285\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/data/ops/map_op.py:40\u001b[0m, in \u001b[0;36m_map_v2\u001b[0;34m(input_dataset, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _MapDataset(\n\u001b[1;32m     38\u001b[0m       input_dataset, map_func, preserve_cardinality\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 40\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ParallelMapDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m      \u001b[49m\u001b[43minput_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_parallel_calls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_parallel_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m      \u001b[49m\u001b[43mpreserve_cardinality\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/data/ops/map_op.py:148\u001b[0m, in \u001b[0;36m_ParallelMapDataset.__init__\u001b[0;34m(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_dataset \u001b[38;5;241m=\u001b[39m input_dataset\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_inter_op_parallelism \u001b[38;5;241m=\u001b[39m use_inter_op_parallelism\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_func \u001b[38;5;241m=\u001b[39m \u001b[43mstructured_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStructuredFunctionWrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transformation_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_legacy_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_legacy_function\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deterministic \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deterministic \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/data/ops/structured_function.py:265\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m    258\u001b[0m       warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    259\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEven though the `tf.config.experimental_run_functions_eagerly` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    260\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moption is set, this option does not apply to tf.data functions. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    261\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo force eager execution of tf.data functions, please use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    262\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.data.experimental.enable_debug_mode()`.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    263\u001b[0m     fn_factory \u001b[38;5;241m=\u001b[39m trace_tf_function(defun_kwargs)\n\u001b[0;32m--> 265\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function \u001b[38;5;241m=\u001b[39m \u001b[43mfn_factory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# There is no graph to add in eager mode.\u001b[39;00m\n\u001b[1;32m    267\u001b[0m add_to_graph \u001b[38;5;241m&\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:1227\u001b[0m, in \u001b[0;36mFunction.get_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1225\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_concrete_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1226\u001b[0m   \u001b[38;5;66;03m# Implements PolymorphicFunction.get_concrete_function.\u001b[39;00m\n\u001b[0;32m-> 1227\u001b[0m   concrete \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_concrete_function_garbage_collected\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1228\u001b[0m   concrete\u001b[38;5;241m.\u001b[39m_garbage_collector\u001b[38;5;241m.\u001b[39mrelease()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1229\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m concrete\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:1197\u001b[0m, in \u001b[0;36mFunction._get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1195\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1196\u001b[0m     initializers \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 1197\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_initializers_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitializers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1198\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_uninitialized_variables(initializers)\n\u001b[1;32m   1200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m   1201\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m   1202\u001b[0m   \u001b[38;5;66;03m# version which is guaranteed to never create variables.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:695\u001b[0m, in \u001b[0;36mFunction._initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_scoped_tracing_options(\n\u001b[1;32m    691\u001b[0m     variable_capturing_scope,\n\u001b[1;32m    692\u001b[0m     tracing_compilation\u001b[38;5;241m.\u001b[39mScopeType\u001b[38;5;241m.\u001b[39mVARIABLE_CREATION,\n\u001b[1;32m    693\u001b[0m )\n\u001b[1;32m    694\u001b[0m \u001b[38;5;66;03m# Force the definition of the function for these arguments\u001b[39;00m\n\u001b[0;32m--> 695\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concrete_variable_creation_fn \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvalid_creator_scope\u001b[39m(\u001b[38;5;241m*\u001b[39munused_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwds):\n\u001b[1;32m    700\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Disables variable creation.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:178\u001b[0m, in \u001b[0;36mtrace_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    175\u001b[0m     args \u001b[38;5;241m=\u001b[39m tracing_options\u001b[38;5;241m.\u001b[39minput_signature\n\u001b[1;32m    176\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 178\u001b[0m   concrete_function \u001b[38;5;241m=\u001b[39m \u001b[43m_maybe_define_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracing_options\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mbind_graph_to_function:\n\u001b[1;32m    183\u001b[0m   concrete_function\u001b[38;5;241m.\u001b[39m_garbage_collector\u001b[38;5;241m.\u001b[39mrelease()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:283\u001b[0m, in \u001b[0;36m_maybe_define_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    282\u001b[0m   target_func_type \u001b[38;5;241m=\u001b[39m lookup_func_type\n\u001b[0;32m--> 283\u001b[0m concrete_function \u001b[38;5;241m=\u001b[39m \u001b[43m_create_concrete_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_func_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlookup_func_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracing_options\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mfunction_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    288\u001b[0m   tracing_options\u001b[38;5;241m.\u001b[39mfunction_cache\u001b[38;5;241m.\u001b[39madd(\n\u001b[1;32m    289\u001b[0m       concrete_function, current_func_context\n\u001b[1;32m    290\u001b[0m   )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:310\u001b[0m, in \u001b[0;36m_create_concrete_function\u001b[0;34m(function_type, type_context, func_graph, tracing_options)\u001b[0m\n\u001b[1;32m    303\u001b[0m   placeholder_bound_args \u001b[38;5;241m=\u001b[39m function_type\u001b[38;5;241m.\u001b[39mplaceholder_arguments(\n\u001b[1;32m    304\u001b[0m       placeholder_context\n\u001b[1;32m    305\u001b[0m   )\n\u001b[1;32m    307\u001b[0m disable_acd \u001b[38;5;241m=\u001b[39m tracing_options\u001b[38;5;241m.\u001b[39mattributes \u001b[38;5;129;01mand\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mattributes\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m    308\u001b[0m     attributes_lib\u001b[38;5;241m.\u001b[39mDISABLE_ACD, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    309\u001b[0m )\n\u001b[0;32m--> 310\u001b[0m traced_func_graph \u001b[38;5;241m=\u001b[39m \u001b[43mfunc_graph_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc_graph_from_py_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtracing_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtracing_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpython_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplaceholder_bound_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplaceholder_bound_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_control_dependencies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdisable_acd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43marg_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction_type_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_arg_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_placeholders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    322\u001b[0m transform\u001b[38;5;241m.\u001b[39mapply_func_graph_transforms(traced_func_graph)\n\u001b[1;32m    324\u001b[0m graph_capture_container \u001b[38;5;241m=\u001b[39m traced_func_graph\u001b[38;5;241m.\u001b[39mfunction_captures\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/framework/func_graph.py:1059\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders)\u001b[0m\n\u001b[1;32m   1056\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[1;32m   1058\u001b[0m _, original_func \u001b[38;5;241m=\u001b[39m tf_decorator\u001b[38;5;241m.\u001b[39munwrap(python_func)\n\u001b[0;32m-> 1059\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mpython_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunc_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunc_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;66;03m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;66;03m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m variable_utils\u001b[38;5;241m.\u001b[39mconvert_variables_to_tensors(func_outputs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:598\u001b[0m, in \u001b[0;36mFunction._generate_scoped_tracing_options.<locals>.wrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m default_graph\u001b[38;5;241m.\u001b[39m_variable_creator_scope(scope, priority\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m):  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    595\u001b[0m   \u001b[38;5;66;03m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[39;00m\n\u001b[1;32m    596\u001b[0m   \u001b[38;5;66;03m# the function a weak reference to itself to avoid a reference cycle.\u001b[39;00m\n\u001b[1;32m    597\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(compile_with_xla):\n\u001b[0;32m--> 598\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mweak_wrapped_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__wrapped__\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    599\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/data/ops/structured_function.py:231\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__.<locals>.trace_tf_function.<locals>.wrapped_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_fn\u001b[39m(\u001b[38;5;241m*\u001b[39margs):  \u001b[38;5;66;03m# pylint: disable=missing-docstring\u001b[39;00m\n\u001b[0;32m--> 231\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mwrapper_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m   ret \u001b[38;5;241m=\u001b[39m structure\u001b[38;5;241m.\u001b[39mto_tensor_list(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_structure, ret)\n\u001b[1;32m    233\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m [ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m ret]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/data/ops/structured_function.py:161\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__.<locals>.wrapper_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _should_unpack(nested_args):\n\u001b[1;32m    160\u001b[0m   nested_args \u001b[38;5;241m=\u001b[39m (nested_args,)\n\u001b[0;32m--> 161\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43mautograph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtf_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag_ctx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnested_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m ret \u001b[38;5;241m=\u001b[39m variable_utils\u001b[38;5;241m.\u001b[39mconvert_variables_to_tensors(ret)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _should_pack(ret):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py:690\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    689\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m conversion_ctx:\n\u001b[0;32m--> 690\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m    692\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py:377\u001b[0m, in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    374\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options)\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m options\u001b[38;5;241m.\u001b[39muser_requested \u001b[38;5;129;01mand\u001b[39;00m conversion\u001b[38;5;241m.\u001b[39mis_allowlisted(f):\n\u001b[0;32m--> 377\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_call_unconverted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;66;03m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;66;03m# call conversion from generated code while in nonrecursive mode. In that\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;66;03m# case we evidently don't want to recurse, but we still have to convert\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;66;03m# things like builtins.\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m options\u001b[38;5;241m.\u001b[39minternal_convert_user_code:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py:459\u001b[0m, in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    456\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__self__\u001b[39m\u001b[38;5;241m.\u001b[39mcall(args, kwargs)\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 459\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/keras/src/engine/data_adapter.py:392\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.slice_inputs.<locals>.grab_batch\u001b[0;34m(i, data)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgrab_batch\u001b[39m(i, data):\n\u001b[0;32m--> 392\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/util/nest.py:631\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnest.map_structure\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    546\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap_structure\u001b[39m(func, \u001b[38;5;241m*\u001b[39mstructure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    547\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a new structure by applying `func` to each atom in `structure`.\u001b[39;00m\n\u001b[1;32m    548\u001b[0m \n\u001b[1;32m    549\u001b[0m \u001b[38;5;124;03m  Refer to [tf.nest](https://www.tensorflow.org/api_docs/python/tf/nest)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;124;03m    ValueError: If wrong keyword arguments are provided.\u001b[39;00m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnest_util\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnest_util\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mModality\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCORE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstructure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/util/nest_util.py:1066\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(modality, func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    969\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates a new structure by applying `func` to each atom in `structure`.\u001b[39;00m\n\u001b[1;32m    970\u001b[0m \n\u001b[1;32m    971\u001b[0m \u001b[38;5;124;03m- For Modality.CORE: Refer to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;124;03m  ValueError: If wrong keyword arguments are provided.\u001b[39;00m\n\u001b[1;32m   1064\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1065\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m modality \u001b[38;5;241m==\u001b[39m Modality\u001b[38;5;241m.\u001b[39mCORE:\n\u001b[0;32m-> 1066\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_tf_core_map_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstructure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m modality \u001b[38;5;241m==\u001b[39m Modality\u001b[38;5;241m.\u001b[39mDATA:\n\u001b[1;32m   1068\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _tf_data_map_structure(func, \u001b[38;5;241m*\u001b[39mstructure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/util/nest_util.py:1106\u001b[0m, in \u001b[0;36m_tf_core_map_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m   1101\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (_tf_core_flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m   1102\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _tf_core_pack_sequence_as(\n\u001b[1;32m   1105\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m-> 1106\u001b[0m     \u001b[43m[\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mentries\u001b[49m\u001b[43m]\u001b[49m,\n\u001b[1;32m   1107\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites,\n\u001b[1;32m   1108\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/util/nest_util.py:1106\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1101\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (_tf_core_flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m   1102\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _tf_core_pack_sequence_as(\n\u001b[1;32m   1105\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m-> 1106\u001b[0m     [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m   1107\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites,\n\u001b[1;32m   1108\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/keras/src/engine/data_adapter.py:393\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.slice_inputs.<locals>.grab_batch.<locals>.<lambda>\u001b[0;34m(d)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgrab_batch\u001b[39m(i, data):\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mmap_structure(\n\u001b[0;32m--> 393\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m d: \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m, data\n\u001b[1;32m    394\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py:1260\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1258\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1259\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1260\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch_target\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1261\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m   1262\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1263\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1264\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/ops/array_ops.py:5251\u001b[0m, in \u001b[0;36mgather_v2\u001b[0;34m(params, indices, validate_indices, axis, batch_dims, name)\u001b[0m\n\u001b[1;32m   5243\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgather\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m   5244\u001b[0m \u001b[38;5;129m@dispatch\u001b[39m\u001b[38;5;241m.\u001b[39madd_dispatch_support\n\u001b[1;32m   5245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgather_v2\u001b[39m(params,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5249\u001b[0m               batch_dims\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   5250\u001b[0m               name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m-> 5251\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5252\u001b[0m \u001b[43m      \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5253\u001b[0m \u001b[43m      \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5254\u001b[0m \u001b[43m      \u001b[49m\u001b[43mvalidate_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5255\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5256\u001b[0m \u001b[43m      \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5257\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbatch_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_dims\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py:1260\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1258\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1259\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1260\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch_target\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1261\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m   1262\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1263\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1264\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/util/deprecation.py:588\u001b[0m, in \u001b[0;36mdeprecated_args.<locals>.deprecated_wrapper.<locals>.new_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    580\u001b[0m         _PRINTED_WARNING[(func, arg_name)] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    581\u001b[0m       _log_deprecation(\n\u001b[1;32m    582\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrom \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: calling \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m (from \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) with \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is deprecated and will \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    583\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbe removed \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mInstructions for updating:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    586\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124min a future version\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m date \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m date),\n\u001b[1;32m    587\u001b[0m           instructions)\n\u001b[0;32m--> 588\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/ops/array_ops.py:5240\u001b[0m, in \u001b[0;36mgather\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   5238\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m params\u001b[38;5;241m.\u001b[39msparse_read(indices, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[1;32m   5239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m-> 5240\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgen_array_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather_v2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/ops/gen_array_ops.py:4068\u001b[0m, in \u001b[0;36mgather_v2\u001b[0;34m(params, indices, axis, batch_dims, name)\u001b[0m\n\u001b[1;32m   4066\u001b[0m   batch_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   4067\u001b[0m batch_dims \u001b[38;5;241m=\u001b[39m _execute\u001b[38;5;241m.\u001b[39mmake_int(batch_dims, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_dims\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 4068\u001b[0m _, _, _op, _outputs \u001b[38;5;241m=\u001b[39m \u001b[43m_op_def_library\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_op_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4069\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGatherV2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4070\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mbatch_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4071\u001b[0m _result \u001b[38;5;241m=\u001b[39m _outputs[:]\n\u001b[1;32m   4072\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _execute\u001b[38;5;241m.\u001b[39mmust_record_gradient():\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/framework/op_def_library.py:778\u001b[0m, in \u001b[0;36m_apply_op_helper\u001b[0;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m g\u001b[38;5;241m.\u001b[39mas_default(), ops\u001b[38;5;241m.\u001b[39mname_scope(name) \u001b[38;5;28;01mas\u001b[39;00m scope:\n\u001b[1;32m    777\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m fallback:\n\u001b[0;32m--> 778\u001b[0m     \u001b[43m_ExtractInputsAndAttrs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop_type_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_def\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallowed_list_attr_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mkeywords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_type_attr_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    780\u001b[0m \u001b[43m                           \u001b[49m\u001b[43minput_types\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m     _ExtractRemainingAttrs(op_type_name, op_def, keywords,\n\u001b[1;32m    782\u001b[0m                            default_type_attr_map, attrs)\n\u001b[1;32m    783\u001b[0m     _ExtractAttrProto(op_type_name, op_def, attrs, attr_protos)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/framework/op_def_library.py:551\u001b[0m, in \u001b[0;36m_ExtractInputsAndAttrs\u001b[0;34m(op_type_name, op_def, allowed_list_attr_map, keywords, default_type_attr_map, attrs, inputs, input_types)\u001b[0m\n\u001b[1;32m    545\u001b[0m       values \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(\n\u001b[1;32m    546\u001b[0m           values,\n\u001b[1;32m    547\u001b[0m           name\u001b[38;5;241m=\u001b[39minput_arg\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    548\u001b[0m           as_ref\u001b[38;5;241m=\u001b[39minput_arg\u001b[38;5;241m.\u001b[39mis_ref,\n\u001b[1;32m    549\u001b[0m           preferred_dtype\u001b[38;5;241m=\u001b[39mdefault_dtype)\n\u001b[1;32m    550\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 551\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_arg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m        \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_arg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_ref\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreferred_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    558\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/profiler/trace.py:183\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m Trace(trace_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrace_kwargs):\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:696\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m    694\u001b[0m \u001b[38;5;66;03m# TODO(b/142518781): Fix all call-sites and remove redundant arg\u001b[39;00m\n\u001b[1;32m    695\u001b[0m preferred_dtype \u001b[38;5;241m=\u001b[39m preferred_dtype \u001b[38;5;129;01mor\u001b[39;00m dtype_hint\n\u001b[0;32m--> 696\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor_conversion_registry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreferred_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccepted_result_types\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/framework/tensor_conversion_registry.py:234\u001b[0m, in \u001b[0;36mconvert\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[0m\n\u001b[1;32m    225\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    226\u001b[0m           _add_error_prefix(\n\u001b[1;32m    227\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConversion function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconversion_func\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m for type \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactual = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mret\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mbase_dtype\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    231\u001b[0m               name\u001b[38;5;241m=\u001b[39mname))\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 234\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mconversion_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_ref\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m    237\u001b[0m   \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py:335\u001b[0m, in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_constant_tensor_conversion_function\u001b[39m(v, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    333\u001b[0m                                          as_ref\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    334\u001b[0m   _ \u001b[38;5;241m=\u001b[39m as_ref\n\u001b[0;32m--> 335\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/ops/weak_tensor_ops.py:142\u001b[0m, in \u001b[0;36mweak_tensor_binary_op_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    141\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mis_auto_dtype_conversion_enabled():\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m   bound_arguments \u001b[38;5;241m=\u001b[39m signature\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    144\u001b[0m   bound_arguments\u001b[38;5;241m.\u001b[39mapply_defaults()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py:271\u001b[0m, in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconstant\u001b[39m(\n\u001b[1;32m    174\u001b[0m     value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConst\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    175\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[ops\u001b[38;5;241m.\u001b[39mOperation, ops\u001b[38;5;241m.\u001b[39m_EagerTensorBase]:\n\u001b[1;32m    176\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;124;03m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 271\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mallow_broadcast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py:286\u001b[0m, in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    283\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[1;32m    284\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[0;32m--> 286\u001b[0m const_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_graph_constant\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    287\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_broadcast\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m const_tensor\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:268\u001b[0m, in \u001b[0;36m_create_graph_constant\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    266\u001b[0m dtype_value \u001b[38;5;241m=\u001b[39m attr_value_pb2\u001b[38;5;241m.\u001b[39mAttrValue(\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39mtensor_value\u001b[38;5;241m.\u001b[39mtensor\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    267\u001b[0m attrs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: tensor_value, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: dtype_value}\n\u001b[0;32m--> 268\u001b[0m const_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_op_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mConst\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mdtype_value\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39moutputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m op_callbacks\u001b[38;5;241m.\u001b[39mshould_invoke_op_callbacks():\n\u001b[1;32m    272\u001b[0m   \u001b[38;5;66;03m# TODO(b/147670703): Once the special-op creation code paths\u001b[39;00m\n\u001b[1;32m    273\u001b[0m   \u001b[38;5;66;03m# are unified. Remove this `if` block.\u001b[39;00m\n\u001b[1;32m    274\u001b[0m   callback_outputs \u001b[38;5;241m=\u001b[39m op_callbacks\u001b[38;5;241m.\u001b[39minvoke_op_callbacks(\n\u001b[1;32m    275\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConst\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mtuple\u001b[39m(), attrs, (const_tensor,), op_name\u001b[38;5;241m=\u001b[39mname, graph\u001b[38;5;241m=\u001b[39mg)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/framework/func_graph.py:670\u001b[0m, in \u001b[0;36mFuncGraph._create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m    668\u001b[0m   inp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcapture(inp)\n\u001b[1;32m    669\u001b[0m   captured_inputs\u001b[38;5;241m.\u001b[39mappend(inp)\n\u001b[0;32m--> 670\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_op_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mop_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_types\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_def\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_device\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:2652\u001b[0m, in \u001b[0;36mGraph._create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m   2649\u001b[0m \u001b[38;5;66;03m# _create_op_helper mutates the new Operation. `_mutation_lock` ensures a\u001b[39;00m\n\u001b[1;32m   2650\u001b[0m \u001b[38;5;66;03m# Session.run call cannot occur between creating and mutating the op.\u001b[39;00m\n\u001b[1;32m   2651\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mutation_lock():\n\u001b[0;32m-> 2652\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mOperation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_node_def\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2653\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnode_def\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2654\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2655\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2656\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2657\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcontrol_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontrol_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2658\u001b[0m \u001b[43m      \u001b[49m\u001b[43minput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2659\u001b[0m \u001b[43m      \u001b[49m\u001b[43moriginal_op\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_default_original_op\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2660\u001b[0m \u001b[43m      \u001b[49m\u001b[43mop_def\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mop_def\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2661\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2662\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_op_helper(ret, compute_device\u001b[38;5;241m=\u001b[39mcompute_device)\n\u001b[1;32m   2663\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:1160\u001b[0m, in \u001b[0;36mOperation.from_node_def\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1157\u001b[0m     control_input_ops\u001b[38;5;241m.\u001b[39mappend(control_op)\n\u001b[1;32m   1159\u001b[0m \u001b[38;5;66;03m# Initialize c_op from node_def and other inputs\u001b[39;00m\n\u001b[0;32m-> 1160\u001b[0m c_op \u001b[38;5;241m=\u001b[39m \u001b[43m_create_c_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_def\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontrol_input_ops\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_def\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mop_def\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m Operation(c_op, SymbolicTensor)\n\u001b[1;32m   1162\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init(g)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:1026\u001b[0m, in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs, op_def, extract_traceback)\u001b[0m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;66;03m# Record the current Python stack trace as the creating stacktrace of this\u001b[39;00m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;66;03m# TF_Operation.\u001b[39;00m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extract_traceback:\n\u001b[1;32m   1025\u001b[0m   pywrap_tf_session\u001b[38;5;241m.\u001b[39mTF_SetOpStackTrace(\n\u001b[0;32m-> 1026\u001b[0m       c_op, \u001b[43mtf_stack\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstacklevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1027\u001b[0m   )\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m c_op\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/tensorflow/python/util/tf_stack.py:162\u001b[0m, in \u001b[0;36mextract_stack\u001b[0;34m(stacklevel)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"An eager-friendly alternative to traceback.extract_stack.\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \n\u001b[1;32m    153\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;124;03m  line, meant to masquerade as traceback.FrameSummary objects.\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    161\u001b[0m thread_key \u001b[38;5;241m=\u001b[39m _get_thread_key()\n\u001b[0;32m--> 162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_tf_stack\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_stack\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_source_mapper_stacks\u001b[49m\u001b[43m[\u001b[49m\u001b[43mthread_key\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minternal_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_source_filter_stacks\u001b[49m\u001b[43m[\u001b[49m\u001b[43mthread_key\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minternal_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstacklevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Evaluate on training data\n",
    "evaluate_mse(train_data, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc4fc43197afaf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-03T06:16:59.708670Z",
     "start_time": "2025-05-03T06:15:13.300547Z"
    },
    "id": "bbc4fc43197afaf"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def generate_submission(data, output_csv, Tobs=50, Tpred=60):\n",
    "    \"\"\"\n",
    "    Applies forecasting and generates a submission CSV with format:\n",
    "    index,x,y where index is auto-generated and matches submission key.\n",
    "\n",
    "    Args:\n",
    "        data (np.ndarray): Test data of shape (num_scenarios, 50, 50, 6).\n",
    "        output_csv (str): Output CSV file path.\n",
    "        Tobs (int): Observed time steps (default 50).\n",
    "        Tpred (int): Prediction time steps (default 60).\n",
    "    \"\"\"\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    for i in range(data.shape[0]):\n",
    "        scenario_data = data[i]            # Shape: (50, 50, 6)\n",
    "        ego_agent_data = scenario_data[0]  # Shape: (50, 6)\n",
    "\n",
    "        # Predict future positions for the ego agent\n",
    "        predicted_positions = forecast_positions(\n",
    "            ego_agent_data[np.newaxis, :, :], Tobs, Tpred, model, X_mean, X_std, y_mean, y_std\n",
    "        )  # Shape: (1, 60, 2)\n",
    "\n",
    "        # Append 60 predictions (x, y) for this scenario\n",
    "        predictions.extend(predicted_positions[0])  # Shape: (60, 2)\n",
    "\n",
    "    # Create DataFrame without explicit ID\n",
    "    submission_df = pd.DataFrame(predictions, columns=[\"x\", \"y\"])\n",
    "    submission_df.index.name = 'index'  # Match Kaggle format\n",
    "\n",
    "    # Save CSV with index\n",
    "    submission_df.to_csv(output_csv)\n",
    "    print(f\"Submission file '{output_csv}' saved with shape {submission_df.shape}\")\n",
    "\n",
    "generate_submission(test_data, 'lstm_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a6d7a17d8677e1",
   "metadata": {
    "id": "94a6d7a17d8677e1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
