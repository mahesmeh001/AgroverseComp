{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-25T20:46:20.280858Z",
     "start_time": "2025-05-25T20:46:19.012488Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data's shape (10000, 50, 110, 6)\n",
      "test_data's shape (2100, 50, 50, 6)\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "import numpy as np\n",
    "train_file = np.load('data/train.npz')\n",
    "train_data = train_file['data']\n",
    "print(\"train_data's shape\", train_data.shape)\n",
    "test_file = np.load('data/test_input.npz')\n",
    "test_data = test_file['data']\n",
    "print(\"test_data's shape\", test_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input, RepeatVector, TimeDistributed, Dropout"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-25T20:46:28.036663Z",
     "start_time": "2025-05-25T20:46:28.030418Z"
    }
   },
   "id": "db97d754762b390b",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_model(model, filepath='lstm_single_step.pkl'):\n",
    "    \"\"\"Save model and scaler together in a pickle file\"\"\"\n",
    "    model_json = model.to_json()\n",
    "    model_weights = model.get_weights()\n",
    "    data = {\n",
    "        'model_json': model_json,\n",
    "        'model_weights': model_weights,\n",
    "    }\n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    print(f\"Model saved to {filepath}\")\n",
    "\n",
    "def load_model(filepath='lstm_single_step.pkl'):\n",
    "    \"\"\"Load model and scaler from pickle file\"\"\"\n",
    "    with open(filepath, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    # Reconstruct model\n",
    "    model = tf.keras.models.model_from_json(data['model_json'])\n",
    "    model.set_weights(data['model_weights'])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-25T20:46:42.842455Z",
     "start_time": "2025-05-25T20:46:42.829063Z"
    }
   },
   "id": "e3d667b335d4a0a",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import (\n",
    "    Input, LSTM, Dense, Dropout, TimeDistributed, Attention, Concatenate,\n",
    "    RepeatVector\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def create_lstm_encoder_decoder(\n",
    "    input_dim, output_dim, timesteps_in, timesteps_out,\n",
    "    lstm_units=512, num_layers=2, loss_fn='mse', lr=0.001\n",
    "):\n",
    "    # --- Input and Learnable Feature Weighting ---\n",
    "    encoder_inputs = Input(shape=(timesteps_in, input_dim))  # shape: (batch, Tobs, 6)\n",
    "    weighted_inputs = TimeDistributed(Dense(input_dim, activation=None))(encoder_inputs)\n",
    "\n",
    "    # --- Encoder ---\n",
    "    x = weighted_inputs\n",
    "    for _ in range(num_layers):\n",
    "        x = LSTM(lstm_units, return_sequences=True)(x)\n",
    "    encoder_outputs = x  # shape: (batch, Tobs, lstm_units)\n",
    "\n",
    "    # --- Decoder ---\n",
    "    decoder_input = RepeatVector(timesteps_out)(encoder_outputs[:, -1, :])\n",
    "    decoder_outputs = decoder_input\n",
    "    for _ in range(num_layers):\n",
    "        decoder_outputs = LSTM(lstm_units, return_sequences=True)(decoder_outputs)\n",
    "\n",
    "    # --- Attention ---\n",
    "    # attention = Attention()([decoder_outputs, encoder_outputs])\n",
    "    # x = Concatenate()([decoder_outputs, attention])\n",
    "\n",
    "    # --- Dense Layers ---\n",
    "    x = TimeDistributed(Dense(128, activation='relu'))(x)\n",
    "    x = TimeDistributed(Dense(64, activation='relu'))(x)\n",
    "    outputs = TimeDistributed(Dense(output_dim))(x)\n",
    "\n",
    "    model = Model(encoder_inputs, outputs)\n",
    "    model.compile(optimizer=Adam(learning_rate=lr, clipnorm=.5), loss=loss_fn, metrics=['mae'])\n",
    "\n",
    "    return model\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-25T20:46:43.765519Z",
     "start_time": "2025-05-25T20:46:43.755463Z"
    }
   },
   "id": "d7f20fc814e583a3",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices('GPU')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-25T20:46:45.703338Z",
     "start_time": "2025-05-25T20:46:45.696154Z"
    }
   },
   "id": "21f1db9fa58aef3",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from keras.src.callbacks import LearningRateScheduler, EarlyStopping, Callback\n",
    "from keras.src.optimizers import Adam\n",
    "from keras import Model\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def exponential_decay_schedule(epoch, lr):\n",
    "    decay_rate = 0.1\n",
    "    decay_steps = 5\n",
    "    if epoch % decay_steps == 0 and epoch:\n",
    "        print('Learning rate update:', lr * decay_rate)\n",
    "        return lr * decay_rate\n",
    "    return lr\n",
    "\n",
    "\n",
    "# Custom callback to monitor LR and stop training\n",
    "class LRThresholdCallback(Callback):\n",
    "    def __init__(self, threshold=9e-5):\n",
    "        super().__init__()\n",
    "        self.threshold = threshold\n",
    "        self.should_stop = False\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        lr = float(self.model.optimizer.learning_rate.numpy())\n",
    "        if lr < self.threshold:\n",
    "            print(f\"\\nLearning rate {lr:.6f} < threshold {self.threshold}, moving to Phase 2.\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_single_step_model(input_dim, timesteps_in, lstm_units=512, num_layers=2, loss_fn='mse', lr=0.001):\n",
    "    \"\"\"\n",
    "    Simplified model for single-step prediction.\n",
    "    Input: (batch, timesteps, features)\n",
    "    Output: (batch, 2) - single delta prediction\n",
    "    \"\"\"\n",
    "    \n",
    "    # Input\n",
    "    inputs = Input(shape=(timesteps_in, input_dim))\n",
    "    \n",
    "    # LSTM layers\n",
    "    x = inputs\n",
    "    for i in range(num_layers):\n",
    "        return_sequences = (i < num_layers - 1)  # Return sequences for all but last layer\n",
    "        x = LSTM(lstm_units, return_sequences=return_sequences, dropout=0.2)(x)\n",
    "    \n",
    "    # Dense layers for final prediction\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    outputs = Dense(2)(x)  # Predict single delta (x, y)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(learning_rate=lr), loss=loss_fn, metrics=['mae'])\n",
    "    \n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-25T20:46:47.512851Z",
     "start_time": "2025-05-25T20:46:47.506629Z"
    }
   },
   "id": "4d68034c165292c5",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "class GradientMonitoringCallback(Callback):\n",
    "    def __init__(self, clip_min=1e-4, clip_max=1e2, monitor_frequency=3):\n",
    "        \"\"\"\n",
    "        Monitor gradient norms during training\n",
    "        \n",
    "        Args:\n",
    "            clip_min: Minimum threshold for gradient norms\n",
    "            clip_max: Maximum threshold for gradient norms  \n",
    "            monitor_frequency: How often to check gradients (every N batches)\n",
    "        \"\"\"\n",
    "        print(f\"ğŸ”§ GradientMonitoringCallback initialized with clip_min={clip_min}, clip_max={clip_max}, monitor_freq={monitor_frequency}\")\n",
    "        self.clip_min = clip_min\n",
    "        self.clip_max = clip_max\n",
    "        self.monitor_frequency = monitor_frequency\n",
    "        self.batch_count = 0\n",
    "        self.total_calls = 0\n",
    "        self.gradient_checks = 0\n",
    "        self.fallback_calls = 0\n",
    "        \n",
    "    def on_train_begin(self, logs=None):\n",
    "        print(\"ğŸš€ GradientMonitoringCallback: Training started!\")\n",
    "        self.batch_count = 0\n",
    "        self.total_calls = 0\n",
    "        self.gradient_checks = 0\n",
    "        self.fallback_calls = 0\n",
    "        \n",
    "    # def on_epoch_begin(self, epoch, logs=None):\n",
    "    #     print(f\"ğŸ“ GradientMonitoringCallback: Starting epoch {epoch + 1}\")\n",
    "        \n",
    "    # def on_train_batch_begin(self, batch, logs=None):\n",
    "    #     # Just to prove we're being called\n",
    "    #     if batch % 50 == 0:  # Print every 50 batches to avoid spam\n",
    "    #         print(f\"âš¡ GradientMonitoringCallback: Batch {batch} starting\")\n",
    "        \n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        self.batch_count += 1\n",
    "        self.total_calls += 1\n",
    "        \n",
    "        # Print every time to show we're being called\n",
    "        # if batch % 50 == 0:  # Print every 50 batches\n",
    "            # print(f\"ğŸ“Š GradientMonitoringCallback: Batch {batch} ended (total calls: {self.total_calls})\")\n",
    "        \n",
    "        # Only monitor every N batches to avoid performance overhead\n",
    "        if self.batch_count % self.monitor_frequency != 0:\n",
    "            return\n",
    "            \n",
    "        # print(f\"ğŸ” GradientMonitoringCallback: Checking gradients at batch {batch} (check #{self.gradient_checks + 1})\")\n",
    "        \n",
    "        # Get gradients from the optimizer's current state\n",
    "        try:\n",
    "            # Access the model's optimizer to get gradient information\n",
    "            optimizer = self.model.optimizer\n",
    "            print(f\"   ğŸ“‹ Optimizer type: {type(optimizer).__name__}\")\n",
    "            \n",
    "            # Get trainable variables\n",
    "            trainable_vars = self.model.trainable_variables\n",
    "            print(f\"   ğŸ“ˆ Number of trainable variables: {len(trainable_vars)}\")\n",
    "            \n",
    "            if hasattr(optimizer, 'get_gradients'):\n",
    "                print(\"   âœ… Optimizer has get_gradients method\")\n",
    "                # For some optimizers, we can access gradients directly\n",
    "                grads = optimizer.get_gradients(self.model.total_loss, trainable_vars)\n",
    "                print(f\"   ğŸ“Š Retrieved {len([g for g in grads if g is not None])} gradients\")\n",
    "            else:\n",
    "                print(\"   âŒ Optimizer doesn't have get_gradients, using variable norms\")\n",
    "                # Alternative approach: check the current variable states\n",
    "                grad_norms = []\n",
    "                for i, var in enumerate(trainable_vars):\n",
    "                    if var is not None:\n",
    "                        var_norm = tf.norm(var)\n",
    "                        grad_norms.append(var_norm)\n",
    "                        if i < 3:  # Print first 3 for debugging\n",
    "                            print(f\"      Variable {i} norm: {float(var_norm.numpy()):.2e}\")\n",
    "                \n",
    "                self._check_norms(grad_norms, \"Variable\")\n",
    "                self.gradient_checks += 1\n",
    "                return\n",
    "                \n",
    "            # Compute gradient norms\n",
    "            grad_norms = []\n",
    "            for i, grad in enumerate(grads):\n",
    "                if grad is not None:\n",
    "                    grad_norm = tf.norm(grad)\n",
    "                    grad_norms.append(grad_norm)\n",
    "                    if i < 3:  # Print first 3 for debugging\n",
    "                        print(f\"      Gradient {i} norm: {float(grad_norm.numpy()):.2e}\")\n",
    "                    \n",
    "            print(f\"   âœ… Computed {len(grad_norms)} gradient norms\")\n",
    "            self._check_norms(grad_norms, \"Gradient\")\n",
    "            self.gradient_checks += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   âŒ Exception in gradient monitoring: {str(e)}\")\n",
    "            self.fallback_calls += 1\n",
    "            # Fallback: just monitor the loss for signs of instability\n",
    "            print('   ğŸ”„ Fallback: monitoring loss only')\n",
    "            if logs:\n",
    "                loss_value = logs.get('loss', 0)\n",
    "                print(f\"   ğŸ“‰ Current loss: {loss_value:.2e}\")\n",
    "                if np.isnan(loss_value) or np.isinf(loss_value):\n",
    "                    print(f\"   âš ï¸  WARNING: Loss became {loss_value} at batch {batch}\")\n",
    "                elif loss_value > 1e6:\n",
    "                    print(f\"   âš ï¸  WARNING: Very large loss {loss_value:.2e} at batch {batch}\")\n",
    "    \n",
    "    def _check_norms(self, norms, norm_type=\"Gradient\"):\n",
    "        \"\"\"Check if norms are within acceptable range\"\"\"\n",
    "        print(f\"   ğŸ”¬ Checking {len(norms)} {norm_type.lower()} norms...\")\n",
    "        warnings = 0\n",
    "        \n",
    "        for idx, norm in enumerate(norms):\n",
    "            try:\n",
    "                norm_value = float(norm.numpy()) if hasattr(norm, 'numpy') else float(norm)\n",
    "                \n",
    "                if norm_value > self.clip_max:\n",
    "                    print(f\"   âš ï¸  WARNING: {norm_type} norm {norm_value:.2e} is too large (layer {idx})\")\n",
    "                    warnings += 1\n",
    "                elif norm_value < self.clip_min:\n",
    "                    print(f\"   âš ï¸  WARNING: {norm_type} norm {norm_value:.2e} is too small (layer {idx})\")\n",
    "                    warnings += 1\n",
    "                elif np.isnan(norm_value) or np.isinf(norm_value):\n",
    "                    print(f\"   âš ï¸  WARNING: {norm_type} norm is {norm_value} (layer {idx})\")\n",
    "                    warnings += 1\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"   âŒ Cannot convert norm to float for layer {idx}: {str(e)}\")\n",
    "                continue\n",
    "                \n",
    "        if warnings == 0:\n",
    "            print(f\"   âœ… All {norm_type.lower()} norms are within acceptable range\")\n",
    "        else:\n",
    "            print(f\"   âš ï¸  Found {warnings} norm warnings\")\n",
    "    \n",
    "    # def on_epoch_end(self, epoch, logs=None):\n",
    "    #     \"\"\"Print summary at end of each epoch\"\"\"\n",
    "    #     print(f\"ğŸ“ˆ GradientMonitoringCallback: Epoch {epoch + 1} completed\")\n",
    "    #     print(f\"   ğŸ“Š Total batch calls: {self.total_calls}\")\n",
    "    #     print(f\"   ğŸ” Gradient checks performed: {self.gradient_checks}\")\n",
    "    #     print(f\"   ğŸ”„ Fallback calls: {self.fallback_calls}\")\n",
    "    #     \n",
    "    #     if logs:\n",
    "    #         loss = logs.get('loss', 0)\n",
    "    #         val_loss = logs.get('val_loss', 0)\n",
    "    #         print(f\"   ğŸ“‰ Final epoch loss: {loss:.2e}\")\n",
    "    #         if val_loss:\n",
    "    #             print(f\"   ğŸ“‰ Final epoch val_loss: {val_loss:.2e}\")\n",
    "    #         \n",
    "    #         if np.isnan(loss) or np.isinf(loss):\n",
    "    #             print(f\"   âš ï¸  WARNING: Training loss became unstable: {loss}\")\n",
    "    #         if val_loss and (np.isnan(val_loss) or np.isinf(val_loss)):\n",
    "    #             print(f\"   âš ï¸  WARNING: Validation loss became unstable: {val_loss}\")\n",
    "        \n",
    "    def on_train_end(self, logs=None):\n",
    "        print(\"ğŸ GradientMonitoringCallback: Training completed!\")\n",
    "        print(f\"   ğŸ“Š Final stats - Total calls: {self.total_calls}, Gradient checks: {self.gradient_checks}, Fallbacks: {self.fallback_calls}\")\n",
    "        \n",
    "        if self.total_calls == 0:\n",
    "            print(\"   âŒ ERROR: Callback was never called! Check if it's properly added to callbacks list.\")\n",
    "        elif self.gradient_checks == 0 and self.fallback_calls == 0:\n",
    "            print(\"   âš ï¸  WARNING: No gradient monitoring was performed. Check monitor_frequency setting.\")\n",
    "        else:\n",
    "            print(\"   âœ… Gradient monitoring completed successfully!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-25T20:46:48.653497Z",
     "start_time": "2025-05-25T20:46:48.639888Z"
    }
   },
   "id": "8fc55c9e1f03b71e",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "class SaveBestModelCallback(Callback):\n",
    "    def __init__(self, save_path='best_model.keras', monitor='val_loss'):\n",
    "        super().__init__()\n",
    "        self.best = float('inf')\n",
    "        self.monitor = monitor\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current = logs.get(self.monitor)\n",
    "        if current is not None and current < self.best:\n",
    "            self.best = current\n",
    "            print(f\"\\nNew best {self.monitor}: {current:.6f}. Saving model...\")\n",
    "            save_model(self.model, 'lstm_single_step.pkl')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-25T20:47:04.751244Z",
     "start_time": "2025-05-25T20:47:04.743996Z"
    }
   },
   "id": "2ef8679e5c2bcf5b",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train_model(train_data, batch_size=32, validation_split=0.2, Tobs=50, epochs=50, epochs2=10):\n",
    "    n_scenarios, n_agents, T, D = train_data.shape  # Expecting (10000, 50, 110, 6)\n",
    "    assert D == 6, \"Expected 6 features per timestep (position, velocity, heading, object_type).\"\n",
    "\n",
    "    X_train_raw, y_train_deltas = [], []\n",
    "    \n",
    "    \n",
    "    pruned_fully_padded = 0\n",
    "    pruned_invalid_positions = 0\n",
    "    pruned_too_many_zeros = 0\n",
    "    valid_samples_count = 0  # To count how many valid samples are being used\n",
    "    \n",
    "    for i in range(n_scenarios):\n",
    "        for j in range(n_agents):\n",
    "            agent_data = train_data[i, j, :, :]  # shape (110, 6)\n",
    "    \n",
    "            if np.all(agent_data == 0):\n",
    "                pruned_fully_padded += 1\n",
    "                continue  # skip fully padded agent\n",
    "    \n",
    "            # Create sliding window samples for single-step prediction\n",
    "            for t in range(Tobs, T - 1):  # From timestep 50 to 108 (inclusive)\n",
    "                # Input: previous Tobs timesteps (e.g., timesteps 0-49, 1-50, 2-51, etc.)\n",
    "                input_window = agent_data[t-Tobs:t, :]  # shape (50, 6)\n",
    "                \n",
    "                # Target: delta from current position to next position\n",
    "                current_pos = agent_data[t, :2]      # shape (2,)\n",
    "                next_pos = agent_data[t+1, :2]       # shape (2,)\n",
    "                delta = next_pos - current_pos       # shape (2,)\n",
    "    \n",
    "                # Skip only if current or next positions are invalid, or if too many zeros in window\n",
    "                if np.all(current_pos == 0) or np.all(next_pos == 0):\n",
    "                    pruned_invalid_positions += 1\n",
    "                    continue\n",
    "                \n",
    "                # Allow some zeros in the window, but not too many (e.g., less than 80% valid)\n",
    "                valid_positions = ~np.all(input_window[:, :2] == 0, axis=1)\n",
    "                if np.sum(valid_positions) < 0.6 * Tobs:  # At least 60% of timesteps must be valid\n",
    "                    pruned_too_many_zeros += 1\n",
    "                    continue\n",
    "    \n",
    "                X_train_raw.append(input_window)     # shape (50, 6)\n",
    "                y_train_deltas.append(delta)         # shape (2,)\n",
    "                valid_samples_count += 1  # Count valid samples\n",
    "    \n",
    "    # Final print statements to summarize pruning\n",
    "    print(f\"Total fully padded agents pruned: {pruned_fully_padded}\")\n",
    "    print(f\"Total invalid positions pruned (either current or next position was all zeros): {pruned_invalid_positions}\")\n",
    "    print(f\"Total samples pruned due to too many zeros in the window: {pruned_too_many_zeros}\")\n",
    "    print(f\"Total valid samples used for training: {valid_samples_count}\\n\")\n",
    "    \n",
    "    # Convert to numpy arrays for training\n",
    "    X_train = np.array(X_train_raw)           # shape (N, 50, 6)\n",
    "    y_train = np.array(y_train_deltas)        # shape (N, 2)\n",
    "\n",
    "    print(f\"Training on {X_train.shape[0]} valid timestep sequences.\")\n",
    "    print(f\"Input shape: {X_train.shape}, Output shape: {y_train.shape}\")\n",
    "\n",
    "    # Add debugging info\n",
    "    print(f\"Delta stats: mean={y_train.mean(axis=0)}, std={y_train.std(axis=0)}\")\n",
    "    print(f\"Delta range: min={y_train.min(axis=0)}, max={y_train.max(axis=0)}\")\n",
    "\n",
    "    # --- Normalize ---\n",
    "    X_mean = X_train.mean(axis=(0, 1), keepdims=True)  # shape: (1, 1, 6)\n",
    "    X_std = X_train.std(axis=(0, 1), keepdims=True) + 1e-8\n",
    "\n",
    "    y_mean = y_train.mean(axis=0, keepdims=True)  # shape: (1, 2)\n",
    "    y_std = y_train.std(axis=0, keepdims=True) + 1e-8\n",
    "\n",
    "    X_train = (X_train - X_mean) / X_std\n",
    "    y_train = (y_train - y_mean) / y_std\n",
    "\n",
    "    # --- Model (simplified for single-step prediction) ---\n",
    "    model = create_single_step_model(\n",
    "        input_dim=X_train.shape[-1],\n",
    "        timesteps_in=Tobs,\n",
    "        loss_fn='mse',\n",
    "        lr=0.001\n",
    "    )\n",
    "    \n",
    "    gradient_monitoring_callback = GradientMonitoringCallback(clip_min=1e-4, clip_max=1e2)\n",
    "    \n",
    "    save_best_callback = SaveBestModelCallback(save_path='lstm_single_step', monitor='val_loss')\n",
    "\n",
    "    phase1_callbacks = [\n",
    "        # LearningRateScheduler(exponential_decay_schedule),\n",
    "        EarlyStopping(patience=4, restore_best_weights=True, monitor='val_loss'),\n",
    "        LRThresholdCallback(threshold=9e-8)\n",
    "    ]\n",
    "\n",
    "    print(\"\\n--- Phase 1: Training ---\")\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=validation_split,\n",
    "        callbacks=phase1_callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- Phase 2: Fine-tuning ---\")\n",
    "    model.compile(optimizer=Adam(1e-4), loss='mse', metrics=['mae'])\n",
    "    phase2_callbacks = [\n",
    "        # LearningRateScheduler(exponential_decay_schedule),\n",
    "        EarlyStopping(patience=3, restore_best_weights=True, monitor='val_loss')\n",
    "    ]\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=epochs2,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=validation_split,\n",
    "        callbacks=phase2_callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    return model, X_mean, X_std, y_mean, y_std"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-25T20:47:26.058789Z",
     "start_time": "2025-05-25T20:47:26.056411Z"
    }
   },
   "id": "523c5a1dd20df920",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def reconstruct_absolute_positions(pred_deltas, last_observed_positions):\n",
    "    \"\"\"\n",
    "    Reconstruct absolute predicted positions by adding deltas to the last observed position.\n",
    "\n",
    "    Args:\n",
    "        pred_deltas: np.ndarray of shape (N, Tpred, 2)\n",
    "        last_observed_positions: np.ndarray of shape (N, 2)\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray of shape (N, Tpred, 2)\n",
    "    \"\"\"\n",
    "    return last_observed_positions[:, None, :] + np.cumsum(pred_deltas, axis=1)\n",
    "\n",
    "\n",
    "def forecast_positions(scenario_data, Tobs, Tpred, model, X_mean=None, X_std=None, y_mean=None, y_std=None):\n",
    "    \"\"\"\n",
    "    Use single-step LSTM model to forecast future deltas iteratively and reconstruct absolute positions.\n",
    "    \n",
    "    This function performs autoregressive prediction: uses the model to predict one step ahead,\n",
    "    then incorporates that prediction into the input window for the next prediction.\n",
    "\n",
    "    Args:\n",
    "        scenario_data (numpy.ndarray): Shape (agents, time_steps, dimensions)\n",
    "        Tobs (int): Number of observed time steps (window size for model input)\n",
    "        Tpred (int): Number of future time steps to predict\n",
    "        model (Model): Trained single-step LSTM model that predicts one normalized delta\n",
    "        X_mean, X_std: Normalization stats for input (optional)\n",
    "        y_mean, y_std: Normalization stats for output (optional)\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Predicted absolute positions of shape (agents, Tpred, 2)\n",
    "    \"\"\"\n",
    "    agents, total_timesteps, features = scenario_data.shape\n",
    "    predicted_positions = np.zeros((agents, Tpred, 2))\n",
    "\n",
    "    for agent_idx in range(agents):\n",
    "        # Get initial observation window\n",
    "        agent_data = scenario_data[agent_idx, :Tobs, :].copy()  # shape (Tobs, 6)\n",
    "\n",
    "        # Skip if fully padded\n",
    "        if np.all(agent_data == 0):\n",
    "            continue\n",
    "\n",
    "        # Initialize the sliding window with observed data\n",
    "        current_window = agent_data.copy()  # shape (Tobs, 6)\n",
    "        \n",
    "        # Store predicted deltas for this agent\n",
    "        pred_deltas = np.zeros((Tpred, 2))\n",
    "        \n",
    "        # Iteratively predict each future timestep\n",
    "        for step in range(Tpred):\n",
    "            # Prepare input for model (current sliding window)\n",
    "            X_pred = np.expand_dims(current_window, axis=0)  # shape (1, Tobs, 6)\n",
    "            \n",
    "            # Normalize input if stats are provided\n",
    "            if X_mean is not None and X_std is not None:\n",
    "                X_pred = (X_pred - X_mean) / X_std\n",
    "\n",
    "            # Predict next delta (single step)\n",
    "            pred_delta = model.predict(X_pred, verbose=0)  # shape (1, 2)\n",
    "            \n",
    "            # Denormalize delta if stats are provided\n",
    "            if y_mean is not None and y_std is not None:\n",
    "                pred_delta = pred_delta * y_std + y_mean\n",
    "                \n",
    "            pred_deltas[step] = pred_delta[0]  # Store the predicted delta\n",
    "\n",
    "            # Calculate next absolute position\n",
    "            current_pos = current_window[-1, :2]  # Last position in window\n",
    "            next_pos = current_pos + pred_delta[0]  # Add predicted delta\n",
    "            \n",
    "            # Create next timestep features\n",
    "            # Copy other features from the last timestep (velocity, heading, object_type)\n",
    "            next_timestep = current_window[-1].copy()  # shape (6,)\n",
    "            next_timestep[:2] = next_pos  # Update position\n",
    "            \n",
    "            # Update sliding window: remove oldest, add newest\n",
    "            current_window = np.roll(current_window, -1, axis=0)  # Shift left\n",
    "            current_window[-1] = next_timestep  # Add new timestep at the end\n",
    "\n",
    "        # Reconstruct absolute positions from deltas\n",
    "        last_observed_pos = agent_data[-1, :2]  # Last observed position\n",
    "        abs_positions = reconstruct_absolute_positions(\n",
    "            pred_deltas=np.expand_dims(pred_deltas, axis=0),  # shape (1, Tpred, 2)\n",
    "            last_observed_positions=np.expand_dims(last_observed_pos, axis=0)  # shape (1, 2)\n",
    "        )[0]  # shape (Tpred, 2)\n",
    "\n",
    "        predicted_positions[agent_idx] = abs_positions\n",
    "\n",
    "    return predicted_positions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-25T20:56:27.760287Z",
     "start_time": "2025-05-25T20:56:27.752629Z"
    }
   },
   "id": "4a1b62d4021903c",
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "source": [
    "funky prediciton"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c9191b70781e7bab"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_augmented_scenario_data(scenario_data, Tobs, Tpred, verbose=False):\n",
    "    \"\"\"\n",
    "    Filters agent trajectories to keep only those similar to ego in velocity profile and type,\n",
    "    and then generates sliding window samples for those agents.\n",
    "\n",
    "    Args:\n",
    "        scenario_data (np.ndarray): (num_agents, time_steps, 6)\n",
    "        Tobs (int): Observation window size\n",
    "        Tpred (int): Prediction horizon (should be 1 for delta-based prediction)\n",
    "        verbose (bool): Whether to print pruning stats\n",
    "\n",
    "    Returns:\n",
    "        X_filtered (np.ndarray): (N, Tobs, 6)\n",
    "        y_filtered (np.ndarray): (N, 2)\n",
    "    \"\"\"\n",
    "    num_agents, T, D = scenario_data.shape\n",
    "    assert D == 6, \"Expected 6 dimensions per timestep\"\n",
    "\n",
    "    ego_traj = scenario_data[0]\n",
    "    ego_type = int(ego_traj[0, 5])\n",
    "    ego_vel = ego_traj[:Tobs, 2:4]  # velocity_x, velocity_y\n",
    "\n",
    "    # Ego guaranteed valid positions for all Tobs steps\n",
    "    if np.any(np.all(ego_traj[:Tobs, :2] == 0, axis=1)):\n",
    "        raise ValueError(\"Ego agent does not have valid positions for all observed steps.\")\n",
    "\n",
    "    ego_flat_vel = ego_vel.flatten()  # shape (Tobs*2,)\n",
    "\n",
    "    X_filtered = []\n",
    "    y_filtered = []\n",
    "\n",
    "    pruned_fully_padded = 0\n",
    "    pruned_type_mismatch = 0\n",
    "    pruned_velocity_mismatch = 0\n",
    "    pruned_invalid_positions = 0\n",
    "    accepted_samples = 0\n",
    "\n",
    "    for agent_idx in range(1, num_agents):  # skip ego\n",
    "        traj = scenario_data[agent_idx]\n",
    "\n",
    "        if np.all(traj == 0):\n",
    "            pruned_fully_padded += 1\n",
    "            continue\n",
    "\n",
    "        agent_type = int(traj[0, 5])\n",
    "        if agent_type != ego_type:\n",
    "            pruned_type_mismatch += 1\n",
    "            continue\n",
    "\n",
    "        # Require agent to have valid positions for all Tobs steps\n",
    "        if np.any(np.all(traj[:Tobs, :2] == 0, axis=1)):\n",
    "            pruned_velocity_mismatch += 1\n",
    "            continue\n",
    "\n",
    "        agent_obs_vel = traj[:Tobs, 2:4]\n",
    "        agent_flat_vel = agent_obs_vel.flatten()\n",
    "\n",
    "        # Compute cosine similarity between velocity profiles\n",
    "        dot = np.dot(ego_flat_vel, agent_flat_vel)\n",
    "        norm = np.linalg.norm(ego_flat_vel) * np.linalg.norm(agent_flat_vel)\n",
    "        similarity = dot / norm if norm > 0 else -1\n",
    "\n",
    "        if similarity < 0.7:\n",
    "            pruned_velocity_mismatch += 1\n",
    "            continue\n",
    "\n",
    "        # Passed similarity filter: generate sliding window samples\n",
    "        for t in range(Tobs, T - Tpred):\n",
    "            input_window = traj[t - Tobs:t, :]  # shape (Tobs, 6)\n",
    "            current_pos = traj[t, :2]\n",
    "            next_pos = traj[t + Tpred, :2]\n",
    "            delta = next_pos - current_pos\n",
    "\n",
    "            # Skip if current or next position is zero (invalid)\n",
    "            if np.all(current_pos == 0) or np.all(next_pos == 0):\n",
    "                pruned_invalid_positions += 1\n",
    "                continue\n",
    "\n",
    "            X_filtered.append(input_window)\n",
    "            y_filtered.append(delta)\n",
    "            accepted_samples += 1\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Total agents: {num_agents}\")\n",
    "        print(f\"Accepted samples: {accepted_samples}\")\n",
    "        print(f\"Pruned fully padded agents: {pruned_fully_padded}\")\n",
    "        print(f\"Pruned type mismatches: {pruned_type_mismatch}\")\n",
    "        print(f\"Pruned due to velocity mismatch: {pruned_velocity_mismatch}\")\n",
    "        print(f\"Pruned invalid positions: {pruned_invalid_positions}\")\n",
    "\n",
    "    return np.array(X_filtered), np.array(y_filtered)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-25T22:48:17.708677Z",
     "start_time": "2025-05-25T22:48:17.693786Z"
    }
   },
   "id": "c0b2fde598692275",
   "execution_count": 140
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def finetune_forecast_positions(scenario_data, Tobs, Tpred, model, \n",
    "                                 X_mean=None, X_std=None, y_mean=None, y_std=None, \n",
    "                                 epochs=3, lr=1e-4, max_repeats=5, verbose=False):\n",
    "    \"\"\"\n",
    "    Fine-tune model on relevant agents from a scenario, then forecast future positions.\n",
    "\n",
    "    Args:\n",
    "        scenario_data (np.ndarray): Array (agents, time_steps, 6)\n",
    "        Tobs (int): Number of observed time steps\n",
    "        Tpred (int): Number of prediction time steps\n",
    "        model (tf.keras.Model): LSTM model to be fine-tuned\n",
    "        X_mean, X_std, y_mean, y_std: Normalization statistics (optional)\n",
    "        epochs (int): Number of fine-tuning epochs\n",
    "        lr (float): Learning rate for fine-tuning\n",
    "        max_repeats (int): Max weight (duplication count) for close agents\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Predicted positions (agents, Tpred, 2)\n",
    "    \"\"\"\n",
    "    import copy\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "    agents, total_steps, _ = scenario_data.shape\n",
    "    assert total_steps >= Tobs + Tpred, \"Not enough time steps for observation + prediction\"\n",
    "\n",
    "    # Generate fine-tuning data with importance scaling\n",
    "    X_finetune, y_finetune = generate_augmented_scenario_data(scenario_data, Tobs, Tpred, verbose=verbose)\n",
    "\n",
    "    if len(X_finetune) == 0:\n",
    "        print(\"No valid agents found for fine-tuning.\")\n",
    "        return np.zeros((agents, Tpred, 2))\n",
    "\n",
    "    # Normalize input/output if stats provided\n",
    "    if X_mean is not None and X_std is not None:\n",
    "        X_finetune = (X_finetune - X_mean) / X_std\n",
    "    if y_mean is not None and y_std is not None:\n",
    "        y_finetune = (y_finetune - y_mean) / y_std\n",
    "        \n",
    "    # print(X_finetune.shape)  # (N, 50, 6)\n",
    "    # print(y_finetune.shape)  # (N, 60, 2)\n",
    "\n",
    "    # Clone model to preserve original weights\n",
    "    model_finetune = copy.deepcopy(model)\n",
    "    model_finetune.compile(optimizer=Adam(learning_rate=lr, clipnorm=0.5), loss='mse')\n",
    "    \n",
    "    gradient_monitoring_callback = GradientMonitoringCallback(clip_min=1e-4, clip_max=1e2)\n",
    "    # Train on augmented samples\n",
    "    model_finetune.fit(\n",
    "        X_finetune,\n",
    "        y_finetune,\n",
    "        epochs=epochs,\n",
    "        verbose=1,\n",
    "        # callbacks=[gradient_monitoring_callback]\n",
    "    )\n",
    "\n",
    "    # Predict for each agent in the scenario\n",
    "    predicted_positions = np.zeros((agents, Tpred, 2))\n",
    "\n",
    "    for agent_idx in range(agents):\n",
    "        agent_obs = scenario_data[agent_idx, :Tobs, :]\n",
    "\n",
    "        if np.any(np.all(agent_obs == 0, axis=1)):\n",
    "            continue\n",
    "\n",
    "        X_pred = np.expand_dims(agent_obs, axis=0)\n",
    "\n",
    "        if X_mean is not None and X_std is not None:\n",
    "            X_pred = (X_pred - X_mean) / X_std\n",
    "\n",
    "        pred_deltas = model_finetune.predict(X_pred, verbose=0)\n",
    "\n",
    "        if y_mean is not None and y_std is not None:\n",
    "            pred_deltas = pred_deltas * y_std + y_mean\n",
    "\n",
    "        last_pos = agent_obs[Tobs - 1, :2]\n",
    "        abs_positions = reconstruct_absolute_positions(\n",
    "            pred_deltas=pred_deltas,\n",
    "            last_observed_positions=np.expand_dims(last_pos, axis=0)\n",
    "        )[0]\n",
    "\n",
    "        predicted_positions[agent_idx] = abs_positions\n",
    "\n",
    "    return predicted_positions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-25T22:48:19.340591Z",
     "start_time": "2025-05-25T22:48:19.300705Z"
    }
   },
   "id": "9e2cae72fd878346",
   "execution_count": 141
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "def make_gif(data_matrix1, data_matrix2, name='comparison'):\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.animation as animation\n",
    "\n",
    "    cmap1 = plt.cm.get_cmap('viridis', 50)\n",
    "    cmap2 = plt.cm.get_cmap('plasma', 50)\n",
    "\n",
    "    assert data_matrix1.shape[1] == data_matrix2.shape[1], \"Both matrices must have same number of timesteps\"\n",
    "    timesteps = data_matrix1.shape[1]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(18, 9))\n",
    "    ax1, ax2 = axes\n",
    "\n",
    "    def update(frame):\n",
    "        for ax in axes:\n",
    "            ax.clear()\n",
    "\n",
    "        for i in range(data_matrix1.shape[0]):\n",
    "            for (data_matrix, ax, cmap) in [(data_matrix1, ax1, cmap1), (data_matrix2, ax2, cmap2)]:\n",
    "                x = data_matrix[i, frame, 0]\n",
    "                y = data_matrix[i, frame, 1]\n",
    "                if x != 0 and y != 0:\n",
    "                    xs = data_matrix[i, :frame+1, 0]\n",
    "                    ys = data_matrix[i, :frame+1, 1]\n",
    "                    mask = (xs != 0) & (ys != 0)\n",
    "                    xs = xs[mask]\n",
    "                    ys = ys[mask]\n",
    "                    if len(xs) > 0 and len(ys) > 0:\n",
    "                        color = cmap(i)\n",
    "                        ax.plot(xs, ys, alpha=0.9, color=color)\n",
    "                        ax.scatter(x, y, s=80, color=color)\n",
    "\n",
    "        # Plot ego vehicle (index 0) on both\n",
    "        ax1.plot(data_matrix1[0, :frame, 0], data_matrix1[0, :frame, 1], color='tab:orange', label='Ego Vehicle')\n",
    "        ax1.scatter(data_matrix1[0, frame, 0], data_matrix1[0, frame, 1], s=80, color='tab:orange')\n",
    "        ax1.set_title('Prediction')\n",
    "\n",
    "        ax2.plot(data_matrix2[0, :frame, 0], data_matrix2[0, :frame, 1], color='tab:orange', label='Ego Vehicle')\n",
    "        ax2.scatter(data_matrix2[0, frame, 0], data_matrix2[0, frame, 1], s=80, color='tab:orange')\n",
    "        ax2.set_title('Actual')\n",
    "\n",
    "        for ax, data_matrix in zip(axes, [data_matrix1, data_matrix2]):\n",
    "            ax.set_xlim(data_matrix[:, :, 0][data_matrix[:, :, 0] != 0].min() - 10,\n",
    "                        data_matrix[:, :, 0][data_matrix[:, :, 0] != 0].max() + 10)\n",
    "            ax.set_ylim(data_matrix[:, :, 1][data_matrix[:, :, 1] != 0].min() - 10,\n",
    "                        data_matrix[:, :, 1][data_matrix[:, :, 1] != 0].max() + 10)\n",
    "            ax.legend()\n",
    "            ax.set_xlabel('X')\n",
    "            ax.set_ylabel('Y')\n",
    "\n",
    "        # Compute MSE over non-zero entries up to current frame\n",
    "        mask = (data_matrix2[:, :frame+1, :] != 0) & (data_matrix1[:, :frame+1, :] != 0)\n",
    "        mse = np.mean((data_matrix1[:, :frame+1, :][mask] - data_matrix2[:, :frame+1, :][mask]) ** 2)\n",
    "\n",
    "        fig.suptitle(f\"Timestep {frame} - MSE: {mse:.4f}\", fontsize=16)\n",
    "        return ax1.collections + ax1.lines + ax2.collections + ax2.lines\n",
    "\n",
    "    anim = animation.FuncAnimation(fig, update, frames=list(range(0, timesteps, 3)), interval=100, blit=True)\n",
    "    anim.save(f'trajectory_visualization_{name}.gif', writer='pillow')\n",
    "    plt.close()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-25T22:02:57.129293Z",
     "start_time": "2025-05-25T22:02:57.124795Z"
    }
   },
   "id": "658c258cac06616e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def plot_mae_by_timestep(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Visualize MAE across timesteps in the prediction horizon.\n",
    "    \n",
    "    Args:\n",
    "        y_true (np.ndarray): shape (N, Tpred, 2)\n",
    "        y_pred (np.ndarray): shape (N, Tpred, 2)\n",
    "    \"\"\"\n",
    "    mae_per_timestep = np.mean(np.abs(y_true - y_pred), axis=(0, 2))  # shape (Tpred,)\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(mae_per_timestep, label='MAE per Timestep')\n",
    "    plt.xlabel('Timestep')\n",
    "    plt.ylabel('MAE (meters)')\n",
    "    plt.title('Mean Absolute Error Over Prediction Horizon')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-25T21:38:34.243535Z",
     "start_time": "2025-05-25T21:38:34.236973Z"
    }
   },
   "id": "8ce3811f963721e2",
   "execution_count": 51
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total fully padded agents pruned: 16\n",
      "Total invalid positions pruned (either current or next position was all zeros): 4620\n",
      "Total samples pruned due to too many zeros in the window: 1251\n",
      "Total valid samples used for training: 2035\n",
      "Training on 2035 valid timestep sequences.\n",
      "Input shape: (2035, 50, 6), Output shape: (2035, 2)\n",
      "Delta stats: mean=[ 0.00797795 -0.04318982], std=[0.39617915 0.21272606]\n",
      "Delta range: min=[-1.34505657 -1.38212565], max=[0.99347377 0.43703067]\n",
      "ğŸ”§ GradientMonitoringCallback initialized with clip_min=0.0001, clip_max=100.0, monitor_freq=3\n",
      "\n",
      "--- Phase 1: Training ---\n",
      "Epoch 1/20\n",
      "\u001B[1m102/102\u001B[0m \u001B[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 39ms/step - loss: 0.4727 - mae: 0.4350\n",
      "Epoch 2/20\n",
      "\u001B[1m  3/102\u001B[0m \u001B[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[1m3s\u001B[0m 39ms/step - loss: 0.3886 - mae: 0.3789"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mehul/Documents/UCSD Classes/Deep Learning/AgroverseComp/.venv/lib/python3.9/site-packages/keras/src/callbacks/early_stopping.py:153: UserWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,mae\n",
      "  current = self.get_monitor_value(logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m102/102\u001B[0m \u001B[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 40ms/step - loss: 0.4151 - mae: 0.3894\n",
      "Epoch 3/20\n",
      "\u001B[1m102/102\u001B[0m \u001B[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 37ms/step - loss: 0.4008 - mae: 0.3854\n",
      "Epoch 4/20\n",
      "\u001B[1m102/102\u001B[0m \u001B[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 37ms/step - loss: 0.4135 - mae: 0.4322\n",
      "Epoch 5/20\n",
      "\u001B[1m102/102\u001B[0m \u001B[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 36ms/step - loss: 0.2753 - mae: 0.3326\n",
      "Epoch 6/20\n",
      "\u001B[1m102/102\u001B[0m \u001B[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 37ms/step - loss: 0.4150 - mae: 0.4423\n",
      "Epoch 7/20\n",
      "\u001B[1m102/102\u001B[0m \u001B[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 37ms/step - loss: 0.5867 - mae: 0.5059\n",
      "Epoch 8/20\n",
      "\u001B[1m102/102\u001B[0m \u001B[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 37ms/step - loss: 0.9016 - mae: 0.6404\n",
      "Epoch 9/20\n",
      "\u001B[1m102/102\u001B[0m \u001B[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 37ms/step - loss: 0.6591 - mae: 0.5507\n",
      "Epoch 10/20\n",
      "\u001B[1m102/102\u001B[0m \u001B[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 37ms/step - loss: 0.4443 - mae: 0.4606\n",
      "Epoch 11/20\n",
      "\u001B[1m102/102\u001B[0m \u001B[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 36ms/step - loss: 0.3536 - mae: 0.4139\n",
      "Epoch 12/20\n",
      "\u001B[1m102/102\u001B[0m \u001B[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 37ms/step - loss: 0.3245 - mae: 0.3835\n",
      "Epoch 13/20\n",
      "\u001B[1m102/102\u001B[0m \u001B[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 37ms/step - loss: 1.0682 - mae: 0.6774\n",
      "Epoch 14/20\n",
      "\u001B[1m102/102\u001B[0m \u001B[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 36ms/step - loss: 0.5460 - mae: 0.4733\n",
      "Epoch 15/20\n",
      "\u001B[1m102/102\u001B[0m \u001B[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 39ms/step - loss: 0.5853 - mae: 0.5239\n",
      "Epoch 16/20\n",
      "\u001B[1m102/102\u001B[0m \u001B[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 40ms/step - loss: 0.5464 - mae: 0.5089\n",
      "Epoch 17/20\n",
      "\u001B[1m102/102\u001B[0m \u001B[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 47ms/step - loss: 0.4362 - mae: 0.4401\n",
      "Epoch 18/20\n",
      "\u001B[1m102/102\u001B[0m \u001B[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 39ms/step - loss: 0.3522 - mae: 0.4092\n",
      "Epoch 19/20\n",
      "\u001B[1m102/102\u001B[0m \u001B[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 39ms/step - loss: 0.3105 - mae: 0.3473\n",
      "Epoch 20/20\n",
      "\u001B[1m102/102\u001B[0m \u001B[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 38ms/step - loss: 0.3479 - mae: 0.3899\n",
      "\n",
      "--- Phase 2: Fine-tuning ---\n"
     ]
    }
   ],
   "source": [
    "# first check that the model can overfit on small data\n",
    "model, X_mean, X_std, y_mean, y_std  = train_model(train_data[:3, ...], batch_size=20, validation_split=0, epochs=20, epochs2=0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-25T21:01:33.275119Z",
     "start_time": "2025-05-25T21:00:12.907951Z"
    }
   },
   "id": "deeaeac04573c115",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/j6ybhf8906j4bmnp171wtn680000gn/T/ipykernel_22244/3735350324.py:9: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "  cmap1 = plt.cm.get_cmap('viridis', 50)\n",
      "/var/folders/wv/j6ybhf8906j4bmnp171wtn680000gn/T/ipykernel_22244/3735350324.py:10: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "  cmap2 = plt.cm.get_cmap('plasma', 50)\n"
     ]
    }
   ],
   "source": [
    "# visualize prediction\n",
    "\n",
    "# model = load_model()\n",
    "\n",
    "# Parameters\n",
    "Tobs = 50\n",
    "Tpred = 60\n",
    "\n",
    "data = train_data[0]\n",
    "\n",
    "# Select a test scenario (can use any valid index)\n",
    "test_scenario = data.copy()  # shape (agents, time_steps, features)\n",
    "\n",
    "# Forecast future positions\n",
    "predicted_positions = forecast_positions(test_scenario, Tobs, Tpred, model, X_mean, X_std, y_mean, y_std)\n",
    "\n",
    "# Create combined matrix of past observed + predicted for ego agent (agent 0)\n",
    "ego_past = test_scenario[0, :Tobs, :2]               # shape (Tobs, 2)\n",
    "ego_future = predicted_positions[0]                  # shape (Tpred, 2)\n",
    "ego_full = np.concatenate([ego_past, ego_future], axis=0)  # shape (Tobs + Tpred, 2)\n",
    "\n",
    "# Create updated scenario with predicted ego and original others\n",
    "updated_scenario = test_scenario.copy()\n",
    "updated_scenario[0, :Tobs+Tpred, :2] = ego_full  # Replace ego trajectory\n",
    "\n",
    "# print(updated_scenario[0])\n",
    "\n",
    "# Visualize\n",
    "make_gif(updated_scenario, data, name='lstm_single_step')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-25T22:18:38.773357Z",
     "start_time": "2025-05-25T22:17:23.040730Z"
    }
   },
   "id": "fedc61e344476eb0",
   "execution_count": 103
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total agents: 50\n",
      "Accepted samples: 0\n",
      "Pruned fully padded agents: 3\n",
      "Pruned type mismatches: 0\n",
      "Pruned due to velocity mismatch: 45\n",
      "Pruned invalid positions: 0\n",
      "No valid agents found for fine-tuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/j6ybhf8906j4bmnp171wtn680000gn/T/ipykernel_22244/3735350324.py:9: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "  cmap1 = plt.cm.get_cmap('viridis', 50)\n",
      "/var/folders/wv/j6ybhf8906j4bmnp171wtn680000gn/T/ipykernel_22244/3735350324.py:10: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "  cmap2 = plt.cm.get_cmap('plasma', 50)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[142], line 29\u001B[0m\n\u001B[1;32m     24\u001B[0m updated_scenario[\u001B[38;5;241m0\u001B[39m, :Tobs\u001B[38;5;241m+\u001B[39mTpred, :\u001B[38;5;241m2\u001B[39m] \u001B[38;5;241m=\u001B[39m ego_full  \u001B[38;5;66;03m# Replace ego trajectory\u001B[39;00m\n\u001B[1;32m     26\u001B[0m \u001B[38;5;66;03m# print(updated_scenario[0])\u001B[39;00m\n\u001B[1;32m     27\u001B[0m \n\u001B[1;32m     28\u001B[0m \u001B[38;5;66;03m# Visualize\u001B[39;00m\n\u001B[0;32m---> 29\u001B[0m \u001B[43mmake_gif\u001B[49m\u001B[43m(\u001B[49m\u001B[43mupdated_scenario\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlstm_single_step\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[50], line 63\u001B[0m, in \u001B[0;36mmake_gif\u001B[0;34m(data_matrix1, data_matrix2, name)\u001B[0m\n\u001B[1;32m     60\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m ax1\u001B[38;5;241m.\u001B[39mcollections \u001B[38;5;241m+\u001B[39m ax1\u001B[38;5;241m.\u001B[39mlines \u001B[38;5;241m+\u001B[39m ax2\u001B[38;5;241m.\u001B[39mcollections \u001B[38;5;241m+\u001B[39m ax2\u001B[38;5;241m.\u001B[39mlines\n\u001B[1;32m     62\u001B[0m anim \u001B[38;5;241m=\u001B[39m animation\u001B[38;5;241m.\u001B[39mFuncAnimation(fig, update, frames\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m0\u001B[39m, timesteps, \u001B[38;5;241m3\u001B[39m)), interval\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m100\u001B[39m, blit\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m---> 63\u001B[0m \u001B[43manim\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtrajectory_visualization_\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mname\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m.gif\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwriter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mpillow\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     64\u001B[0m plt\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[0;32m~/Documents/UCSD Classes/Deep Learning/AgroverseComp/.venv/lib/python3.9/site-packages/matplotlib/animation.py:1085\u001B[0m, in \u001B[0;36mAnimation.save\u001B[0;34m(self, filename, writer, fps, dpi, codec, bitrate, extra_args, metadata, extra_anim, savefig_kwargs, progress_callback)\u001B[0m\n\u001B[1;32m   1082\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m data \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39m[a\u001B[38;5;241m.\u001B[39mnew_saved_frame_seq() \u001B[38;5;28;01mfor\u001B[39;00m a \u001B[38;5;129;01min\u001B[39;00m all_anim]):\n\u001B[1;32m   1083\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m anim, d \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(all_anim, data):\n\u001B[1;32m   1084\u001B[0m         \u001B[38;5;66;03m# TODO: See if turning off blit is really necessary\u001B[39;00m\n\u001B[0;32m-> 1085\u001B[0m         \u001B[43manim\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_draw_next_frame\u001B[49m\u001B[43m(\u001B[49m\u001B[43md\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mblit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m   1086\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m progress_callback \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1087\u001B[0m             progress_callback(frame_number, total_frames)\n",
      "File \u001B[0;32m~/Documents/UCSD Classes/Deep Learning/AgroverseComp/.venv/lib/python3.9/site-packages/matplotlib/animation.py:1121\u001B[0m, in \u001B[0;36mAnimation._draw_next_frame\u001B[0;34m(self, framedata, blit)\u001B[0m\n\u001B[1;32m   1119\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pre_draw(framedata, blit)\n\u001B[1;32m   1120\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_draw_frame(framedata)\n\u001B[0;32m-> 1121\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_post_draw\u001B[49m\u001B[43m(\u001B[49m\u001B[43mframedata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mblit\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/UCSD Classes/Deep Learning/AgroverseComp/.venv/lib/python3.9/site-packages/matplotlib/animation.py:1146\u001B[0m, in \u001B[0;36mAnimation._post_draw\u001B[0;34m(self, framedata, blit)\u001B[0m\n\u001B[1;32m   1144\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_blit_draw(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_drawn_artists)\n\u001B[1;32m   1145\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1146\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcanvas\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdraw_idle\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/UCSD Classes/Deep Learning/AgroverseComp/.venv/lib/python3.9/site-packages/matplotlib/backend_bases.py:1905\u001B[0m, in \u001B[0;36mFigureCanvasBase.draw_idle\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1903\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_is_idle_drawing:\n\u001B[1;32m   1904\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_idle_draw_cntx():\n\u001B[0;32m-> 1905\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdraw\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/UCSD Classes/Deep Learning/AgroverseComp/.venv/lib/python3.9/site-packages/matplotlib/backends/backend_agg.py:387\u001B[0m, in \u001B[0;36mFigureCanvasAgg.draw\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    384\u001B[0m \u001B[38;5;66;03m# Acquire a lock on the shared font cache.\u001B[39;00m\n\u001B[1;32m    385\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtoolbar\u001B[38;5;241m.\u001B[39m_wait_cursor_for_draw_cm() \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtoolbar\n\u001B[1;32m    386\u001B[0m       \u001B[38;5;28;01melse\u001B[39;00m nullcontext()):\n\u001B[0;32m--> 387\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfigure\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdraw\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrenderer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    388\u001B[0m     \u001B[38;5;66;03m# A GUI class may be need to update a window using this draw, so\u001B[39;00m\n\u001B[1;32m    389\u001B[0m     \u001B[38;5;66;03m# don't forget to call the superclass.\u001B[39;00m\n\u001B[1;32m    390\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mdraw()\n",
      "File \u001B[0;32m~/Documents/UCSD Classes/Deep Learning/AgroverseComp/.venv/lib/python3.9/site-packages/matplotlib/artist.py:95\u001B[0m, in \u001B[0;36m_finalize_rasterization.<locals>.draw_wrapper\u001B[0;34m(artist, renderer, *args, **kwargs)\u001B[0m\n\u001B[1;32m     93\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(draw)\n\u001B[1;32m     94\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mdraw_wrapper\u001B[39m(artist, renderer, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m---> 95\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mdraw\u001B[49m\u001B[43m(\u001B[49m\u001B[43martist\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrenderer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     96\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m renderer\u001B[38;5;241m.\u001B[39m_rasterizing:\n\u001B[1;32m     97\u001B[0m         renderer\u001B[38;5;241m.\u001B[39mstop_rasterizing()\n",
      "File \u001B[0;32m~/Documents/UCSD Classes/Deep Learning/AgroverseComp/.venv/lib/python3.9/site-packages/matplotlib/artist.py:72\u001B[0m, in \u001B[0;36mallow_rasterization.<locals>.draw_wrapper\u001B[0;34m(artist, renderer)\u001B[0m\n\u001B[1;32m     69\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m artist\u001B[38;5;241m.\u001B[39mget_agg_filter() \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     70\u001B[0m         renderer\u001B[38;5;241m.\u001B[39mstart_filter()\n\u001B[0;32m---> 72\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mdraw\u001B[49m\u001B[43m(\u001B[49m\u001B[43martist\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrenderer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     73\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     74\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m artist\u001B[38;5;241m.\u001B[39mget_agg_filter() \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/Documents/UCSD Classes/Deep Learning/AgroverseComp/.venv/lib/python3.9/site-packages/matplotlib/figure.py:3162\u001B[0m, in \u001B[0;36mFigure.draw\u001B[0;34m(self, renderer)\u001B[0m\n\u001B[1;32m   3159\u001B[0m             \u001B[38;5;66;03m# ValueError can occur when resizing a window.\u001B[39;00m\n\u001B[1;32m   3161\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpatch\u001B[38;5;241m.\u001B[39mdraw(renderer)\n\u001B[0;32m-> 3162\u001B[0m     \u001B[43mmimage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_draw_list_compositing_images\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   3163\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrenderer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43martists\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msuppressComposite\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3165\u001B[0m     renderer\u001B[38;5;241m.\u001B[39mclose_group(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfigure\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m   3166\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
      "File \u001B[0;32m~/Documents/UCSD Classes/Deep Learning/AgroverseComp/.venv/lib/python3.9/site-packages/matplotlib/image.py:132\u001B[0m, in \u001B[0;36m_draw_list_compositing_images\u001B[0;34m(renderer, parent, artists, suppress_composite)\u001B[0m\n\u001B[1;32m    130\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m not_composite \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m has_images:\n\u001B[1;32m    131\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m a \u001B[38;5;129;01min\u001B[39;00m artists:\n\u001B[0;32m--> 132\u001B[0m         \u001B[43ma\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdraw\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrenderer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    133\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    134\u001B[0m     \u001B[38;5;66;03m# Composite any adjacent images together\u001B[39;00m\n\u001B[1;32m    135\u001B[0m     image_group \u001B[38;5;241m=\u001B[39m []\n",
      "File \u001B[0;32m~/Documents/UCSD Classes/Deep Learning/AgroverseComp/.venv/lib/python3.9/site-packages/matplotlib/artist.py:72\u001B[0m, in \u001B[0;36mallow_rasterization.<locals>.draw_wrapper\u001B[0;34m(artist, renderer)\u001B[0m\n\u001B[1;32m     69\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m artist\u001B[38;5;241m.\u001B[39mget_agg_filter() \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     70\u001B[0m         renderer\u001B[38;5;241m.\u001B[39mstart_filter()\n\u001B[0;32m---> 72\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mdraw\u001B[49m\u001B[43m(\u001B[49m\u001B[43martist\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrenderer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     73\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     74\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m artist\u001B[38;5;241m.\u001B[39mget_agg_filter() \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/Documents/UCSD Classes/Deep Learning/AgroverseComp/.venv/lib/python3.9/site-packages/matplotlib/axes/_base.py:3137\u001B[0m, in \u001B[0;36m_AxesBase.draw\u001B[0;34m(self, renderer)\u001B[0m\n\u001B[1;32m   3134\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m artists_rasterized:\n\u001B[1;32m   3135\u001B[0m     _draw_rasterized(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfigure, artists_rasterized, renderer)\n\u001B[0;32m-> 3137\u001B[0m \u001B[43mmimage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_draw_list_compositing_images\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   3138\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrenderer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43martists\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfigure\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msuppressComposite\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3140\u001B[0m renderer\u001B[38;5;241m.\u001B[39mclose_group(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maxes\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m   3141\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstale \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/UCSD Classes/Deep Learning/AgroverseComp/.venv/lib/python3.9/site-packages/matplotlib/image.py:132\u001B[0m, in \u001B[0;36m_draw_list_compositing_images\u001B[0;34m(renderer, parent, artists, suppress_composite)\u001B[0m\n\u001B[1;32m    130\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m not_composite \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m has_images:\n\u001B[1;32m    131\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m a \u001B[38;5;129;01min\u001B[39;00m artists:\n\u001B[0;32m--> 132\u001B[0m         \u001B[43ma\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdraw\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrenderer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    133\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    134\u001B[0m     \u001B[38;5;66;03m# Composite any adjacent images together\u001B[39;00m\n\u001B[1;32m    135\u001B[0m     image_group \u001B[38;5;241m=\u001B[39m []\n",
      "File \u001B[0;32m~/Documents/UCSD Classes/Deep Learning/AgroverseComp/.venv/lib/python3.9/site-packages/matplotlib/artist.py:72\u001B[0m, in \u001B[0;36mallow_rasterization.<locals>.draw_wrapper\u001B[0;34m(artist, renderer)\u001B[0m\n\u001B[1;32m     69\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m artist\u001B[38;5;241m.\u001B[39mget_agg_filter() \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     70\u001B[0m         renderer\u001B[38;5;241m.\u001B[39mstart_filter()\n\u001B[0;32m---> 72\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mdraw\u001B[49m\u001B[43m(\u001B[49m\u001B[43martist\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrenderer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     73\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     74\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m artist\u001B[38;5;241m.\u001B[39mget_agg_filter() \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/Documents/UCSD Classes/Deep Learning/AgroverseComp/.venv/lib/python3.9/site-packages/matplotlib/collections.py:1005\u001B[0m, in \u001B[0;36m_CollectionWithSizes.draw\u001B[0;34m(self, renderer)\u001B[0m\n\u001B[1;32m   1002\u001B[0m \u001B[38;5;129m@artist\u001B[39m\u001B[38;5;241m.\u001B[39mallow_rasterization\n\u001B[1;32m   1003\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mdraw\u001B[39m(\u001B[38;5;28mself\u001B[39m, renderer):\n\u001B[1;32m   1004\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mset_sizes(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sizes, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfigure\u001B[38;5;241m.\u001B[39mdpi)\n\u001B[0;32m-> 1005\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdraw\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrenderer\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/UCSD Classes/Deep Learning/AgroverseComp/.venv/lib/python3.9/site-packages/matplotlib/artist.py:72\u001B[0m, in \u001B[0;36mallow_rasterization.<locals>.draw_wrapper\u001B[0;34m(artist, renderer)\u001B[0m\n\u001B[1;32m     69\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m artist\u001B[38;5;241m.\u001B[39mget_agg_filter() \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     70\u001B[0m         renderer\u001B[38;5;241m.\u001B[39mstart_filter()\n\u001B[0;32m---> 72\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mdraw\u001B[49m\u001B[43m(\u001B[49m\u001B[43martist\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrenderer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     73\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     74\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m artist\u001B[38;5;241m.\u001B[39mget_agg_filter() \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/Documents/UCSD Classes/Deep Learning/AgroverseComp/.venv/lib/python3.9/site-packages/matplotlib/collections.py:394\u001B[0m, in \u001B[0;36mCollection.draw\u001B[0;34m(self, renderer)\u001B[0m\n\u001B[1;32m    392\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    393\u001B[0m     combined_transform \u001B[38;5;241m=\u001B[39m transform\n\u001B[0;32m--> 394\u001B[0m extents \u001B[38;5;241m=\u001B[39m \u001B[43mpaths\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_extents\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcombined_transform\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    395\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (extents\u001B[38;5;241m.\u001B[39mwidth \u001B[38;5;241m<\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfigure\u001B[38;5;241m.\u001B[39mbbox\u001B[38;5;241m.\u001B[39mwidth\n\u001B[1;32m    396\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m extents\u001B[38;5;241m.\u001B[39mheight \u001B[38;5;241m<\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfigure\u001B[38;5;241m.\u001B[39mbbox\u001B[38;5;241m.\u001B[39mheight):\n\u001B[1;32m    397\u001B[0m     do_single_path_optimization \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/UCSD Classes/Deep Learning/AgroverseComp/.venv/lib/python3.9/site-packages/matplotlib/path.py:639\u001B[0m, in \u001B[0;36mPath.get_extents\u001B[0;34m(self, transform, **kwargs)\u001B[0m\n\u001B[1;32m    636\u001B[0m xys \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m    637\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m curve, code \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39miter_bezier(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    638\u001B[0m     \u001B[38;5;66;03m# places where the derivative is zero can be extrema\u001B[39;00m\n\u001B[0;32m--> 639\u001B[0m     _, dzeros \u001B[38;5;241m=\u001B[39m \u001B[43mcurve\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maxis_aligned_extrema\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    640\u001B[0m     \u001B[38;5;66;03m# as can the ends of the curve\u001B[39;00m\n\u001B[1;32m    641\u001B[0m     xys\u001B[38;5;241m.\u001B[39mappend(curve([\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m*\u001B[39mdzeros, \u001B[38;5;241m1\u001B[39m]))\n",
      "File \u001B[0;32m~/Documents/UCSD Classes/Deep Learning/AgroverseComp/.venv/lib/python3.9/site-packages/matplotlib/bezier.py:302\u001B[0m, in \u001B[0;36mBezierSegment.axis_aligned_extrema\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    300\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m n \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m    301\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39marray([]), np\u001B[38;5;241m.\u001B[39marray([])\n\u001B[0;32m--> 302\u001B[0m Cj \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpolynomial_coefficients\u001B[49m\n\u001B[1;32m    303\u001B[0m dCj \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marange(\u001B[38;5;241m1\u001B[39m, n\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m)[:, \u001B[38;5;28;01mNone\u001B[39;00m] \u001B[38;5;241m*\u001B[39m Cj[\u001B[38;5;241m1\u001B[39m:]\n\u001B[1;32m    304\u001B[0m dims \u001B[38;5;241m=\u001B[39m []\n",
      "File \u001B[0;32m~/Documents/UCSD Classes/Deep Learning/AgroverseComp/.venv/lib/python3.9/site-packages/matplotlib/bezier.py:277\u001B[0m, in \u001B[0;36mBezierSegment.polynomial_coefficients\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    274\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m n \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m10\u001B[39m:\n\u001B[1;32m    275\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPolynomial coefficients formula unstable for high \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    276\u001B[0m                   \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124morder Bezier curves!\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;167;01mRuntimeWarning\u001B[39;00m)\n\u001B[0;32m--> 277\u001B[0m P \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcontrol_points\u001B[49m\n\u001B[1;32m    278\u001B[0m j \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marange(n\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m)[:, \u001B[38;5;28;01mNone\u001B[39;00m]\n\u001B[1;32m    279\u001B[0m i \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marange(n\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m)[\u001B[38;5;28;01mNone\u001B[39;00m, :]  \u001B[38;5;66;03m# _comb is non-zero for i <= j\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/UCSD Classes/Deep Learning/AgroverseComp/.venv/lib/python3.9/site-packages/matplotlib/bezier.py:235\u001B[0m, in \u001B[0;36mBezierSegment.control_points\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    232\u001B[0m \u001B[38;5;129m@property\u001B[39m\n\u001B[1;32m    233\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mcontrol_points\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    234\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"The control points of the curve.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 235\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_cpoints\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 1800x900 with 2 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize fine-tunded prediction\n",
    "\n",
    "# model = load_model()\n",
    "\n",
    "# Parameters\n",
    "Tobs = 50\n",
    "Tpred = 60\n",
    "\n",
    "data = train_data[0]\n",
    "\n",
    "# Select a test scenario (can use any valid index)\n",
    "test_scenario = data.copy()  # shape (agents, time_steps, features)\n",
    "\n",
    "# Forecast future positions\n",
    "predicted_positions = finetune_forecast_positions(test_scenario, Tobs, Tpred, model, X_mean, X_std, y_mean, y_std, verbose=True)\n",
    "\n",
    "# Create combined matrix of past observed + predicted for ego agent (agent 0)\n",
    "ego_past = test_scenario[0, :Tobs, :2]               # shape (Tobs, 2)\n",
    "ego_future = predicted_positions[0]                  # shape (Tpred, 2)\n",
    "ego_full = np.concatenate([ego_past, ego_future], axis=0)  # shape (Tobs + Tpred, 2)\n",
    "\n",
    "# Create updated scenario with predicted ego and original others\n",
    "updated_scenario = test_scenario.copy()\n",
    "updated_scenario[0, :Tobs+Tpred, :2] = ego_full  # Replace ego trajectory\n",
    "\n",
    "# print(updated_scenario[0])\n",
    "\n",
    "# Visualize\n",
    "make_gif(updated_scenario, data, name='lstm_single_step')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-25T22:48:27.457640Z",
     "start_time": "2025-05-25T22:48:21.957339Z"
    }
   },
   "id": "c1b534ab5b272dcf",
   "execution_count": 142
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Train the model -> to beat val of 0.0711\n",
    "model, X_mean, X_std, y_mean, y_std = train_model(train_data)\n",
    "\n",
    "# Save the model \n",
    "save_model(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-05-25T22:47:01.937412Z"
    }
   },
   "id": "3338be4ed075b368",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_mse(train_data, model, forecast_fn, Tobs=50, Tpred=60, \n",
    "                 X_mean=None, X_std=None, y_mean=None, y_std=None):\n",
    "    \"\"\"\n",
    "    Evaluate MSE of a forecast function on ego agent across scenarios.\n",
    "\n",
    "    Args:\n",
    "        train_data (np.ndarray): Shape (N, agents, timesteps, features)\n",
    "        model (tf.keras.Model): Trained single-step model\n",
    "        forecast_fn (callable): Function to call for forecasting. Must match:\n",
    "            (scenario_data, Tobs, Tpred, model, X_mean, X_std, y_mean, y_std) -> np.ndarray\n",
    "        Tobs (int): Number of observed steps\n",
    "        Tpred (int): Number of predicted steps\n",
    "        X_mean, X_std, y_mean, y_std (optional): Normalization statistics\n",
    "\n",
    "    Returns:\n",
    "        float: Mean squared error across valid scenarios\n",
    "    \"\"\"\n",
    "    N = train_data.shape[0]\n",
    "    mse_list = []\n",
    "    valid_scenarios = 0\n",
    "    \n",
    "    print(f\"Evaluating {N} scenarios...\")\n",
    "    report_interval = max(1, N // 10)\n",
    "\n",
    "    for i in range(N):\n",
    "        if i % report_interval == 0 or i == N - 1:\n",
    "            print(f\"Processing scenario {i+1}/{N} ({(i+1)/N*100:.1f}%)\")\n",
    "        \n",
    "        scenario_data = train_data[i]  # shape (agents, timesteps, 6)\n",
    "        ego_agent_data = scenario_data[0]  # shape (timesteps, 6)\n",
    "        ground_truth = ego_agent_data[Tobs:Tobs + Tpred, :2]\n",
    "\n",
    "        if np.all(ground_truth == 0):\n",
    "            continue\n",
    "\n",
    "        valid_scenarios += 1\n",
    "\n",
    "        predicted_positions = forecast_fn(\n",
    "            ego_agent_data[np.newaxis, :, :],  # (1, timesteps, 6)\n",
    "            Tobs, Tpred, model,\n",
    "            X_mean=X_mean, X_std=X_std, y_mean=y_mean, y_std=y_std\n",
    "        )\n",
    "\n",
    "        mse = mean_squared_error(ground_truth, predicted_positions[0])\n",
    "        mse_list.append(mse)\n",
    "\n",
    "        if i % report_interval == 0:\n",
    "            print(f\"  Current scenario {i} MSE: {mse:.4f}\")\n",
    "\n",
    "    if mse_list:\n",
    "        overall_mse = np.mean(mse_list)\n",
    "        print(f\"\\nEvaluation complete: {valid_scenarios} valid scenarios\")\n",
    "        print(f\"Mean Squared Error (MSE): {overall_mse:.4f}\")\n",
    "        print(f\"Min MSE: {np.min(mse_list):.4f}, Max MSE: {np.max(mse_list):.4f}\")\n",
    "        return overall_mse\n",
    "    else:\n",
    "        print(\"No valid scenarios for evaluation.\")\n",
    "        return None"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-25T21:50:04.215120Z",
     "start_time": "2025-05-25T21:50:04.191750Z"
    }
   },
   "id": "94a6d7a17d8677e1",
   "execution_count": 67
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating 30 scenarios...\n",
      "Processing scenario 1/30 (3.3%)\n",
      "  Current scenario 0 MSE: 1.2999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mehul/Documents/UCSD Classes/Deep Learning/AgroverseComp/.venv/lib/python3.9/site-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 26 variables whereas the saved optimizer has 2 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "/Users/mehul/Documents/UCSD Classes/Deep Learning/AgroverseComp/.venv/lib/python3.9/site-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 26 variables whereas the saved optimizer has 2 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing scenario 4/30 (13.3%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mehul/Documents/UCSD Classes/Deep Learning/AgroverseComp/.venv/lib/python3.9/site-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 26 variables whereas the saved optimizer has 2 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Current scenario 3 MSE: 1043.6423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mehul/Documents/UCSD Classes/Deep Learning/AgroverseComp/.venv/lib/python3.9/site-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 26 variables whereas the saved optimizer has 2 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "/Users/mehul/Documents/UCSD Classes/Deep Learning/AgroverseComp/.venv/lib/python3.9/site-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 26 variables whereas the saved optimizer has 2 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing scenario 7/30 (23.3%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mehul/Documents/UCSD Classes/Deep Learning/AgroverseComp/.venv/lib/python3.9/site-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 26 variables whereas the saved optimizer has 2 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Current scenario 6 MSE: 0.3687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mehul/Documents/UCSD Classes/Deep Learning/AgroverseComp/.venv/lib/python3.9/site-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 26 variables whereas the saved optimizer has 2 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "/Users/mehul/Documents/UCSD Classes/Deep Learning/AgroverseComp/.venv/lib/python3.9/site-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 26 variables whereas the saved optimizer has 2 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing scenario 10/30 (33.3%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mehul/Documents/UCSD Classes/Deep Learning/AgroverseComp/.venv/lib/python3.9/site-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 26 variables whereas the saved optimizer has 2 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Current scenario 9 MSE: 711.5614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mehul/Documents/UCSD Classes/Deep Learning/AgroverseComp/.venv/lib/python3.9/site-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 26 variables whereas the saved optimizer has 2 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "/Users/mehul/Documents/UCSD Classes/Deep Learning/AgroverseComp/.venv/lib/python3.9/site-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 26 variables whereas the saved optimizer has 2 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing scenario 13/30 (43.3%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mehul/Documents/UCSD Classes/Deep Learning/AgroverseComp/.venv/lib/python3.9/site-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 26 variables whereas the saved optimizer has 2 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Current scenario 12 MSE: 0.5274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mehul/Documents/UCSD Classes/Deep Learning/AgroverseComp/.venv/lib/python3.9/site-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 26 variables whereas the saved optimizer has 2 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "/Users/mehul/Documents/UCSD Classes/Deep Learning/AgroverseComp/.venv/lib/python3.9/site-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 26 variables whereas the saved optimizer has 2 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing scenario 16/30 (53.3%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mehul/Documents/UCSD Classes/Deep Learning/AgroverseComp/.venv/lib/python3.9/site-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 26 variables whereas the saved optimizer has 2 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Current scenario 15 MSE: 648.1710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mehul/Documents/UCSD Classes/Deep Learning/AgroverseComp/.venv/lib/python3.9/site-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 26 variables whereas the saved optimizer has 2 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "/Users/mehul/Documents/UCSD Classes/Deep Learning/AgroverseComp/.venv/lib/python3.9/site-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 26 variables whereas the saved optimizer has 2 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing scenario 19/30 (63.3%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mehul/Documents/UCSD Classes/Deep Learning/AgroverseComp/.venv/lib/python3.9/site-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 26 variables whereas the saved optimizer has 2 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Current scenario 18 MSE: 617.8995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mehul/Documents/UCSD Classes/Deep Learning/AgroverseComp/.venv/lib/python3.9/site-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 26 variables whereas the saved optimizer has 2 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "/Users/mehul/Documents/UCSD Classes/Deep Learning/AgroverseComp/.venv/lib/python3.9/site-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 26 variables whereas the saved optimizer has 2 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing scenario 22/30 (73.3%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mehul/Documents/UCSD Classes/Deep Learning/AgroverseComp/.venv/lib/python3.9/site-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 26 variables whereas the saved optimizer has 2 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Current scenario 21 MSE: 74.1905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mehul/Documents/UCSD Classes/Deep Learning/AgroverseComp/.venv/lib/python3.9/site-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 26 variables whereas the saved optimizer has 2 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "/Users/mehul/Documents/UCSD Classes/Deep Learning/AgroverseComp/.venv/lib/python3.9/site-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 26 variables whereas the saved optimizer has 2 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing scenario 25/30 (83.3%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mehul/Documents/UCSD Classes/Deep Learning/AgroverseComp/.venv/lib/python3.9/site-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 26 variables whereas the saved optimizer has 2 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Current scenario 24 MSE: 29.3670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mehul/Documents/UCSD Classes/Deep Learning/AgroverseComp/.venv/lib/python3.9/site-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 26 variables whereas the saved optimizer has 2 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "/Users/mehul/Documents/UCSD Classes/Deep Learning/AgroverseComp/.venv/lib/python3.9/site-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 26 variables whereas the saved optimizer has 2 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing scenario 28/30 (93.3%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mehul/Documents/UCSD Classes/Deep Learning/AgroverseComp/.venv/lib/python3.9/site-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 26 variables whereas the saved optimizer has 2 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Current scenario 27 MSE: 0.5658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mehul/Documents/UCSD Classes/Deep Learning/AgroverseComp/.venv/lib/python3.9/site-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 26 variables whereas the saved optimizer has 2 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing scenario 30/30 (100.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mehul/Documents/UCSD Classes/Deep Learning/AgroverseComp/.venv/lib/python3.9/site-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 26 variables whereas the saved optimizer has 2 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation complete: 30 valid scenarios\n",
      "Mean Squared Error (MSE): 292.7695\n",
      "Min MSE: 0.0970, Max MSE: 1093.6461\n"
     ]
    },
    {
     "data": {
      "text/plain": "292.76945905004925"
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate on training data\n",
    "evaluate_mse(train_data[3000:3030], model, forecast_fn=finetune_forecast_positions)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-25T21:51:55.519225Z",
     "start_time": "2025-05-25T21:50:39.342446Z"
    }
   },
   "id": "76a45de5cfb6e914",
   "execution_count": 71
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def generate_submission(data, output_csv, model, forecast_fn, \n",
    "                        Tobs=50, Tpred=60,\n",
    "                        X_mean=None, X_std=None, y_mean=None, y_std=None):\n",
    "    \"\"\"\n",
    "    Generates a submission CSV file with predicted (x, y) positions for the ego agent.\n",
    "\n",
    "    Args:\n",
    "        data (np.ndarray): Test data of shape (num_scenarios, agents, timesteps, features).\n",
    "        output_csv (str): Output CSV file path.\n",
    "        model (tf.keras.Model): Trained prediction model.\n",
    "        forecast_fn (callable): Forecast function with signature:\n",
    "            (scenario_data, Tobs, Tpred, model, X_mean, X_std, y_mean, y_std) -> np.ndarray\n",
    "        Tobs (int): Number of observed time steps.\n",
    "        Tpred (int): Number of predicted time steps.\n",
    "        X_mean, X_std, y_mean, y_std (optional): Normalization statistics.\n",
    "    \"\"\"\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    print(f\"Generating predictions for {data.shape[0]} scenarios...\")\n",
    "\n",
    "    for i in range(data.shape[0]):\n",
    "        scenario_data = data[i]            # Shape: (agents, timesteps, 6)\n",
    "        ego_agent_data = scenario_data[0]  # Shape: (timesteps, 6)\n",
    "\n",
    "        predicted_positions = forecast_fn(\n",
    "            ego_agent_data[np.newaxis, :, :], Tobs, Tpred, model,\n",
    "            X_mean=X_mean, X_std=X_std, y_mean=y_mean, y_std=y_std\n",
    "        )  # Shape: (1, Tpred, 2)\n",
    "\n",
    "        predictions.extend(predicted_positions[0])  # Append shape: (Tpred, 2)\n",
    "\n",
    "    submission_df = pd.DataFrame(predictions, columns=[\"x\", \"y\"])\n",
    "    submission_df.index.name = 'index'  # Match required format\n",
    "    submission_df.to_csv(output_csv)\n",
    "\n",
    "    print(f\"Submission file '{output_csv}' saved with shape {submission_df.shape}\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c2557ca9bf552c0b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "generate_submission(\n",
    "    data=test_data,\n",
    "    output_csv='lstm_submission.csv',\n",
    "    model=model,\n",
    "    forecast_fn=forecast_positions,  # or finetune_forecast_positions\n",
    "    Tobs=50,\n",
    "    Tpred=60,\n",
    "    X_mean=X_mean,\n",
    "    X_std=X_std,\n",
    "    y_mean=y_mean,\n",
    "    y_std=y_std\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d4f8d7605d54296d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
