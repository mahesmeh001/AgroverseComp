{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-21T00:03:05.442279Z",
     "start_time": "2025-04-21T00:03:03.536556Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data's shape (10000, 50, 110, 6)\n",
      "test_data's shape (2100, 50, 50, 6)\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "import numpy as np\n",
    "train_file = np.load('data/train.npz')\n",
    "train_data = train_file['data']\n",
    "print(\"train_data's shape\", train_data.shape)\n",
    "test_file = np.load('data/test_input.npz')\n",
    "test_data = test_file['data']\n",
    "print(\"test_data's shape\", test_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Train Data Shape: (10000, 50, 110, 6)\n",
      "Processed Train Data Length: 10000 (variable agents per scenario)\n",
      "Original Test Data Shape: (2100, 50, 50, 6)\n",
      "Processed Test Data Length: 2100 (variable agents per scenario)\n"
     ]
    }
   ],
   "source": [
    "# preprocess\n",
    "def preprocess_data(data):\n",
    "    \"\"\"\n",
    "    Removes padded agents (agents with all zero values across time steps).\n",
    "    \n",
    "    Args:\n",
    "        data (numpy.ndarray): Shape (scenarios, agents, time_steps, dimensions)\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: Filtered dataset without padded agents.\n",
    "    \"\"\"\n",
    "    scenarios, agents, time_steps, dimensions = data.shape\n",
    "    processed_data = []\n",
    "\n",
    "    for i in range(scenarios):\n",
    "        scenario_data = data[i]  # Shape (agents, time_steps, dimensions)\n",
    "        \n",
    "        # Identify non-padded agents (at least one nonzero value across all time steps)\n",
    "        valid_agents = np.any(scenario_data != 0, axis=(1, 2))  # Shape (agents,)\n",
    "        \n",
    "        # Filter out only the valid agents\n",
    "        filtered_agents = scenario_data[valid_agents]  # Shape (valid_agents, time_steps, dimensions)\n",
    "        \n",
    "        processed_data.append(filtered_agents)\n",
    "\n",
    "    return processed_data  # List of variable-length arrays per scenario\n",
    "\n",
    "train_data_processed = preprocess_data(train_data)\n",
    "test_data_processed = preprocess_data(test_data)\n",
    "\n",
    "# Print results\n",
    "print(f\"Original Train Data Shape: {train_data.shape}\")\n",
    "print(f\"Processed Train Data Length: {len(train_data_processed)} (variable agents per scenario)\")\n",
    "\n",
    "print(f\"Original Test Data Shape: {test_data.shape}\")\n",
    "print(f\"Processed Test Data Length: {len(test_data_processed)} (variable agents per scenario)\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-21T00:03:06.831779Z",
     "start_time": "2025-04-21T00:03:05.444312Z"
    }
   },
   "id": "81a062b78c6346e7",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# some kind of missing trajectory handling? "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-21T00:03:06.841310Z",
     "start_time": "2025-04-21T00:03:06.833841Z"
    }
   },
   "id": "c5df04a84c63e60f",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input, RepeatVector, TimeDistributed, Dropout"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "db97d754762b390b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def create_lstm_encoder_decoder(\n",
    "    input_dim=6,\n",
    "    output_dim=2,\n",
    "    timesteps_in=50,\n",
    "    timesteps_out=60,\n",
    "    lstm_units=128,\n",
    "    num_layers=2,          # New parameter for stacking\n",
    "    loss_fn='huber',\n",
    "    lr=1e-3\n",
    "):\n",
    "    encoder_inputs = Input(shape=(timesteps_in, input_dim))\n",
    "    x = encoder_inputs\n",
    "\n",
    "    # Encoder: dynamically stacked LSTMs\n",
    "    for i in range(num_layers - 1):\n",
    "        x = LSTM(lstm_units, return_sequences=True)(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "    x = LSTM(lstm_units)(x)  # Final layer with return_sequences=False\n",
    "\n",
    "    decoder_input = RepeatVector(timesteps_out)(x)\n",
    "\n",
    "    # Decoder: dynamically stacked LSTMs\n",
    "    x = decoder_input\n",
    "    for _ in range(num_layers):\n",
    "        x = LSTM(lstm_units, return_sequences=True)(x)\n",
    "        x = Dropout(0.2)(x)\n",
    "\n",
    "    decoder_outputs = TimeDistributed(Dense(output_dim))(x)\n",
    "\n",
    "    model = Model(encoder_inputs, decoder_outputs)\n",
    "    model.compile(optimizer=Adam(learning_rate=lr), loss=loss_fn)\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d7f20fc814e583a3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from keras.src.callbacks import LearningRateScheduler, EarlyStopping, Callback\n",
    "from keras.src.optimizers import Adam\n",
    "from keras import Model\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def exponential_decay_schedule(epoch, lr):\n",
    "    decay_rate = 0.9\n",
    "    decay_steps = 5\n",
    "    if epoch % decay_steps == 0 and epoch:\n",
    "        print('Learning rate update:', lr * decay_rate)\n",
    "        return lr * decay_rate\n",
    "    return lr\n",
    "\n",
    "\n",
    "# Custom callback to monitor LR and stop training\n",
    "class LRThresholdCallback(Callback):\n",
    "    def __init__(self, threshold=9e-5):\n",
    "        super().__init__()\n",
    "        self.threshold = threshold\n",
    "        self.should_stop = False\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        lr = float(self.model.optimizer.learning_rate.numpy())\n",
    "        if lr < self.threshold:\n",
    "            print(f\"\\nLearning rate {lr:.6f} < threshold {self.threshold}, moving to Phase 2.\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "def train_model(train_data, batch_size=32, validation_split=0.2):\n",
    "    n_scenarios = train_data.shape[0]\n",
    "    Tobs = 50\n",
    "\n",
    "    X_train_raw = []\n",
    "    y_train_deltas = []\n",
    "\n",
    "    for i in range(n_scenarios):\n",
    "        ego_data = train_data[i, 0, :, :]\n",
    "        if np.all(ego_data == 0):\n",
    "            continue\n",
    "\n",
    "        observed_data = ego_data[:Tobs]            # shape (50, 6)\n",
    "        future_positions = ego_data[Tobs:, :2]     # shape (60, 2)\n",
    "        last_observed_pos = observed_data[-1, :2]  # shape (2,)\n",
    "\n",
    "        if np.any(np.all(observed_data == 0, axis=1)) or np.any(np.all(future_positions == 0, axis=1)):\n",
    "            continue\n",
    "\n",
    "        # Compute deltas\n",
    "        delta = future_positions - last_observed_pos  # shape (60, 2)\n",
    "\n",
    "        X_train_raw.append(observed_data)\n",
    "        y_train_deltas.append(delta)\n",
    "\n",
    "    X_train = np.array(X_train_raw)\n",
    "    y_train = np.array(y_train_deltas)\n",
    "\n",
    "    print(f\"Training with {X_train.shape[0]} valid scenarios\")\n",
    "    print(f\"Input shape: {X_train.shape}, Output shape: {y_train.shape}\")\n",
    "\n",
    "    # === Phase 1 === now\n",
    "    model = create_lstm_encoder_decoder(\n",
    "        input_dim=X_train.shape[-1],\n",
    "        output_dim=2,\n",
    "        timesteps_in=Tobs,\n",
    "        timesteps_out=y_train.shape[1],\n",
    "        loss_fn='mse',\n",
    "        lr=0.001\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- Phase 1: Training with MSE loss ---\\n\")\n",
    "    phase1_callbacks = [\n",
    "        LearningRateScheduler(exponential_decay_schedule),\n",
    "        EarlyStopping(patience=4, restore_best_weights=True, monitor='val_loss'),\n",
    "        LRThresholdCallback(threshold=9e-5)\n",
    "    ]\n",
    "\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=10,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=validation_split,\n",
    "        verbose=1,\n",
    "        callbacks=phase1_callbacks\n",
    "    )\n",
    "\n",
    "    # === Phase 2 ===\n",
    "    print(\"\\n--- Phase 2: Fine-tuning with MSE loss ---\\n\")\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-4), loss='mse')\n",
    "\n",
    "    phase2_callbacks = [\n",
    "        LearningRateScheduler(exponential_decay_schedule),\n",
    "        EarlyStopping(patience=3, restore_best_weights=True, monitor='val_loss')\n",
    "    ]\n",
    "\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=10,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=validation_split,\n",
    "        verbose=1,\n",
    "        callbacks=phase2_callbacks\n",
    "    )\n",
    "\n",
    "    return model\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-21T00:25:50.103659Z",
     "start_time": "2025-04-21T00:25:50.014702Z"
    }
   },
   "id": "4d68034c165292c5",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_model(model, filepath='lstm_1.pkl'):\n",
    "    \"\"\"Save model and scaler together in a pickle file\"\"\"\n",
    "    model_json = model.to_json()\n",
    "    model_weights = model.get_weights()\n",
    "    data = {\n",
    "        'model_json': model_json,\n",
    "        'model_weights': model_weights,\n",
    "    }\n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    print(f\"Model saved to {filepath}\")\n",
    "\n",
    "def load_model(filepath='lstm_1.pkl'):\n",
    "    \"\"\"Load model and scaler from pickle file\"\"\"\n",
    "    with open(filepath, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    # Reconstruct model\n",
    "    model = tf.keras.models.model_from_json(data['model_json'])\n",
    "    model.set_weights(data['model_weights'])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-21T00:25:53.569418Z",
     "start_time": "2025-04-21T00:25:53.567622Z"
    }
   },
   "id": "e3d667b335d4a0a",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 10000 valid scenarios\n",
      "Input shape: (10000, 50, 6), Output shape: (10000, 60, 2)\n",
      "\n",
      "--- Phase 1: Training with Huber loss ---\n",
      "\n",
      "Epoch 1/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m24s\u001B[0m 91ms/step - loss: 9.7306 - val_loss: 9.2787 - learning_rate: 0.0100\n",
      "Epoch 2/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m26s\u001B[0m 102ms/step - loss: 9.5527 - val_loss: 9.2788 - learning_rate: 0.0100\n",
      "Epoch 3/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m23s\u001B[0m 90ms/step - loss: 9.6450 - val_loss: 9.2787 - learning_rate: 0.0100\n",
      "Epoch 4/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m25s\u001B[0m 100ms/step - loss: 9.5809 - val_loss: 9.3047 - learning_rate: 0.0100\n",
      "Epoch 5/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m24s\u001B[0m 94ms/step - loss: 9.6332 - val_loss: 9.2785 - learning_rate: 0.0100\n",
      "Learning rate update: 0.008999999798834325\n",
      "Epoch 6/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m23s\u001B[0m 94ms/step - loss: 9.5290 - val_loss: 9.2792 - learning_rate: 0.0090\n",
      "Epoch 7/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m22s\u001B[0m 89ms/step - loss: 9.3877 - val_loss: 9.2788 - learning_rate: 0.0090\n",
      "Epoch 8/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m21s\u001B[0m 86ms/step - loss: 9.6329 - val_loss: 9.2783 - learning_rate: 0.0090\n",
      "Epoch 9/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m23s\u001B[0m 92ms/step - loss: 9.5725 - val_loss: 9.2784 - learning_rate: 0.0090\n",
      "Epoch 10/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m22s\u001B[0m 88ms/step - loss: 9.7252 - val_loss: 9.2798 - learning_rate: 0.0090\n",
      "\n",
      "--- Phase 2: Fine-tuning with MSE loss ---\n",
      "Epoch 1/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m23s\u001B[0m 82ms/step - loss: 309.6442 - val_loss: 299.5163 - learning_rate: 1.0000e-04\n",
      "Epoch 2/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m22s\u001B[0m 87ms/step - loss: 312.1733 - val_loss: 299.5144 - learning_rate: 1.0000e-04\n",
      "Epoch 3/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m21s\u001B[0m 84ms/step - loss: 315.4665 - val_loss: 299.4946 - learning_rate: 1.0000e-04\n",
      "Epoch 4/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m23s\u001B[0m 93ms/step - loss: 309.3809 - val_loss: 299.4539 - learning_rate: 1.0000e-04\n",
      "Epoch 5/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m24s\u001B[0m 96ms/step - loss: 308.0648 - val_loss: 299.4585 - learning_rate: 1.0000e-04\n",
      "Learning rate update: 8.999999772640876e-05\n",
      "Epoch 6/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m22s\u001B[0m 89ms/step - loss: 304.2874 - val_loss: 299.4493 - learning_rate: 9.0000e-05\n",
      "Epoch 7/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m22s\u001B[0m 88ms/step - loss: 304.9165 - val_loss: 299.4245 - learning_rate: 9.0000e-05\n",
      "Epoch 8/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m28s\u001B[0m 111ms/step - loss: 308.0185 - val_loss: 299.4318 - learning_rate: 9.0000e-05\n",
      "Epoch 9/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m25s\u001B[0m 99ms/step - loss: 309.6598 - val_loss: 299.4288 - learning_rate: 9.0000e-05\n",
      "Epoch 10/10\n",
      "\u001B[1m250/250\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m23s\u001B[0m 92ms/step - loss: 312.2578 - val_loss: 299.4923 - learning_rate: 9.0000e-05\n",
      "Model saved to lstm_1.pkl\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model = train_model(train_data)\n",
    "\n",
    "# Save the model \n",
    "save_model(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-21T00:33:41.957818Z",
     "start_time": "2025-04-21T00:25:54.009598Z"
    }
   },
   "id": "3338be4ed075b368",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def reconstruct_absolute_positions(pred_deltas, last_observed_positions):\n",
    "    \"\"\"\n",
    "    Reconstruct absolute predicted positions by adding deltas to the last observed position.\n",
    "\n",
    "    Args:\n",
    "        pred_deltas: np.ndarray of shape (N, Tpred, 2)\n",
    "        last_observed_positions: np.ndarray of shape (N, 2)\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray of shape (N, Tpred, 2)\n",
    "    \"\"\"\n",
    "    return last_observed_positions[:, None, :] + pred_deltas\n",
    "\n",
    "\n",
    "def forecast_positions(scenario_data, Tobs, Tpred, model):\n",
    "    \"\"\"\n",
    "    Use LSTM model to forecast future positions from deltas and reconstruct absolute positions.\n",
    "\n",
    "    Args:\n",
    "        scenario_data (numpy.ndarray): Shape (agents, time_steps, dimensions)\n",
    "        Tobs (int): Number of observed time steps\n",
    "        Tpred (int): Number of future time steps to predict\n",
    "        model (Model): Trained LSTM model that predicts deltas\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Predicted absolute positions of shape (agents, Tpred, 2)\n",
    "    \"\"\"\n",
    "    agents, _, _ = scenario_data.shape\n",
    "    predicted_positions = np.zeros((agents, Tpred, 2))\n",
    "\n",
    "    for agent_idx in range(agents):\n",
    "        agent_data = scenario_data[agent_idx, :Tobs, :]\n",
    "\n",
    "        # Skip padding\n",
    "        if np.all(agent_data == 0):\n",
    "            continue\n",
    "\n",
    "        # Predict deltas\n",
    "        X_pred = np.expand_dims(agent_data, axis=0)\n",
    "        delta_pred = model.predict(X_pred, verbose=0)[0]  # (Tpred, 2)\n",
    "\n",
    "        # Last observed absolute position\n",
    "        last_pos = agent_data[Tobs - 1, :2]  # shape (2,)\n",
    "\n",
    "        # Reconstruct absolute positions\n",
    "        absolute_pred = reconstruct_absolute_positions(\n",
    "            pred_deltas=np.expand_dims(delta_pred, axis=0),\n",
    "            last_observed_positions=np.expand_dims(last_pos, axis=0)\n",
    "        )[0]\n",
    "\n",
    "        predicted_positions[agent_idx] = absolute_pred\n",
    "\n",
    "    return predicted_positions\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-21T00:34:47.280969Z",
     "start_time": "2025-04-21T00:34:47.279416Z"
    }
   },
   "id": "4180a40e0501cb00",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "def make_gif(data_matrix1, data_matrix2, name='comparison'):\n",
    "    cmap1 = plt.cm.get_cmap('viridis', 50)\n",
    "    cmap2 = plt.cm.get_cmap('plasma', 50)\n",
    "\n",
    "    assert data_matrix1.shape[1] == data_matrix2.shape[1], \"Both matrices must have same number of timesteps\"\n",
    "    timesteps = data_matrix1.shape[1]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(18, 9))\n",
    "    ax1, ax2 = axes\n",
    "\n",
    "    def update(frame):\n",
    "        for ax in axes:\n",
    "            ax.clear()\n",
    "\n",
    "        for i in range(data_matrix1.shape[0]):\n",
    "            for (data_matrix, ax, cmap) in [(data_matrix1, ax1, cmap1), (data_matrix2, ax2, cmap2)]:\n",
    "                x = data_matrix[i, frame, 0]\n",
    "                y = data_matrix[i, frame, 1]\n",
    "                if x != 0 and y != 0:\n",
    "                    xs = data_matrix[i, :frame+1, 0]\n",
    "                    ys = data_matrix[i, :frame+1, 1]\n",
    "                    mask = (xs != 0) & (ys != 0)\n",
    "                    xs = xs[mask]\n",
    "                    ys = ys[mask]\n",
    "                    if len(xs) > 0 and len(ys) > 0:\n",
    "                        color = cmap(i)\n",
    "                        ax.plot(xs, ys, alpha=0.9, color=color)\n",
    "                        ax.scatter(x, y, s=80, color=color)\n",
    "\n",
    "        # Plot ego vehicle (index 0) on both\n",
    "        ax1.plot(data_matrix1[0, :frame, 0], data_matrix1[0, :frame, 1], color='tab:orange', label='Ego Vehicle')\n",
    "        ax1.scatter(data_matrix1[0, frame, 0], data_matrix1[0, frame, 1], s=80, color='tab:orange')\n",
    "        ax1.set_title('Prediction')\n",
    "\n",
    "        ax2.plot(data_matrix2[0, :frame, 0], data_matrix2[0, :frame, 1], color='tab:orange', label='Ego Vehicle')\n",
    "        ax2.scatter(data_matrix2[0, frame, 0], data_matrix2[0, frame, 1], s=80, color='tab:orange')\n",
    "        ax2.set_title('Actual')\n",
    "\n",
    "        for ax, data_matrix in zip(axes, [data_matrix1, data_matrix2]):\n",
    "            ax.set_xlim(data_matrix[:, :, 0][data_matrix[:, :, 0] != 0].min() - 10,\n",
    "                        data_matrix[:, :, 0][data_matrix[:, :, 0] != 0].max() + 10)\n",
    "            ax.set_ylim(data_matrix[:, :, 1][data_matrix[:, :, 1] != 0].min() - 10,\n",
    "                        data_matrix[:, :, 1][data_matrix[:, :, 1] != 0].max() + 10)\n",
    "            ax.legend()\n",
    "            ax.set_xlabel('X')\n",
    "            ax.set_ylabel('Y')\n",
    "\n",
    "        fig.suptitle(f\"Timestep {frame}\", fontsize=16)\n",
    "        return ax1.collections + ax1.lines + ax2.collections + ax2.lines\n",
    "\n",
    "    anim = animation.FuncAnimation(fig, update, frames=list(range(0, timesteps, 3)), interval=100, blit=True)\n",
    "    anim.save(f'trajectory_visualization_{name}.gif', writer='pillow')\n",
    "    plt.close()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-21T00:39:27.764190Z",
     "start_time": "2025-04-21T00:39:27.761629Z"
    }
   },
   "id": "658c258cac06616e",
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/j6ybhf8906j4bmnp171wtn680000gn/T/ipykernel_97117/3762067397.py:5: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "  cmap1 = plt.cm.get_cmap('viridis', 50)\n",
      "/var/folders/wv/j6ybhf8906j4bmnp171wtn680000gn/T/ipykernel_97117/3762067397.py:6: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "  cmap2 = plt.cm.get_cmap('plasma', 50)\n"
     ]
    }
   ],
   "source": [
    "# visualize prediction\n",
    "\n",
    "# model = load_model()\n",
    "\n",
    "# Parameters\n",
    "Tobs = 50\n",
    "Tpred = 60\n",
    "\n",
    "data = train_data[5]\n",
    "\n",
    "# Select a test scenario (can use any valid index)\n",
    "test_scenario = data.copy()  # shape (agents, time_steps, features)\n",
    "\n",
    "# Forecast future positions\n",
    "predicted_positions = forecast_positions(test_scenario, Tobs, Tpred, model)\n",
    "\n",
    "# Create combined matrix of past observed + predicted for ego agent (agent 0)\n",
    "ego_past = test_scenario[0, :Tobs, :2]               # shape (Tobs, 2)\n",
    "ego_future = predicted_positions[0]                  # shape (Tpred, 2)\n",
    "ego_full = np.concatenate([ego_past, ego_future], axis=0)  # shape (Tobs + Tpred, 2)\n",
    "\n",
    "# Create updated scenario with predicted ego and original others\n",
    "updated_scenario = test_scenario.copy()\n",
    "updated_scenario[0, :Tobs+Tpred, :2] = ego_full  # Replace ego trajectory\n",
    "\n",
    "# Visualize\n",
    "make_gif(updated_scenario, data, name='lstm1')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-04-21T00:42:13.077517Z",
     "start_time": "2025-04-21T00:42:04.592854Z"
    }
   },
   "id": "5a5f4cc76c3c72d3",
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "def evaluate_mse(train_data, model, scaler, Tobs=50, Tpred=60):\n",
    "    \"\"\"\n",
    "    Computes LSTM prediction for ego agent and evaluates MSE with progress reporting.\n",
    "    \"\"\"\n",
    "    N = train_data.shape[0]\n",
    "    mse_list = []\n",
    "    valid_scenarios = 0\n",
    "    \n",
    "    print(f\"Evaluating {N} scenarios...\")\n",
    "    \n",
    "    # Progress reporting variables\n",
    "    report_interval = max(1, N // 10)  # Report at 10% intervals\n",
    "    \n",
    "    for i in range(N):\n",
    "        # Progress reporting\n",
    "        if i % report_interval == 0 or i == N-1:\n",
    "            print(f\"Processing scenario {i+1}/{N} ({(i+1)/N*100:.1f}%)\")\n",
    "        \n",
    "        scenario_data = train_data[i]\n",
    "        ego_agent_data = scenario_data[0]\n",
    "        ground_truth = ego_agent_data[Tobs:Tobs+Tpred, :2]\n",
    "        \n",
    "        # Skip if ground truth contains all zeros (padded)\n",
    "        if np.all(ground_truth == 0):\n",
    "            continue\n",
    "            \n",
    "        valid_scenarios += 1\n",
    "        \n",
    "        # Forecast future positions\n",
    "        predicted_positions = forecast_positions(\n",
    "            ego_agent_data[np.newaxis, :, :],\n",
    "            Tobs, Tpred, model, scaler\n",
    "        )\n",
    "        \n",
    "        # Compute MSE\n",
    "        mse = mean_squared_error(ground_truth, predicted_positions[0])\n",
    "        mse_list.append(mse)\n",
    "        \n",
    "        # Occasional MSE reporting\n",
    "        if i % report_interval == 0:\n",
    "            print(f\"  Current scenario MSE: {mse:.4f}\")\n",
    "    \n",
    "    # Final results\n",
    "    if mse_list:\n",
    "        overall_mse = np.mean(mse_list)\n",
    "        print(f\"Evaluation complete: {valid_scenarios} valid scenarios\")\n",
    "        print(f\"Mean Squared Error (MSE): {overall_mse:.4f}\")\n",
    "        print(f\"Min MSE: {np.min(mse_list):.4f}, Max MSE: {np.max(mse_list):.4f}\")\n",
    "        return overall_mse\n",
    "    else:\n",
    "        print(\"No valid scenarios for evaluation.\")\n",
    "        return None"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-04-21T00:03:49.940582Z"
    }
   },
   "id": "e8bf6a02ebcd4b24",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Evaluate on training data\n",
    "evaluate_mse(train_data, model, scaler)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-04-21T00:03:49.941170Z"
    }
   },
   "id": "6ae1b88910d7e8bb",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
